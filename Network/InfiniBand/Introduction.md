[TOC]

# 1 InfiniBand简介

InfiniBand架构是一种支持**多并发链接**的“**转换线缆**”技术，在这种技术中，**每种链接**都可以达到**2.5 Gbps**的运行速度。这种架构在一个链接的时候速度是500 MB/秒，四个链接的时候速度是2 GB/秒，12个链接的时候速度可以达到6 GB /秒。IBTA成立于1999年8月31日，由Compaq、惠普、IBM、戴尔、英特尔、微软和Sun七家公司牵头，共同研究发展的高速先进的I/O标准。最初的命名为System I/O，1999年10月，正式改名为InfiniBand。InfiniBand是一种长缆线的连接方式，具有**高速、低延迟**的传输特性。

InfiniBand用于**服务器系统内部(内部总线！！！**)并没有发展起来，原因在于英特尔和微软在2002年就退出了IBTA。在此之前，英特尔早已另行倡议**Arapahoe**，亦称为**3GIO(3rd Generation I/O，第三代I/O**)，即今日鼎鼎大名的**PCI Express(PCI\-E**)，InfiniBand、3GIO经过一年的并行，英特尔终究还是选择了PCI\-E。因此，现在应用InfiniBand，主要是用于在**服务器集群**，**系统之间的互联(！！！**）。 

随着CPU性能的飞速发展，I/O系统的性能成为制约服务器性能的瓶颈。于是人们开始重新审视使用了十几年的PCI总线架构。虽然PCI总线结构把数据的传输从8位/16位一举提升到32位，甚至当前的64位，但是它的一些先天劣势限制了其继续发展的势头。PCI总线有如下缺陷：

(1)由于采用了基于总线的共享传输模式，在PCI总线上不可能同时传送两组以上的数据，当一个PCI设备占用总线时，其他设备只能等待；

(2)随着总线频率从33MHz提高到66MHz，甚至133MHz（PCI-X），信号线之间的相互干扰变得越来越严重，在一块主板上布设多条总线的难度也就越来越大；

(3)由于PCI设备采用了内存映射I/O地址的方式建立与内存的联系，热添加PCI设备变成了一件非常困难的工作。目前的做法是在内存中为每一个PCI设备划出一块50M到100M的区域，这段空间用户是不能使用的，因此如果一块主板上支持的热插拔PCI接口越多，用户损失的内存就越多；

(4)PCI的总线上虽然有buffer作为数据的缓冲区，但是它不具备纠错的功能，如果在传输的过程中发生了数据丢失或损坏的情况，控制器只能触发一个NMI中断通知操作系统在PCI总线上发生了错误。

InfiniBand 采 用双队列程序提取技术,使应用程序直接将数据从适配器 送入到应用内存(称为远程直接存储器存取或RDMA), 反之依然。在TCP/IP协议中,来自网卡的数据先拷贝到 核心内存,然后再拷贝到应用存储空间,或从应用空间 将数据拷贝到核心内存,再经由网卡发送到Internet。这 种I/O操作方式,始终需要经过核心内存的转换,它不 仅增加了数据流传输路径的长度,而且大大降低了I/O 的访问速度,增加了CPU的负担。而SDP则是将来自网 卡的数据直接拷贝到用户的应用空间,从而避免了核心内存参入。这种方式就称为零拷贝,它可以在进行大量 数据处理时,达到该协议所能达到的最大的吞吐量