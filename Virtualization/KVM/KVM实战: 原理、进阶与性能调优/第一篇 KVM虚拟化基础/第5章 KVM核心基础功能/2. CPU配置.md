
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 vCPU的概念](#1-vcpu的概念)
* [2 SMP的支持](#2-smp的支持)
* [4 CPU模型](#4-cpu模型)

<!-- /code_chunk_output -->

# 1 vCPU的概念

**QEMU/KVM**为客户机提供一套完整的**硬件系统环境**，在**客户机**看来，其所拥有的CPU即是**vCPU（virtual CPU**）。

在KVM环境中，**每个客户机**都是一个**标准的Linux进程（QEMU进程**），而**每一个vCPU**在宿主机中是**QEMU进程派生**的一个**普通线程**。

在**普通的Linux系统**中，进程一般有**两种执行模式**：**内核模式**和**用户模式**。

而在**KVM环境**中，增加了第3种模式：**客户模式**。vCPU在3种执行模式下的不同分工如下。

（1）用户模式（User Mode）

主要处理**I/O的模拟和管理**，由**QEMU**的代码实现。

（2）内核模式（Kernel Mode）

主要处理特别需要高性能和安全相关的指令，如处理客户模式到内核模式的转换，处理客户模式下的I/O指令或其他特权指令引起的退出（VM\-Exit），处理影子内存管理（shadow MMU）。

（3）客户模式（Guest Mode）

主要执行Guest中的大部分指令，I/O和一些特权指令除外（它们会引起VM\-Exit，被Hypervisor截获并模拟）。

vCPU在KVM中的这3种执行模式下的转换如图5\-1所示。

![](./images/2019-05-19-20-06-17.png)

在KVM环境中，整个系统的基本分层架构如图5\-2所示。

在系统的底层CPU硬件中需要有硬件辅助虚拟化技术的支持（Intel VT或AMD\-V等），宿主机就运行在硬件之上，**KVM的内核部分**是作为**可动态加载内核模块**运行在宿主机中的，其中

- 一个模块是与**硬件平台无关**的实现虚拟化核心基础架构的**kvm模块**，
- 另一个是**硬件平台相关**的kvm\_intel（或kvm\_amd）模块。

而KVM中的**一个客户机**是作为一个**用户空间进程（qemu**）运行的，它和其他普通的用户空间进程（如gnome、kde、firefox、chrome等）一样由内核来调度，使其运行在物理CPU上，不过它由kvm模块的控制，可以在前面介绍的**3种执行模式**下运行。

**多个客户机**就是宿主机中的**多个QEMU进程**，而**一个客户机**的**多个vCPU**就是**一个QEMU进程**中的**多个线程**。

和普通操作系统一样，在**客户机系统**中，同样分别运行着客户机的内核和客户机的用户空间应用程序。

![](./images/2019-05-19-20-10-10.png)

# 2 SMP的支持

在**SMP系统**中，**多个程序（进程**）可以真正做到**并行执行**，而且单个进程的多个线程也可以得到并行执行，这极大地提高了计算机系统并行处理能力和整体性能。

在**硬件**方面，**早期**的计算机系统更多的是在**一个主板**上拥有**多个物理的CPU插槽**，来实现SMP系统。后来，随着**多核技术**、**超线程（Hyper\-Threading**）技术的出现，SMP系统使用**多处理器**、**多核**、**超线程**等技术中的**一个或多个**。

在**操作系统软件**方面，多数的现代操作系统都提供了**对SMP系统的支持**。

例如，在Linux中，下面的Bash脚本（cpu\-info.sh）可以根据/proc/cpuinfo文件来检查当前系统中的CPU数量、多核及超线程的使用情况。

```sh
#!/bin/bash￼
#filename: cpu-info.sh￼
#this script only works in a Linux system which has one or more identical physical CPU(s).￼
￼
echo -n "logical CPU number in total: "￼
#逻辑CPU个数￼
cat /proc/cpuinfo | grep "processor" | wc -l￼
￼
#有些系统没有多核也没有打开超线程，就直接退出脚本￼
cat /proc/cpuinfo | grep -qi "core id"￼
if [ $? -ne 0 ]; then￼
    echo "Warning. No multi-core or hyper-threading is enabled."￼
    exit 0;￼
fi￼
￼
echo -n "physical CPU number in total: "￼
#物理CPU个数￼
cat /proc/cpuinfo | grep "physical id" | sort | uniq | wc -l￼
￼
echo -n "core number in a physical CPU: "￼
#每个物理CPU上core的个数(未计入超线程)￼
core_per_phy_cpu=$(cat /proc/cpuinfo | grep "core id" | sort | uniq | wc -l)￼
echo $core_per_phy_cpu￼
￼
echo -n "logical CPU number in a physical CPU: "￼
#每个物理CPU中逻辑CPU(可能是core、threads或both)的个数￼
logical_cpu_per_phy_cpu=$(cat /proc/cpuinfo | grep "siblings" | sort | uniq | awk- F: '{print $2}')￼
echo $logical_cpu_per_phy_cpu￼
￼
#是否打开超线程，以及每个core上的超线程数目￼
#如果在同一个物理CPU上的两个逻辑CPU具有相同的”core id”，那么超线程是打开的￼
#此处根据前面计算的core_per_phy_cpu和logical_core_per_phy_cpu的比较来查看超线程￼
if [ $logical_cpu_per_phy_cpu -gt $core_per_phy_cpu ]; then￼
    echo "Hyper threading is enabled. Each core has $(expr $logical_cpu_per_phy_cpu / $core_per_phy_cpu ) threads."￼
elif [ $logical_cpu_per_phy_cpu -eq $core_per_phy_cpu ]; then￼
    echo "Hyper threading is NOT enabled."￼
else￼
    echo "Error. There's something wrong."￼
fi
```

SMP是如此的普及和被广泛使用，而QEMU在给客户机模拟CPU时，也可以提供对SMP架构的模拟，让客户机运行在SMP系统中，充分利用物理硬件的SMP并行处理优势。由于**每个vCPU**在**宿主机**中都是**一个线程**，并且**宿主机Linux系统**是支持**多任务处理**的，因此可以通过**两种操作来实现客户机的SMP**，

- 一是将**不同的vCPU的进程交换执行**（分时调度，即使物理硬件非SMP，也可以为客户机模拟出SMP系统环境），
- 二是将在**物理SMP硬件系统**上**同时执行多个vCPU的进程**。

在qemu命令行中，“\-**smp**”参数即是配置客户机的SMP系统，其具体参数如下：

```
-smp [cpus=]n[,maxcpus=cpus][,cores=cores][,threads=threads][,sockets=sockets]
```

其中：
·n用于设置客户机中使用的逻辑CPU数量（默认值是1）。
·maxcpus用于设置客户机中最大可能被使用的CPU数量，包括启动时处于下线（offline）状态的CPU数量（可用于热插拔hot-plug加入CPU，但不能超过maxcpus这个上限）。
·cores用于设置每个CPU的core数量（默认值是1）。
·threads用于设置每个core上的线程数（默认值是1）。
·sockets用于设置客户机中看到的总的CPU socket数量。
下面通过KVM中的几个qemu命令行示例，来看一下如何将SMP应用于客户机中。
示例1：￼

qemu-system-x86_64 -m 1G rhel7.img

不加smp参数，使用其默认值1，在客户机中查看CPU情况，如下：

[root@kvm-guest ~]# cat /proc/cpuinfo￼ processor       : 0￼ vendor_id       : AuthenticAMD￼ cpu family      : 6￼ model           : 6￼ model name      : QEMU Virtual CPU version 2.5+￼ stepping        : 3￼ cpu MHz         : 3591.617￼ cache size      : 512 KB￼ physical id     : 0￼ siblings        : 1￼ core id        : 0￼ cpu cores        : 1￼ apciid        : 0￼ initial apicid  : 0￼ fpu             : yes￼ fpu_exception   : yes￼ cpuid level     : 13￼ wp              : yes￼ flags           : fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36￼                   clflush mmx fxsr sse sse2 syscall nx lm nopl pni cx16 hypervisor￼                   lahf_lm svm￼ bogomips        : 7183.23￼ TLB size        : 1024 4K pages￼ clflush size     : 64￼ cache_alignment : 64￼ address sizes   : 40 bits physical, 48 bits virtual￼ power management:￼ [root@kvm-guest ~]# ls /sys/devices/system/cpu/￼ cpu0 isolated modalias offline possible present￼ cpuidle kernel_max nohz_full online power uevent

由上面的输出信息可知，客户机系统识别到1个QEMU模拟的CPU（cpu0），并且在QEMU monitor（默认按Alt+Ctrl+2组合键切换到monitor）中用“info cpus”命令可以看到客户机中的CPU数量及其对应QEMU线程的ID，如下所示：

(qemu) info cpus￼ * CPU #0: pc=0xffffffff81058e96 (halted) thread_id=10747

在宿主机中看到相应的QEMU进程和线程如下：

[root@kvm-host ~]# ps -eLf | grep qemu￼ root   10744  9335 10744  0  4 15:30 pts/0  00:00:11 qemu-system-x86_64 rhel7.img -m 1G￼ root   10744  9335 10745  0  4 15:30 pts/0  00:00:00 qemu-system-x86_64 rhel7.img -m 1G￼ root   10744  9335 10747 32  4 15:30 pts/0  00:07:51 qemu-system-x86_64 rhel7.img -m 1G￼ root   10744  9335 10748  0  4 15:30 pts/0  00:00:01 qemu-system-x86_64 rhel7.img -m 1G￼ root   11394  9766 11394  0  1 15:54 pts/2  00:00:00 grep --color=auto qemu

由以上信息可以看出，PID（Process ID）为10744的进程是客户机的进程，它产生了1个线程（线程ID为10747）作为客户机的vCPU运行在宿主机中￼。其中，ps命令的-L参数指定打印出线程的ID和线程的个数，-e参数指定选择所有的进程，-f参数指定选择打印出完全的各列。
示例2：

qemu-system-x86_64 -smp 8 -m 8G rhel7.img

“-smp 8”表示分配了8个虚拟的CPU给客户机，在客户机中用前面提到的“cpu-info.sh”脚本查看CPU情况，如下：

[root@kvm-guest ~]# cat /proc/cpuinfo￼ <! --  此处省略/proc/cpuinfo的输出细节 -->￼ [root@kvm-guest ~]# ./cpu-info.sh        #这个是前面提到的Bash脚本￼ logical CPU number in total: 8￼ physical CPU number in total: 8￼ core number in a physical CPU: 1￼ logical CPU number in a physical CPU: 1￼ Hyper threading is NOT enabled.￼ [root@kvm-guest ~]# ls /sys/devices/system/cpu/￼ cpu0 cpu1 cpu2 cpu3 cpu4 cpu5 cpu6 cpu7 cpuidle isolated kernel_max modalias nohz_full offline online possible power present uevent

由上面的输出信息可知，客户机使用了8个CPU（cpu0~cpu7），系统识别的CPU数量也总共是8个，是8个物理CPU，而没有多核、超线程之类的架构。
在QEMU monitor中查询CPU状态，如下：

QEMU 2.7.0 monitor - type 'help' for more information￼ (qemu) VNC server running on ::1:5900￼ (qemu) info cpus￼ * CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=186374￼   CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=186375￼   CPU #2: pc=0xffffffff81060eb6 (halted) thread_id=186376￼   CPU #3: pc=0xffffffff81060eb6 (halted) thread_id=186377￼   CPU #4: pc=0xffffffff81060eb6 (halted) thread_id=186378￼   CPU #5: pc=0xffffffff81060eb6 (halted) thread_id=186379￼   CPU #6: pc=0xffffffff81060eb6 (halted) thread_id=186380￼   CPU #7: pc=0xffffffff81060eb6 (halted) thread_id=186381

在宿主机中看到相应的QEMU进程和线程如下：

[root@kvm-host ~]# ps -eLf | grep qemu￼ root     186360  96470 186360  0   12 21:43 pts/4    00:00:01 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186361  0   12 21:43 pts/4    00:00:00 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186374  7   12 21:43 pts/4    00:00:15 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186375  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186376  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186377  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186378  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186379  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186380  1   12 21:43 pts/4    00:00:04 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186381  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G￼ root     186360  96470 186383  0   12 21:43 pts/4    00:00:01 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G ￼ root     186360  96470 186492  0   12 21:46 pts/4    00:00:00 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G ￼ root     186519 178215 186519  0    1 21:47 pts/6    00:00:00 grep --color=auto qemu

由以上信息可知，PID为186360的进程是QEMU启动客户机的进程，它产生了8个线程作为客户机的8个vCPU在运行。
示例3：

qemu-system-x86_64 -m 8G -smp 8,sockets=2,cores=2,threads=2 rhel7.img

通过-smp参数详细定义了客户机中SMP的架构，在客户中得到的CPU信息如下：

[root@kvm-guest ~]# cat /proc/cpuinfo￼ <! --  此处省略/proc/cpuinfo的输出细节 -->￼ [root@kvm-guest ~]# sh cpu-info.sh ￼ logical CPU number in total: 8￼ physical CPU number in total: 2￼ core number in a physical CPU: 2￼ logical CPU number in a physical CPU: 4￼ Hyper threading is enabled. Each core has 2 threads.￼ [root@kvm-guest ~]# ls /sys/devices/system/cpu/￼ cpu0  cpu1  cpu2  cpu3  cpu4  cpu5  cpu6  cpu7  cpuidle  isolated  kernel_max  microcode  modalias  nohz_full  offline  online  possible  power  present  uevent

从上面的输出信息可知，客户机中共有8个逻辑CPU（cpu0~cpu7），2个CPU socket，每个socket有2个核，每个核有2个线程（超线程处于打开状态）。
示例4：

qemu-system-x86_64 -m 8G -smp 4,maxcpus=8 rhel7.img -enable-kvm

通过-smp参数详细定义了客户机中最多有8个CPU可用，在系统启动之时有4个处于开启状态，在客户中得到的CPU信息如下：

[root@kvm-guest ~]# sh cpu-info.sh ￼ logical CPU number in total: 4￼ physical CPU number in total: 4￼ core number in a physical CPU: 1￼ logical CPU number in a physical CPU: 1￼ Hyper threading is NOT enabled.￼ [root@kvm-guest ~]# lscpu￼ Architecture:          x86_64￼ CPU op-mode(s):        32-bit, 64-bit￼ Byte Order:            Little Endian￼ CPU(s):                4￼ On-line CPU(s) list:   0-3￼ Thread(s) per core:    1￼ Core(s) per socket:    1￼ Socket(s):             4￼ NUMA node(s):          1￼ Vendor ID:             GenuineIntel￼ CPU family:            6￼ Model:                 6￼ Model name:            QEMU Virtual CPU version 2.5+￼ Stepping:              3￼ CPU MHz:               2194.980￼ BogoMIPS:              4389.69￼ Hypervisor vendor:     KVM￼ Virtualization type:   full￼ L1d cache:             32K￼ L1i cache:             32K￼ L2 cache:              4096K￼ NUMA node0 CPU(s):     0-3

可以看到，客户机启动之后有4个CPU，分别位于4个socket的单一core上。
下面我们来做热插拔操作，切换到QEMU monitor执行“cpu-add￼<id>”的指令。

(qemu) info cpus￼ * CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=190609￼   CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=190610￼   CPU #2: pc=0xffffffff81060eb6 (halted) thread_id=190611￼   CPU #3: pc=0xffffffff81060eb6 (halted) thread_id=190612￼ (qemu) cpu-add 7￼ (qemu) info cpus￼ * CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=190609￼   CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=190610￼   CPU #2: pc=0xffffffff81060eb6 (halted) thread_id=190611￼   CPU #3: pc=0xffffffff81060eb6 (halted) thread_id=190612￼   CPU #7: pc=0xffffffff81060eb6 (halted) thread_id=190690

我们热插入7号CPU（0～7中最后一个），可以在QEMU monitor中看到#7号CPU上线了。
再在客户机里面检查：

[root@kvm-guest ~]# lscpu￼ Architecture:          x86_64￼ CPU op-mode(s):        32-bit, 64-bit￼ Byte Order:            Little Endian￼ CPU(s):                5￼ On-line CPU(s) list:   0-4￼ Thread(s) per core:    1￼ Core(s) per socket:    1￼ Socket(s):             5￼ NUMA node(s):          1￼ Vendor ID:             GenuineIntel￼ CPU family:            6￼ Model:                 6￼ Model name:            QEMU Virtual CPU version 2.5+￼ Stepping:              3￼ CPU MHz:               2194.980￼ BogoMIPS:              4389.69￼ Hypervisor vendor:     KVM￼ Virtualization type:   full￼ L1d cache:             32K￼ L1i cache:             32K￼ L2 cache:              4096K￼ NUMA node0 CPU(s):     0-4￼ [root@kvm-guest ~]# sh cpu-info.sh ￼ logical CPU number in total: 5￼ physical CPU number in total: 5￼ core number in a physical CPU: 1￼ logical CPU number in a physical CPU: 1￼ Hyper threading is NOT enabled. ￼ [root@kvm-guest ~]# ls /sys/devices/system/cpu/￼ cpu0  cpu1  cpu2  cpu3  cpu4  cpuidle  isolated  kernel_max  microcode  modalias  nohz_full  offline  online  possible  power  present  uevent

可以看到客户机变成有5个CPU了。但是可以看到，新增的CPU编号却是连续的（cpu4），尽管我们热插入的是#7号CPU。

5.2.3 CPU过载使用

KVM允许客户机过载使用（over-commit）物理资源，即允许为客户机分配的CPU和内存数量多于物理上实际存在的资源。
物理资源的过载使用能带来资源充分利用方面的好处。试想在一台强大的硬件服务器中运行Web服务器、图片存储服务器、后台数据统计服务器等作为虚拟客户机，但是它们不会在同一时刻都负载很高，如Web服务器和图片服务器在白天工作时间负载较重，而后台数据统计服务器主要在晚上工作，所以对物理资源进行合理的过载使用，给这几个客户机分配的系统资源总数多余实际拥有的物理资源，就可能在白天和夜晚都充分利用物理硬件资源，而且由于几个客户机不会同时对物理资源造成很大的压力，它们各自的服务质量（QoS）也能得到保障。
CPU的过载使用是让一个或多个客户机使用vCPU的总数量超过实际拥有的物理CPU数量。QEMU会启动更多的线程来为客户机提供服务，这些线程也被Linux内核调度运行在物理CPU硬件上。
关于CPU的过载使用，推荐的做法是对多个单CPU的客户机使用over-commit，比如，在拥有4个逻辑CPU的宿主机中，同时运行多于4个（如8个、16个）客户机，其中每个客户机都分配一个vCPU。这时，如果每个宿主机的负载不是很大，宿主机Linux对每个客户机的调度是非常有效的，这样的过载使用并不会带来客户机的性能损失。
关于CPU的过载使用，最不推荐的做法是让某一个客户机的vCPU数量超过物理系统上存在的CPU数量。比如，在拥有4个逻辑CPU的宿主机中，同时运行一个或多个客户机，其中每个客户机的vCPU数量多于4个（如16个）。这样的使用方法会带来比较明显的性能下降，其性能反而不如为客户机分配2个（或4个）vCPU的情况。而且如果客户机负载过重，可能会让整个系统运行不稳定。不过，在并非100%满负载的情况下，一个（或多个）有4个vCPU的客户机运行在拥有4个逻辑CPU的宿主机中并不会带来明显的性能损失。
总的来说，KVM允许CPU的过载使用，但是并不推荐在实际的生产环境（特别是负载较重的环境）中过载使用CPU。在生产环境中过载使用CPU，有必要在部署前进行严格的性能和稳定性测试。

# 4 CPU模型

每一种虚拟机管理程序（Virtual Machine Monitor，简称VMM或Hypervisor）都会定义自己的策略，让客户机看起来有一个默认的CPU类型。有的Hypervisor会简单地将宿主机中CPU的类型和特性直接传递给客户机使用，而QEMU/KVM在默认情况下会向客户机提供一个名为qemu64或qemu32的基本CPU模型。QEMU/KVM的这种策略会带来一些好处，如可以对CPU特性提供一些高级的过滤功能，还可以将物理平台根据提供的基本CPU模型进行分组（如将几台IvyBridge和Sandybridge硬件平台分为一组，都提供相互兼容的SandyBridge或qemu64的CPU模型），从而使