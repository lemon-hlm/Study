
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 内存设置基本参数](#1-内存设置基本参数)
* [2 EPT和VPID简介](#2-ept和vpid简介)

<!-- /code_chunk_output -->

# 1 内存设置基本参数

在通过qemu命令行启动客户机时设置内存大小的参数如下：

```
-m megs #设置客户机的内存为megs MB大小
```

**默认的单位为MB**，也支持加上“**M**”或“**G**”作为后缀来显式指定使用**MB**或**GB**作为内存分配的单位。

如果**不设置\-m参数**，QEMU对客户机分配的内存大小**默认值为128MB**（参见源代码中**hw/core/machine.c**中的函数**machine\_class\_init**），这个大小对于RHEL OS来说是不够的（见https://access.redhat.com/articles/rhel\-limits），所以笔者在尝试不指定内存大小而启动RHEL7 guest，guest启动过程中会失败，告警内存不足。

一般我们启动客户机，这个参数都是必不可少的。下面通过两个示例来进一步说明“\-m”参数设置内存的具体用法。

# 2 EPT和VPID简介

**EPT（Extended Page Tables，扩展页表**），属于Intel的**第二代硬件虚拟化**技术，它是针对内存管理单元（MMU）的虚拟化扩展。EPT降低了内存虚拟化的难度（与影子页表相比），也提升了内存虚拟化的性能。从基于Intel的Nehalem￼架构的平台开始，EPT就作为CPU的一个特性加入CPU硬件中了。

和运行在真实物理硬件上的操作系统一样，在客户机操作系统看来，客户机可用的内存空间也是一个从零地址开始的连续的物理内存空间。为了达到这个目的，Hypervisor（即KVM）引入了一层新的地址空间，即**客户机物理地址空间**，这个地址空间不是真正的硬件上的地址空间，它们之间还有一层映射。

所以，在**虚拟化环境**下，内存使用就需要**两层的地址转换**，即客户机应用程序可见的客户机虚拟地址（Guest Virtual Address，**GVA**）到客户机物理地址（Guest Physical Address，**GPA**）的转换，再从客户机物理地址（**GPA**）到宿主机物理地址（Host Physical Address，**HPA**）的转换。其中，**前一个转换**由**客户机操作系统**来完成，而**后一个转换**由**Hypervisor**来负责。

在硬件EPT特性加入之前，**影子页表（Shadow Page Tables**）从软件上维护了从客户机虚拟地址（**GVA**）到宿主机物理地址（**HPA**）之间的映射，每一份客户机操作系统的页表也对应一份影子页表。有了影子页表，在普通的内存访问时都可实现从GVA到HPA的直接转换，从而避免了上面前面提到的两次地址转换。Hypervisor将影子页表载入物理上的内存管理单元（Memory Management Unit，MMU）中进行地址翻译。图5\-3展示了GVA、GPA、HPA之间的转换，以及影子页表的作用。

尽管影子页表提供了在物理MMU硬件中能使用的页表，但是其缺点也是比较明显的。首先影子页表的实现非常复杂，导致其开发、调试和维护都比较困难。其次，影子页表的内存开销也比较大，因为需要为每个客户机进程对应的页表的都维护一个影子页表。

为了解决影子页表存在的问题，Intel的CPU提供了EPT技术（AMD提供的类似技术叫作NPT，即Nested Page Tables），直接在硬件上支持GVA→GPA→HPA的两次地址转换，从而降低内存虚拟化实现的复杂度，也进一步提升了内存虚拟化的性能。图5-4展示了Intel EPT技术的基本原理。
￼
图5-3　影子页表的作用
￼
图5-4　EPT基本原理

CR3（控制寄存器3）将客户机程序所见的客户机虚拟地址（GVA）转化为客户机物理地址（GPA），然后再通过EPT将客户机物理地址（GPA）转化为宿主机物理地址（HPA）。这两次地址转换都是由CPU硬件来自动完成的，其转换效率非常高。在使用EPT的情况下，客户机内部的Page Fault、INVLPG（使TLB￼项目失效）指令、CR3寄存器的访问等都不会引起VM\-Exit，因此大大减少了VM-Exit的数量，从而提高了性能。另外，EPT只需要维护一张EPT页表，而不需要像“影子页表”那样为每个客户机进程的页表维护一张影子页表，从而也减少了内存的开销。VPID（Virtual Processor Identifiers，虚拟处理器标识）是在硬件上对TLB资源管理的优化，通过在硬件上为每个TLB项增加一个标识，用于不同的虚拟处理器的地址空间，从而能够区分Hypervisor和不同处理器的TLB。硬件区分了不同的TLB项分别属于不同虚拟处理器，因此可以避免每次进行VM\-Entry和VM\-Exit时都让TLB全部失效，提高了VM切换的效率。由于有了这些在VM切换后仍然继续存在的TLB项，硬件减少了一些不必要的页表访问，减少了内存访问次数，从而提高了Hypervisor和客户机的运行速度。VPID也会对客户机的实时迁移（Live Migration）有很好的效率提升，会节省实时迁移的开销，提升实时迁移的速度，降低迁移的延迟（Latency）。VPID与EPT是一起加入CPU中的特性，也是Intel公司在2009年推出Nehalem系列处理器上新增的与虚拟化相关的重要功能。

在Linux操作系统中，可以通过如下命令查看/proc/cpuinfo中的CPU标志，来确定当前系统是否支持EPT和VPID功能。

```
[root@kvm-host ~]# grep -E “ept|vpid” /proc/cpuinfo ￼
flags   : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat￼
 pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx￼
 pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl￼
 xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64￼
 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca￼
 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx￼
 f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm￼
 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust￼
 bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt￼
 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local
```

在宿主机中，可以根据sysfs￼文件系统中kvm_intel模块的当前参数值来确定KVM是否打开EPT和VPID特性。在默认情况下，如果硬件支持了EPT、VPID，则kvm_intel模块加载时默认开启EPT和VPID特性，这样KVM会默认使用它们。

[root@kvm-host ~]# cat /sys/module/kvm_intel/parameters/ept￼ Y￼ [root@kvm-host ~]# cat /sys/module/kvm_intel/parameters/vpid￼ Y

在加载kvm_intel模块时，可以通过设置ept和vpid参数的值来打开或关闭EPT和VPID。当然，如果kvm_intel模块已经处于加载状态，则需要先卸载这个模块，在重新加载之时加入所需的参数设置。当然，一般不要手动关闭EPT和VPID功能，否则会导致客户机中内存访问的性能下降。

[root@kvm-host ~]# modprobe kvm_intel ept=0,vpid=0 ￼ [root@kvm-host ~]# rmmod kvm_intel￼ [root@kvm-host ~]# modprobe kvm_intel ept=1,vpid=1

5.3.3　内存过载使用
同5.2.3节中介绍的CPU过载使用类似，在KVM中内存也是允许过载使用（over-commit）的，KVM能够让分配给客户机的内存总数大于实际可用的物理内存总数。由于客户机操作系统及其上的应用程序并非一直100%地利用其分配到的内存，并且宿主机上的多个客户机一般也不会同时达到100%的内存使用率，所以内存过载分配是可行的。一般来说，有如下3种方式来实现内存的过载使用。
1）内存交换（swapping）：用交换空间（swap space）来弥补内存的不足。
2）气球（ballooning）：通过virio_balloon驱动来实现宿主机Hypervisor和客户机之间的协作。
3）页共享（page sharing）：通过KSM（Kernel Samepage Merging）合并多个客户机进程使用的相同内存页。
其中，第1种内存交换的方式是最成熟的（Linux中很早就开始应用），也是目前广泛使用的，不过，相比KSM和ballooning的方式效率较低一些。ballooning和KSM将分别在其他章介绍，本章主要介绍利用swapping这种方式实现内存过载使用。
KVM中客户机是一个QEMU进程，宿主机系统没有特殊对待它而分配特定的内存给QEMU，只是把它当作一个普通Linux进程。Linux内核在进程请求更多内存时才分配给它们更多的内存，所以也是在客户机操作系统请求更多内存时，KVM才向其分配更多的内存。
用swapping方式来让内存过载使用，要求有足够的交换空间（swap space）来满足所有的客户机进程和宿主机中其他进程所需内存。可用的物理内存空间和交换空间的大小之和应该等于或大于配置给所有客户机的内存总和，否则，在各个客户机内存使用同时达到较高比率时，可能会有客户机（因内存不足）被强制关闭。
下面通过一个实际的例子来说明如何计算应该分配的交换空间大小以满足内存的过载使用。
某个服务器有32GB的物理内存，想在其上运行64个内存配置为1GB的客户机。在宿主机中，大约需要4GB大小的内存来满足系统进程、驱动、磁盘缓存及其他应用程序所需内存（不包括客户机进程所需内存）。计算过程如下：
客户机所需交换分区为：64×1GB+4GB―32GB=36GB。
根据Redhat的建议￼，对于32GB物理内存的RHEL系统，推荐使用至少4GB的交换分区。
所以，在宿主机中总共需要建立至少40GB（36GB+4GB）的交换分区，来满足安全实现客户机内存的过载使用。
从下面的简单实验可以看出，客户机并非一开始就在宿主机中占用其启动时配置的内存：分配4G，但启动后实际消耗掉系统内存1G。
在宿主机中，在启动客户机之前和之后查看到的系统内存情况如下：

[root@kvm-host ~]# free -g￼               total        used        free      shared  buff/cache   available￼ Mem:            125           3         116           0           6         121￼ Swap:            31           0          31￼ [root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize ￼ [root@kvm-host ~]# free -g￼               total        used        free      shared  buff/cache   available￼ Mem:            125           3         115           0           6         121￼ Swap:            31           0          31

在客户机中，查看内存使用情况如下（它确实有3G没有用，所以宿主机并不急于分配这3G给它）：

[root@kvm-guest ~]# free -g￼               total        used        free      shared  buff/cache   available￼ Mem:              3           0           3           0           0           3￼ Swap:             3           0           3

从理论上来说，供客户机过载使用的内存可以达到实际物理内存的几倍甚至几十倍，不过除非特殊情况，一般不建议过多地过载使用内存。一方面，交换空间通常是由磁盘分区来实现的，其读写速度比物理内存读写速度慢得多，性能并不好；另一方面，过多的内存过载使用也可能导致系统稳定性降低。所以，KVM允许内存过载使用，但在生产环境中配置内存的过载使用之前，仍然应该根据实际应用进行充分的测试。
