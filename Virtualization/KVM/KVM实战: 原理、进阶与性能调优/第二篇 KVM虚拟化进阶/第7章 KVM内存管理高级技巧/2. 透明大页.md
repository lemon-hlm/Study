
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 大页的缺点](#1-大页的缺点)
* [2 透明大页原理](#2-透明大页原理)
* [3 透明大页的使用](#3-透明大页的使用)
	* [3.1 内核编译选项和启动参数](#31-内核编译选项和启动参数)
	* [3.2 宿主机中查看并配置THP的使用方式](#32-宿主机中查看并配置thp的使用方式)
		* [3.2.1 enable接口](#321-enable接口)
		* [3.2.1 defrag接口](#321-defrag接口)
		* [3.2.2 khugepaged/defrag接口](#322-khugepageddefrag接口)

<!-- /code_chunk_output -->

# 1 大页的缺点

在上一节中已经介绍过，使用大页可以提高系统内存的使用效率和性能。

不过大页有如下几个缺点：

1）大页必须在**使用前就准备好**。

2）应用程序代码必须**显式地使用大页**（一般是调用mmap、shmget系统调用，或者使用libhugetlbfs库对它们的封装）。

3）大页必须**常驻物理内存**中，**不能交换到交换分区**中。

4）需要**超级用户权限**来**挂载hugetlbfs文件系统**，尽管挂载之后可以指定挂载点的uid、gid、mode等使用权限供普通用户使用。

5）如果**预留了大页内存**但**没实际使用**，就会造成**物理内存的浪费**。

# 2 透明大页原理

而**透明大页**￼（Transparent Hugepage）既发挥了大页的一些优点，又能**避免**了**上述缺点**。

透明大页（THP）是Linux内核的一个特性。目前一些流行的Linux发行版（如RHEL 6/7、Ubuntu 12.04以后等）的内核都默认提供了透明大页的支持。

- 透明大页，如它的名称描述的一样，**对所有应用程序都是透明**的（**transparent**），应用程序不需要任何修改即可享受透明大页带来的好处。

- 在使用透明大页时，普通的使用hugetlbfs大页依然可以正常使用，而在**没有普通的大页可供使用**时，**才使用透明大页**。

- **透明大页**是**可交换的（swapable**），当需要交换到**交换空间**时，透明大页**被打碎为常规的4KB大小的内存页**。

- 在**使用透明大页**时，如果因为**内存碎片**导致**大页内存分配失败**，这时系统可以优雅地**使用常规的4KB页替换**，而且**不会发生任何错误**、**故障或用户态的通知**。

- 而当系统内存较为充裕、有**很多的大页可用**时，**常规的页分配的物理内存**可以通过**khugepaged内核线程自动迁往透明大页内存**。

内核线程khugepaged的作用是，**扫描正在运行的进程**，然后**试图将使用的常规内存页转换到使用大页！！！**。

目前，**透明大页**仅仅**支持匿名内存（anonymous memory）的映射**，对磁盘缓存（page cache）和共享内存（shared memory）￼的支持还处于开发之中。

# 3 透明大页的使用

下面看一下使用透明大页的步骤。

## 3.1 内核编译选项和启动参数

1）在**编译Linux内核**时，配置好透明大页的支持。配置文件中的示例如下：

```
CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE=y￼
CONFIG_TRANSPARENT_HUGEPAGE=y￼
CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS=y￼
# CONFIG_TRANSPARENT_HUGEPAGE_MADVISE is not set
```

这表示**默认对所有应用程序的内存分配！！！** 都**尽可能地使用透明大页**。

当然，还可以在**系统启动**时修改Linux内核**启动参数**“**transparent\_hugepage**”来调整这个默认值。其取值为如下3个值之一：

```
transparent_hugepage=[always|madvise|never]
```

它们的具体含义在下面解释。

## 3.2 宿主机中查看并配置THP的使用方式

2）在运行的宿主机中查看及配置透明大页的使用方式，命令行如下：

```
[root@kvm-host ~]# cat /sys/kernel/mm/transparent_hugepage/enabled ￼
[always] madvise never￼

[root@kvm-host ~]# cat /sys/kernel/mm/transparent_hugepage/defrag ￼
always defer [madvise] never￼

[root@kvm-host ~]# cat /sys/kernel/mm/transparent_hugepage/khugepaged/defrag ￼
1￼

[root@kvm-host ~]# echo never > /sys/kernel/mm/transparent_hugepage/defrag￼

[root@kvm-host ~]# cat /sys/kernel/mm/transparent_hugepage/defrag￼
always madvise [never]
```

在本示例的系统中，

### 3.2.1 enable接口

“/sys/kernel/mm/transparent\_hugepage/**enabled**”接口的值

- 为“**always**”，表示**尽可能地在内存分配中使用透明大页**；

- 若将该值设置为“**madvise**”，则表示**仅**在“**MADV\_HUGEPAGE**”标识的**内存区域**使用透明大页；

- 若将该值设置为“**never**”则表示**关闭透明内存大页的功能**。

enable接口对**khugepaged内核进程**的影响:

- 当设置为“**always**”或“**madvice**”时，系统会**自动启动“khugepaged**”这个**内核进程**去执行透明大页的功能；
- 当设置为“**never**”时，系统会**停止“khugepaged”进程**的运行。

### 3.2.1 defrag接口

defrag内存碎片整理

“transparent\_hugepage/**defrag**”这个接口是表示系统在**发生页故障（page fault**）时**同步地做内存碎片的整理**工作，其运行的**频率较高**（在某些情况下可能会带来**额外的负担**）；

### 3.2.2 khugepaged/defrag接口

而“transparent\_hugepage/**khugepaged/defrag**”接口表示在**khugepaged进程运行时**进行**内存碎片的整理工作**，它运行的**频率较低**。

当然还可以在**KVM客户机**中使用**透明大页**，这样在宿主机和客户机同时使用的情况下，更容易提高内存使用的性能。

关于透明大页的详细配置参数还有不少，读者可以阅读 https://www.kernel.org/doc/Docu-mentation/vm/transhuge.txt 了解更多，这里就不赘述了。

3）查看系统使用透明大页的效果，可以通过查看“/proc/meminfo”文件中的“Anon-HugePages”这行来看系统内存中透明大页的大小。要查看具体应用程序使用了多少内存，我们可以查看“/proc/进程号/smaps”中“AnonHugePages”的大小（它会有多个VM地址区间的映射，但只有少数匿名内存区可以使用透明大页）。下面我们来启动一个客户机，看看前后透明大页有没有变化。

启动前，通过下面的命令行可以看到系统中已使用了132个透明大页。

```
[root@kvm-host ~]# cat /proc/meminfo | grep -i AnonHugePages￼ AnonHugePages:    270336 kB￼ [root@kvm-host ~]# echo $((270336/2048))￼ 132
```

我们按如下方式启动一个16G的客户机：

```
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
```

客户机起来以后，通过下面的命令行可以看到系统中已经在用的透明大页有756个。注意，这个数字随系统运行而动态变化，并且，客户机并没有“-mem-prealloc”，所以此时系统并没有真的给它分配16G内存。

```
[root@kvm-host ~]# cat /proc/meminfo | grep -i AnonHugePages￼ AnonHugePages:   1548288 kB￼ [root@kvm-host ~]# echo $((1548288/2048))￼ 756
```

然后进一步查看QEMU进程使用了多少透明大页。从下面的输出我们可以看到，这个客户机使用了2048+1253376+6144=1261568KB，共计616个2MB透明大页，差不多就是客户机启动前后系统透明大页的数量差。这说明QEMU启动客户机，什么特殊设置也没有做，就默默地（莫名其妙地）利用上了透明大页。

```
[root@kvm-host ~]# ps aux | grep qemu￼ root       7212  7.9  1.0 12822852 1321188 pts/1 Sl+ 16:02   0:47 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot￼ root       7614  0.0  0.0 112648   960 pts/2    S+   16:13   0:00 grep --color=auto qemu￼ [root@kvm-host ~]# cat /proc/7212/smaps | grep -e "AnonHugePages"￼ ...￼ AnonHugePages:      2048 kB￼ ...￼ AnonHugePages:   1253376 kB￼ ...￼ AnonHugePages:      6144 kB￼ ...
```

在KVM 2010年论坛（KVM Forum 2010）￼上，透明大页的作者Andrea Arcangeli发表了一个题为“Transparent Hugepage Support”的演讲，其中展示了不少透明大页对性能提升的数据。图7-1展示了在是否打开宿主机和客户机的透明大页（THP）、是否打开Intel EPT特性时，内核编译（kernel build）所需时间长度的对比，时间越短说明效率越高。
￼
图7-1　打开EPT、THP对内核编译效率的提高

由图7-1所示的数据可知，当打开EPT的支持且在宿主机和客户机中同时打开透明大页的支持时，内核编译的效率只比原生的非虚拟化环境中的系统降低了5.67%；而打开EPT但关闭宿主机和客户机的透明大页时，内核编译效率比原生系统降低了24.81%；EPT和透明大页全部关闭时，内核编译效率比原生系统降低了260.15%。这些数据说明，EPT对内存访问效率有很大的提升作用，透明大页对内存访问效率也有较大的提升作用。