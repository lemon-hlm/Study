
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [0 NUMA和UMA](#0-numa和uma)
* [1 numastat](#1-numastat)
* [2 numad](#2-numad)
	* [2.1 numad的参数](#21-numad的参数)
	* [2.1 numad相关操作](#21-numad相关操作)
		* [2.1.1 启动和退出numad](#211-启动和退出numad)
		* [2.1.2 numad绑定进程到numa节点](#212-numad绑定进程到numa节点)
* [3 numactl](#3-numactl)

<!-- /code_chunk_output -->

# 0 NUMA和UMA

NUMA（Non\-Uniform Memory Access，非统一内存访问架构）是相对于UMA（Uniform Memory Access）而言的。早年的计算机架构都是UMA，如图7\-2所示。所有的CPU处理单元（Processor）均质地通过共享的总线访问内存，所有CPU访问所有内存单元的速度是一样的。在多处理器的情形下，多个任务会被分派在各个处理器上并发执行，则它们**竞争内存资源**的情况会非常频繁，从而引起效率的下降。
￼
![](./images/2019-05-28-21-14-56.png)

所以，随着多处理器架构的逐渐普及以及数量的不断增长，NUMA架构兴起，如图7\-3所示。处理器与内存被划分成一个个的节点（node），处理器访问自己节点内的内存会比访问其他节点的内存快。

![](./images/2019-05-28-21-15-17.png)

Intel Xeon系列平台从**2007年**的**Nehalem**那一代开始，就支持NUMA架构了。现在主流的E5、E7系列Xeon平台，通常是2个、4个NUMA node的。

# 1 numastat

numastat用来查看某个（些）进程或者整个系统的内存消耗在各个NUMA节点的分布情况。它的典型输出如下：

```
[root@kvm-host ~]# numastat ￼
                           node0           node1￼
numa_hit                72050204        55925951￼
numa_miss                      0               0￼
numa_foreign                   0               0￼
interleave_hit             38068           39139￼
local_node              71816493        54027058￼
other_node                233711         1898893
```

- numa\_hit表示成功地从该节点分配到的内存页数。
- numa\_miss表示成功地从该节点分配到的内存页数，但其本意是希望从别的节点分配，失败以后退而求其次从该节点分配。
- numa\_foreign与numa\_miss互为“影子”，每个numa\_miss都来自另一个节点的numa\_foreign。
- interleave\_hit，有时候内存请求是没有NUMA节点偏好的，此时会均匀分配自各个节点（interleave），这个数值就是这种情况下从该节点分配出去的内存页面数。
- local\_node表示分配给运行在同一节点的进程的内存页数。
- other\_node与上面相反。local\_node值加上other\_node值就是numa\_hit值。

以上数值**默认**都是**内存页数**，要看具体**多少MB**，可以通过加上\-**n参数**实现。

numastat还可以**只看某些进程**，甚至只要名字片段匹配。比如看QEMU进程的内存分布情况，可以通过“**numastat qemu**”即可。比如：

```
[root@kvm-host ~]# numastat qemu￼
￼
Per-node process memory usage (in MBs) for PID 73658 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                         0.00           21.96           21.96￼
Stack                        0.00            8.37            8.37￼
Private                      0.03         1538.17         1538.20￼
----------------  --------------- --------------- ---------------￼
Total                        0.03         1568.49         1568.52
```

更多参数及用法，大家可以通过man numastat命令了解。

# 2 numad

numad是一个可以**自动管理NUMA亲和性**（affinity）的**工具**（同时也是一个后台进程）。

- 它**实时监控NUMA拓扑结构（topology**）和**资源使用**，
- 并**动态调整**。
- 同时它还可以在**启动一个程序前**，提供**NUMA优化建议**。

与numad功能类似，Kernel的**auto NUMA balancing**（/proc/sys/kernel/**numa\_balancing**）也是进行**动态NUMA资源的调节**。numad启动后会**覆盖**Kernel的auto NUMA balancing功能。

numad与THP和KSM都有些纠葛，我们下面讲到。

## 2.1 numad的参数

numad比较复杂，它有很多参数进行**精细化控制**，下面是几个重要的。

- \-p\<pid\>，\-x\<pid\>，\-r\<pid\>，分别指定numad针对哪些pid以及不针对哪些pid进行自动的NUMA资源优化。

numad自己**内部**维护一个**inclusive list**和一个**exclusive list**；\-p\<pid\>、\-x\<pid\>就是分别往这两个list里面**添加进程id**；\-r\<pid\>就是从这两个**list里面移除**。在numad首次启动时候，可以重复多个\-p或者\-x；启动后，每次调用numad只能跟一个\-p、\-x或者\-r参数。

**默认没有这些指定**的话，numad会对系统**所有进程进行NUMA资源优化**。

- \-S 0\/1，**0**表示只对**inclusive list**里面的进程进行NUMA优化；**1**表示对**除exclusive list**以外的**所有进程**进行优化。通常，\-S与\-p、\-x搭配使用。

- \-R\<cpu\_list\>，reserve，指定一些CPU是numad不能染指的，numad**不会**在自动优化NUMA资源的时候把**进程放到这些CPU上**去运行。

- \-t\<百分比\>，它指示**逻辑（logical）CPU**（比如Intel HyperThread打开时）运算能力对于它的**HW core**的比例。这个值关系到**numad内部调配资源**时的计算，**默认是20%￼**。

- \-u\<百分比\>，numad最多能消耗**每个NUMA节点多少资源**（CPU和内存），默认是**85%**。numad毕竟**不能取代内核调度器**，并不能接管系统里所有route的调度，所以，留有余地是必须的。但当你确定将一个node专属（dedicate）给一个进程时，也可以设置\-u 100%，甚至超过100%，但要小心。

- \-C 0\/1，是否将NUMA节点的inactive file cache作为free memory对待。默认为1，表示进程的inactive file cache不纳入NUMA优化的考量，即如果一个进程还有一些inactive file cache留在另一个节点上，numad也不会把它搬过来。

- \-K 0\/1，控制是否将interleaved memory合并到一个NUMA节点。默认是合并的，但要注意，合并到一个节点并不一定有最好的performance，而应该根据实际的work-load来决定。比如，如果一个系统里主要就是一个大型的数据库应用程序（大量内存访问且地址随机），\-K 1禁止numad合并interleaved memory反而有更好的性能。

- \-m\<百分比>，它是一个阈值，表示当内存中在本地节点的数量达到它所属进程的内存总量的多少时，numad停止对该进程的NUMA优化。

- \-i\<最小间隔：最大间隔>，最小值可以省略。它设置numad 2次扫描系统情况的时间间隔。通常用它来终止（退出）numad，-i 0。

- \-H\<时间间隔>，它设置（override）透明大页（见7.2节）的扫描间隔时间。默认地，numad会将/sys/kernel/mm/tranparent_hugepage/khugepaged/scan_sleep_millisecs值从默认的10000毫秒缩短为1000毫秒，因为更激进的透明大页合并更有利于numad将页面在NUMA节点之间迁移。

- \-w\<NCPUS\[:MB]\>，它就是numad一次性地运行一下（而不是作为系统后台daemon）以供咨询：“我有一个应用程序将要运行，它会需要NCPUS个CPU，M兆内存，numad，你告诉我该把它放到哪个NUMA节点上运行好啊？”numad此时会返回一个合适的NUMA node list，这个list可以作为后面numactl（下面介绍）的参数。

还有一些numad参数，这里笔者就不赘述了。

另外，与THP的关联，上面\-H参数已涉及。与KSM的关联，在于/sys/kernel/mm/ksm/m erge\_nodes最好设置为0，禁止KSM跨NUMA节点地同页合并。

## 2.1 numad相关操作

我们通过几个典型用例来了解上面部分重要参数的用法。

### 2.1.1 启动和退出numad

1）**启动和退出numad**；退出通过“\-**i 0**”完成。

```
[root@kvm-host ~]# numad￼

[root@kvm-host ~]# ps aux | grep numad￼
root   175821  0.2  0.0  19860  572 ?      Ssl  20:44  0:00 numad￼
root   175827  0.0  0.0 112652  960 pts/2  S+   20:44  0:00 grep --color=auto numad￼

[root@kvm-host ~]# numad -i 0      #退出numad￼
[root@kvm-host ~]# ps aux | grep numad￼
root   175836  0.0  0.0 112652 960 pts/2    S+  20:44  0:00 grep --color=auto numad
```

### 2.1.2 numad绑定进程到numa节点

2）通过**numad**将QEMU进程**搬到一个NUMA节点**上。

先看看**没有numad**的时候，内核的**auto NUMA balancing**会是怎样的行为。

查看当前情况，并启动一个客户机。

```
[root@kvm-host ~]# cat /proc/sys/kernel/numa_balancing￼
1￼

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
```

启动后，查看numastat。可以看到，**客户机**所使用的内存，在**两个节点上都有分布**。

```
[root@kvm-host ~]# numastat qemu-system￼
￼
Per-node process memory usage (in MBs) for PID 61898 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                        19.80            0.36           20.17￼
Stack                        8.23            0.11            8.34￼
Private                    127.71          893.88         1021.60￼
----------------  --------------- --------------- ---------------￼
Total                      155.75          894.36         1050.11
```

接下来在**客户机里进行编译内核的行为**，并时不时地在宿主机中查看其numastat的状况。

```
[root@kvm-guest linux-4.9]# make -j 4
```

随着**客户机的运行**，可以看到**两个节点**依然各自分布着一些**内存占用**。

```
[root@kvm-host ~]# numastat qemu-system￼
￼
Per-node process memory usage (in MBs) for PID 61898 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                        20.69            1.71           22.40￼
Stack                       10.41            6.21           16.63￼
Private                   1192.11         2469.65         3661.76￼
----------------  --------------- --------------- ---------------￼
Total                     1223.21         2477.57         3700.79
```

接下来我们**打开numad**，并重做上面的实验。可以看到，一开始**打开numad之前**，客户机（QEMU进程）的内存**分布于两个节点**上，随着客户机的运行，**numad**把内存都搬到**一个节点**上了。

```
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot￼
[root@kvm-host ~]# ps aux | grep -i qemu￼
root  61898  130  0.7 10604948 996800 pts/1 Sl+  15:08   0:19 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot￼
root  61959  0.0  0.0 112652  976 pts/8  S+  15:09  0:00 grep --color=auto -i qemu￼
[root@kvm-host ~]# numad -S 0 -p 64686￼
[root@kvm-host ~]# numastat 64686￼
￼
Per-node process memory usage (in MBs) for PID 64686 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                         5.05           16.94           21.98￼
Stack                        0.06            2.06            2.12￼
Private                    137.43         1121.89         1259.32￼
----------------  --------------- --------------- ---------------￼
Total                      142.54         1140.89         1283.43￼
[root@kvm-host ~]# numastat 64686￼
￼
Per-node process memory usage (in MBs) for PID 64686 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                         0.00           22.09           22.09￼
Stack                        0.00           20.58           20.58￼
Private                      0.00         2105.27         2105.27￼
----------------  --------------- --------------- ---------------￼
Total                        0.00         2147.93         2147.93
```

# 3 numactl

如果说numad是事后（客户机起来以后）调节NUMA资源分配，那么numactl则是主动地在程序起来时候就指定好它的NUMA节点。numactl其实不止它名字表示的那样设置NUMA相关亲和性，它还可以设置共享内存/大页文件系统的内存策略，以及进程的CPU和内存的亲和性。
它的主要用法如下：

```
numactl  [  --all  ]  [ --interleave nodes ] [ --preferred node ] [ --membind nodes ] [ --cpunodebind nodes ] [ --physcpubind cpus ] [ --localalloc ] [--] command {arguments￼...}
```

它的一些主要参数如下：
·--hardware，列出来目前系统中可用的NUMA节点，以及它们之间的距离（distance）。
·--membind，确保command执行时候内存都是从指定的节点上分配；如果该节点没有足够内存，返回失败。
·--cpunodebind，确保command只在指定node的CPU上面执行。
·--phycpubind，确保command只在指定的CPU上执行。
·--localalloc，指定内存只从本地节点上分配。
·--preferred，指定一个偏好的节点，command执行时内存优先从这个节点分配，不够的话才从别的节点分配。
其他还有更多参数，读者可以通过“man numactl”命令了解。
我们还是通过一个简单的例子来了解一下numactl的一般用法。
通过--hardware，我们可以看到，笔者系统上有两个NUMA节点，相互间的distance为21。

```
[root@kvm-host ~]# numactl --hardware￼
available: 2 nodes (0-1)￼
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65￼
node 0 size: 65439 MB￼
node 0 free: 43948 MB￼
node 1 cpus: 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87￼
node 1 size: 65536 MB￼
node 1 free: 53230 MB￼
node distances:￼
node   0   1 ￼
  0:  10  21 ￼
  1:  21  10
```

我们用numactl来控制启动一个客户机，让它只运行在节点1上，然后通过numastat来确认。可以看到，这个qemu-system-x86_64进程，一开始就被绑定到了节点1上。￼

```
[root@kvm-host ~]# numactl --membind=1 --cpunodebind=1 -- qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot --daemonize￼
[root@kvm-host ~]# numastat qemu-system￼
￼
Per-node process memory usage (in MBs) for PID 72511 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                         0.00           23.03           23.03￼
Stack                        0.00           10.41           10.41￼
Private                      0.00          979.96          979.96￼
----------------  --------------- --------------- ---------------￼
Total                        0.00         1013.41         1013.41
```

我们如果想让客户机均匀地占用两个节点的资源，可以使用--interleave参数。

```
[root@kvm-host ~]# numactl --interleave=0,1 -- qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot --daemonize￼
[root@kvm-host ~]# numastat qemu-system￼
￼
Per-node process memory usage (in MBs) for PID 73119 (qemu-system-x86)￼
                           Node 0          Node 1           Total￼
                  --------------- --------------- ---------------￼
Huge                         0.00            0.00            0.00￼
Heap                        11.35           11.55           22.89￼
Stack                        6.12            2.13            8.25￼
Private                    470.18          467.83          938.01￼
----------------  --------------- --------------- ---------------￼
Total                      487.65          481.50          969.15
```

综上，我们介绍了numastat、numad、numactl三个常用的NUMA控制、查看的工具。numad可以在程序（包括客户机的QEMU进程）起来以后，事后调节、优化其NUMA资源。numactl则是在一开始就指定好这个命令的NUMA政策。numastat则可以用来方便地查看当前系统或某个进程的NUMA资源使用分布情况。

在想要专属地让某个客户机得到优先服务的时候，我们可以把KSM关闭，通过numactl将客户机QEMU进程绑定在某个node或某些CPU上。当我们想要更高的客户机密度，而不考虑特别的服务质量的时候，我们可以通过numactl--all，同时打开KSM，关掉numad。
