
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 大页的介绍](#1-大页的介绍)
	* [1.1 大页的优点](#11-大页的优点)
	* [1.2 内核的支持](#12-内核的支持)

<!-- /code_chunk_output -->

# 1 大页的介绍

x86（包括x86\-32和x86\-64）架构的CPU**默认使用4KB大小的内存页面**，但是它们也支持较大的内存页，如x86\-64系统就支持2MB￼及1GB￼大小的大页（Huge Page）。Linux 2.6及以上的内核都支持Huge Page。

## 1.1 大页的优点

如果在系统中使用了Huge Page，则内存页的数量会减少，从而需要更少的页表（Page Table），节约了页表所占用的内存数量，并且所需的地址转换也减少了，TLB缓存失效的次数就减少了，从而提高了内存访问的性能。

另外，由于**地址转换所需的信息**一般保存在**CPU的缓存**中，Huge Page的使用让**地址转换信息减少**，从而**减少了CPU缓存**的使用，**减轻了CPU缓存的压力**，让CPU缓存能**更多**地用于应用程序的**数据缓存**，也能够在整体上提升系统的性能。

## 1.2 内核的支持

**编译内核**时候，下面这些Config选项与Huge Page相关，需要在内核配置文件中使能。

```
CONFIG_HUGETLBFS=y￼
CONFIG_HUGETLB_PAGE=y
```

内核**启动**时候的**参数**中，与大页相关的如下：
- **default\_hugepagesz**，表示**默认的大页的大小**，可以是2MB或者1GB。
- **hugepages**，表示内核启动后给系统准备的**大页的数量**。
- **hugepagesz**，表示内核启动后给系统准备的**大页的大小**，可以是2MB或者1GB。

hugepages和hugepagesz可以**组合交替出现**，表示**不同大小的大页**分别准备多少。

如下，我们以hugepages=64、hugepagesz=1G、hugepages=16组合启动内核，故意省略掉第一组中hugepagesz的设置，可以看到，前面**不指定hugepagesz**的情况，分配的**64个大页是2MB**的，后面指定**1GB大小**，分配**16个大页**也是成功的。

```
[root@kvm-host ~]# cat /proc/cmdline￼
BOOT_IMAGE=/vmlinuz-3.10.0-514.el7.x86_64 root=/dev/mapper/rhel-root ro crashkernel= auto rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet LANG=en_US.UTF-8 intel_iommu=on hugepages=64 hugepagesz=1G hugepages=16￼

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼
64￼

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages￼
16
```

内核启动之后，系统大页信息主要从以下面几个处查看与配置。

（1）/proc/meminfo里面的一些信息
/proc/meminfo里面有这些信息：HugePages_Total，HugePages_Free，HugePages_Rsvd，HugePages_Surp，Hugepagesize。但要注意，它只体现“default_hugepagesz”的huge page信息。如按笔者上面的启动项启动后，查看/proc/meminfo，发现1GB的Huge Page信息并没有体现在这里，如下所示。

[root@kvm-host ~]# cat /proc/meminfo￼ HugePages_Total:      64￼ HugePages_Free:       64￼ HugePages_Rsvd:        0￼ HugePages_Surp:        0￼ Hugepagesize:       2048 kB

（2）/proc/sys/vm/下面的一些选项
在/proc/sys/vm目录下我们可以看到一些与大页相关的节点文件，通过读取它们的值（cat命令），就可以获得当前系统中与大页相关的实时信息。

[root@kvm-host ~]# ls -l /proc/sys/vm/*huge*￼ -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/hugepages_treat_as_movable￼ -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/hugetlb_shm_group￼ -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_hugepages￼ -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_hugepages_mempolicy￼ -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_overcommit_hugepages

这些主要参数介绍如下：
·hugepages_treat_as_movable：这个参数设置为非0时，表示允许hugepage从ZONE_MOVABLE￼（前面内存热插拔里提到过）里面分配。但这有利有弊：虽然这扩大了hugepage的来源，但它也增加了内存热拔出时的失败几率，如果当时这个hugepage正在被用，哪怕只用到了一小块，它也不能当时被换出，这样整个内存条也就不能拔出。更极端的情况：系统中没有其他的大页可以供这个大页的内容迁移，则内存热拔出一直会失败。所以，这个参数默认是0。
·hugetlb_shm_group：SysV共享内存段（shared memory segment）的id，该内存段将使用大页。我们常用的POSIX共享内存的方法用不到这个。
·nr_hugepages_mempolicy：通常与NUMA mempolicy（内存策略）相关。后面详细讲到。
·nr_hugepages：就是大页的数目。
·nr_overcommit_hugepages：在nr_hugepages数目以外，在nr_hugepages数目不够的时候，还可以动态补充多少大页。
读者可以这样去理解：大页资源放在一个池（huge page pool）中，nr_hugepages指定的就是恒定的大页数目（persistent hugepage），它们是不会被拆分成小页的，即使系统需要小页的时候。而在persistent hugepage不够的时候，系统可以向大页池中补充大页，这些补充的大页由系统中空闲且物理连续的小页拼凑而来，最多允许拼凑nr_overcommit_hugepages个大页。当这些额外拼凑来的大页后来被释放出来的时候，它们又会被解散成小页，释放回小页池。
（3）/sys/kernel/mm/hugepages/目录
其实，现在的kernel已经逐渐废弃通过/proc文件系统去配置大页￼，而转向/sys文件系统。在/sys/kernel/mm/hugepages目录下，我们可以看到上面两种方式重复的地方，同时又有比它们更详尽的地方。
首先，/sys/kernel/mm/hugepages目录下按不同大页大小分子目录。如笔者环境中，配置了2MB和1GB的大页。

[root@kvm-host ~]# ls /sys/kernel/mm/hugepages/￼ hugepages-1048576kB hugepages-2048kB

每个目录下面会有这些文件free_hugepages、nr_hugepages、nr_hugepages_mempolicy、nr_overcommit_hugepages、resv_hugepages、surplus_hugepages，其中nr_hugepages、nr_hugepages_mempolicy、nr_overcommit_hugepages在前面已经介绍过了。
·free_hugepages，大页池中有多少空闲的大页。
·resv_hugepages，被保留的暂时未分配出去的大页。
·surplus_hugepages，具体的额外分配来的大页（nr_overcommit_hugepages指示的是这个数目的上限）。
上述6个/sys FS的控制文件中，nr_hugepages、nr_hugepages_mempolicy、nr_over-commit_hugepages是可读可写的，也就是说，可以通过它们在系统起来以后动态地调整大页池的大小。前面提到的内核启动参数hugepages等，是指示系统起来时候预分配大页池。起来以后，因为存在内存碎片，分配大页成功机会没有系统启动时候大。
我们以2MB大页为例，当前池中是有64个大页，我们可以把它动态地调整为128。具体的设置如下所示：

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages￼ 64￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼ 64￼ [root@kvm-host ~]# echo 128 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages￼ 128￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼ 128

7.1.2　KVM虚拟化对大页的利用
以上介绍了大页是什么，以及在系统层面如何配置大页。那么应用程序是如何能利用上大页？KVM虚拟化是如何利用大页的呢？
操作系统之上的应用程序（包括QEMU创建的客户机）要利用上大页，无非下面3种途径之一。
·mmap系统调用使用MAP_HUGETLB flag创建一段内存映射。
·shmget系统调用使用SHM_HUGETLB flag创建一段共享内存区。
·使用hugetlbfs创建一个文件挂载点，这个挂载目录之内的文件就都是使用大页的了￼。
在KVM虚拟化环境中，就是使用第3种方法：创建hugetlbfs的挂载点，通过“-mem-path FILE”选项，让客户机使用宿主机的huge page挂载点作为它内存的backend。另外，还有一个参数“-mem-prealloc”是让宿主机在启动客户机时就全部分配好客户机的内存，而不是在客户机实际用到更多内存时才按需分配。-mem-prealloc必须在有“-mem-path”参数时才能使用。提前分配好内存的好处是客户机的内存访问速度更快，缺点是客户机启动时就得到了所有的内存，从而使得宿主机的内存很快减少（而不是根据客户机的需求而动态地调整内存分配）。
下面我们就以1GB大页为例（2MB的类似，读者自行实验），说明怎样让KVM客户机用上大页。因为RHEL 7.3自带的3.10内核mount-t hugetlbfs-o min_size参数的要求，为了向读者展示它对reserve huge page的作用，本节使用较新的4.9.6内核。
1）检查硬件是否支持大页。检查方法如下：

[root@kvm-host ~]# cat /proc/cpuinfo | grep flags | uniq | grep pdpe1gb | grep pae | grep pse￼ flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts

目前，几乎所有的Intel处理器都有上述对大页的硬件支持。如果没有硬件的支持，内核将无法使用大页。
2）检查系统是否支持hugetlbfs（7.1.1节提到的kernel Config选项要打开）。

[root@kvm-host ~]# cat /proc/filesystems | grep hugetlbfs￼ nodev hugetlbfs

如果没有hugetlbfs的支持，虚拟机也是无法使用大页的。这时需要参照7.1.1节打开内核选项并重新编译宿主机内核并重新启动。
3）检查并确保大页池（huge page pool）中有足够大页，本例创建16个1GB大页的pool。系统起来时候已经通过Kernel boot param（hugepages=，hugepagesz=）指定了，那么此时应该已经有了。本例因为启动时候没有指定大页，所以此时才创建大页池。

[root@kvm-host ~]   # cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages￼ 0￼ [root@kvm-host ~]   # echo 16 > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages￼ [root@kvm-host ~]   # cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages￼ 16￼ [root@kvm-host ~]   # cat /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages￼ 16

4）创建hugetlbfs的挂载点。我们指定min_size=4G，可以看到大页池里就给它保留（reserve）了4个1GB大页，虽然还没有正式分配出去。￼

[root@kvm-host ~]# mount -t hugetlbfs -o pagesize=1G,size=8G,min_size=4G nodev /mnt/1G-hugepage/￼ [root@kvm-host ~]# mount | grep /mnt/1G-hugepage￼ nodev on /mnt/1G-hugepage type hugetlbfs (rw,relatime,pagesize=1G,size=8G,min_size=4G)￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages ￼ 16￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages￼ 16￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages ￼ 4

5）启动客户机让其使用hugepage的内存，使用“-mem-path”“-mem-prealloc”参数。可以看到，当客户机创建以后，大页池的空闲页只剩8个，其中包括之前创建hugetlbfs时候预留的4个大页，此时一共被分配出去8个1GB大页给客户机。

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -smp 4 -m 8G -mem-path /mnt/1G-hugepage -mem-prealloc -drive file=./rhel7.img,if=virtio,media=disk -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages ￼ 0￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages ￼ 8￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages￼ 16

至此，如果在客户机中运行的应用程序具备使用Huge Page的能力，那么就可以在客户机中使用Huge Page，从而带来性能的提升。
我们还可以同时创建2MB大页池，创建客户机，并故意给客户机分配内存大于池内persistent huge page数量而出现over commit的现象。相关命令行总结如下：

[root@kvm-host ~]# echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼     2048￼ [root@kvm-host ~]# echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages￼ 2048￼ [root@kvm-host ~]# mkdir /mnt/2M-hugepage￼ [root@kvm-host ~]# mount -t hugetlbfs -o pagesize=2M,size=8G nodev /mnt/2M-hugepage/￼ [root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -smp 4 -m 8G -mem-path /mnt/2M-hugepage/ -mem-prealloc -drive file=./rhel7.img,if=virtio,media=disk -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

这个客户机创建以后，通过下面的命令行可以看到，2MB大页池中的free hugepage为0了，并且surplus hugepage为2048。因为客户机需要8G内存，而大页池中本来只有2048个2MB大页，同时我们事先设置了nr_overcommit_hugepages为2048，即允许它即时拼凑最多2048个大页。

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages￼ 4096￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages ￼ 0￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages ￼ 2048￼ [root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages ￼ 2048

总的来说，对于内存访问密集型的应用，在KVM客户机中使用Huge Page是可以较明显地提高客户机性能的。不过，它也有一个缺点，使用Huge Page的内存不能被换出（swap out），也不能使用ballooning方式自动增长。
