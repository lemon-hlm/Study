[TOC]

# 0 概述

基本原理里面提到kvm虚拟化由用户态程序Qemu和[内核态驱动](http://www.oenhan.com/size-512-slab-kmalloc)kvm配合完成，qemu负责HOST用户态层面进程管理，IO处理等，KVM负责把qemu的部分指令在硬件上直接实现，从[虚拟机](http://www.oenhan.com/sort-optimal-solution)的创建和运行上看，qemu的代码占了流程上的主要部分。

前面讲了KVM内核层创建及初始化虚拟机的一些工作过程，现在讲一下QEMU层的流程以及与KVM内核层的配合过程。

QEMU和KVM是通过IOCTL进行配合的，直接抓住这个线看有kvm\_ioctl、kvm\_vm\_ioctl、kvm\_vcpu\_ioctl、kvm\_device\_ioctl等，他们还都在一个C文件里面。

使用kvm\_ioctl很少了，直接看调用的代码，有KVM\_GET\_VCPU\_MMAP\_SIZE，KVM\_CHECK\_EXTENSION，KVM\_GET\_API\_VERSION，KVM\_CREATE\_VM，KVM\_GET\_SUPPORTED\_CPUID等等，需要记住只有KVM\_CREATE\_VM。

而调用kvm\_vm\_ioctl的函数真是海了去了，需要看的是KVM\_SET\_USER\_MEMORY\_REGION，KVM\_CREATE\_VCPU，KVM\_CREATE\_DEVICE。

所有寄存器的交换信息都是通过kvm\_vcpu\_ioctl，需要记住的操作只有，KVM_RUN。

所有看QEMU和KVM的配合流程如下：

![config](images/4.png)

# 1 QEMU初始化主流程

参考上图分析qemu代码流程：

Qemu层是从vl.c中的main()函数开始的，这里通过在代码中添加一些注释的方式来进行讲解。

```c
int main(int argc, charchar **argv, charchar **envp)  
{  
    ......  
    //注册了Qemu的退出函数
    atexit(qemu_run_exit_notifiers);  
    ......  
    //初始化Qemu的QOM模块
    module_call_init(MODULE_INIT_QOM);
  
    qemu_add_opts(&qemu_drive_opts);//将各种函数指针（也就是操作）集合添加到链表中   
    qemu_add_opts(&qemu_chardev_opts);   
    qemu_add_opts(&qemu_device_opts);   
    qemu_add_opts(&qemu_netdev_opts);   
    qemu_add_opts(&qemu_net_opts);   
    qemu_add_opts(&qemu_rtc_opts);   
    qemu_add_opts(&qemu_global_opts);   
    qemu_add_opts(&qemu_mon_opts);   
    qemu_add_opts(&qemu_trace_opts);   
    qemu_add_opts(&qemu_option_rom_opts);   
    qemu_add_opts(&qemu_machine_opts);   
    qemu_add_opts(&qemu_boot_opts);   
    qemu_add_opts(&qemu_sandbox_opts);   
    qemu_add_opts(&qemu_add_fd_opts);   
    qemu_add_opts(&qemu_object_opts);   
    qemu_add_opts(&qemu_tpmdev_opts);   
    qemu_add_opts(&qemu_realtime_opts);  
     ......   
    init_clocks();//时钟初始化相关   
    rtc_clock = host_clock;   
    ......   
    //初始化Qemu的MACHINE模块
    module_call_init(MODULE_INIT_MACHINE);   
    machine = find_default_machine();  
    ......   
    current_machine = MACHINE(object_new(object_class_get_name(
                        OBJECT_CLASS(machine_class))));  
    ...
    // qemu的内存结构体的初始化
    cpu_exec_init_all();
    ...
    // VCPU的不同型号的模拟
    cpudef_init();//初始化CPU def相关
     ......   
    //日志相关的设置，KVM对外的日志在这里配置
    if (log_mask) {   
        int mask;   
        if (log_file) {   
            qemu_set_log_filename(log_file);   
        }  
        mask = qemu_str_to_log_mask(log_mask);   
        if (!mask) {   
            qemu_print_log_usage(stdout);   
            exit(1);   
        }  
        qemu_set_log(mask);  
     }   
    ......  
    //进行虚拟机模拟器的配置，这里重点注意，它内部调用了accel_list[i].init()函数
    configure_accelerator();
    ......       
    current_machine->ram_size = ram_size;
    current_machine->maxram_size = maxram_size;
    current_machine->ram_slots = ram_slots;
    current_machine->boot_order = boot_order;
    current_machine->cpu_model = cpu_model;
    machine_class->init(current_machine);
    return 0;  
}  

/* 
    for (i = 0; i < ARRAY_SIZE(accel_list); i++) { 
        if (strcmp(accel_list[i].opt_name, buf) == 0) { 
            if (!accel_list[i].available()) { 
                printf("%s not supported for this target\n",accel_list[i].name); 
                continue; 
            } 
            *(accel_list[i].allowed) = true; 
            ret = accel_list[i].init(); 
            if (ret < 0) { 
                init_failed = true; 
                fprintf(stderr, "failed to initialize %s: %s\n",accel_list[i].name,strerror(-ret)); 
               *(accel_list[i].allowed) = false;         
            } else {             
                accel_initialised = true;        
            }        
            break;    
         } 
     }  
    //accel_list定义如下，实际上在kvm平台，我们就关注kvm_init即可。     
    static struct { 
        const char *opt_name;   
        const char *name;   
        int (*available)(void); 
        int (*init)(void); 
        bool *allowed;  
    } accel_list[] = { 
       { "tcg", "tcg", tcg_available, tcg_init, &tcg_allowed }, 
       { "xen", "Xen", xen_available, xen_init, &xen_allowed }, 
       { "kvm", "KVM", kvm_available, kvm_init, &kvm_allowed }, 
       { "qtest", "QTest", qtest_available, qtest_init, &qtest_allowed },  
    };     
    //kvm_init函数内首先打开用于用户层以及内核层交互的字符设备文件/dev/kvm，
    然后通过kvm_ioctl()与内核进行交互，比如KVM_GET_API_VERSION，KVM_CREATE_VM等命令，
    其中KVM_CREATE_VM命令创建虚拟机并获得虚拟机句柄，
    后续kvm_arch_init()、 kvm_irqchip_create()等函数
    就可以通过kvm_vm_ioctl系统调用进行更进一步的一些配置。
    这些系统调用实际上是传递到内核层，由内核来完成相应的操作并返回到用户层，
    内核层的相关函数很多就是前一篇文章注册过的函数指针。 
*/
```

## 1.1 atexit()注册QEMU退出处理函数

atexit(qemu\_run\_exit\_notifiers)注册了qemu的退出处理函数，后面再具体看qemu\_run\_exit\_notifiers函数。

## 1.2 module\_call\_init机制

module\_call\_init则开始**初始化qemu的各个模块(！！！**)，陆陆续续的有**以下参数(！！！后续都会调用到并进行初始化**)：

```c
typedef enum { 
    MODULE_INIT_BLOCK, 
    MODULE_INIT_MACHINE, 
    MODULE_INIT_QAPI, 
    MODULE_INIT_QOM, 
    MODULE_INIT_MAX 
} module_init_type;
```

最开始初始化的**MODULE\_INIT\_QOM**，**QOM**是qemu实现的一种**模拟设备**，具体可以参考 http://wiki.qemu.org/Features/QOM ，代码下面的不远处就**MODULE\_INIT\_MACHINE**的**初始化**，这两条语句放到一起看，直接说一下**module\_call\_init的机制**。 

module\_call\_init实际上是设计了一个**函数链表ModuleTypeList**，参数作为一个**Type**，**相关的函数注册到这个函数链表**上，然后**内部通过调用e\-\>init**()函数完成**所有Type相关的设备的初始化**。

**ModuleTypeList**的链表关系如下图

![config](images/5.png)

它把**相关的函数**注册到**对应的数组链表**上，通过**执行init**项目完成**所有设备的初始化**。

```c
[util/module.c]
void module_call_init(module_init_type type)
{ 
    ModuleTypeList *l; 
    ModuleEntry *e; 
  
    module_load(type);
    l = find_type(type);
  
    QTAILQ_FOREACH(e, l, node) {
        e->init();
    }
}
```

**module\_call\_init**就是执行**e\-\>init**()完成功能的.

而e\-\>init是什么时候通过**register\_module\_init(！！！**)注册到**ModuleTypeList**上的**ModuleEntry**？？是通过**module\_init注册**的, 而**调用module\_init**的有

```
[include/qemu/module.h]
#define block_init(function) module_init(function, MODULE_INIT_BLOCK)
#define machine_init(function) module_init(function, MODULE_INIT_MACHINE)
#define qapi_init(function) module_init(function, MODULE_INIT_QAPI)
#define type_init(function) module_init(function, MODULE_INIT_QOM)
```

那么执行**machine\_init**则是**挂到了MODULE\_INIT\_MACHINE**，**type\_init**则将函数挂载了**MODULE\_INIT\_QOM**。

### 1.2.1 PC的注册

那么排查一下是，我们**只关注PC的注册**，那么就是**machine\_init(pc\_machine\_init\_\#\#suffix**)，源自**DEFINE\_PC\_MACHINE(suffix, namestr, initfn, optsfn)宏**会调用

```c
[hw/i386/pc_piix.c]
#define DEFINE_I440FX_MACHINE(suffix, name, compatfn, optionfn) \
    static void pc_init_##suffix(MachineState *machine) \
    { \
        void (*compat)(MachineState *m) = (compatfn); \
        if (compat) { \ 
            compat(machine); \
        } \
        pc_init1(machine); \
    } \
    DEFINE_PC_MACHINE(suffix, name, pc_init_##suffix, optionfn)

[include/hw/i386/pc.h]
#define DEFINE_PC_MACHINE(suffix, namestr, initfn, optsfn) \
    static void pc_machine_##suffix##_class_init(ObjectClass *oc, void *data) \
    { \
        MachineClass *mc = MACHINE_CLASS(oc); \
        optsfn(mc); \
        mc->name = namestr; \
        mc->init = initfn; \
    } \
    static const TypeInfo pc_machine_type_##suffix = { \
        .name       = namestr TYPE_MACHINE_SUFFIX, \
        .parent     = TYPE_PC_MACHINE, \
        .class_init = pc_machine_##suffix##_class_init, \
    }; \
    static void pc_machine_init_##suffix(void) \
    { \
        type_register(&pc_machine_type_##suffix); \
    } \
    machine_init(pc_machine_init_##suffix)
```

**DEFINE\_PC\_MACHINE**注册的函数**pc\_init\_\#\#suffix**在**DEFINE\_I440FX\_MACHINE中定义**，怎么组合都无关，**pc\_init1(machine**)函数一定要执行，本质就是**pc\_init1赋值给了mc\-\>init**。

### 1.2.2 module\_init

而**module\_init**的宏是

```c
[include/qemu/module.h]
#define module_init(function, type) \
static void __attribute__((constructor)) do_qemu_init_ ## function(void) \
{ \
    register_dso_module_init(function, type); \
} \
#else \
/* This should not be used directly.  Use block_init etc. instead.  */
#define module_init(function, type) \
static void __attribute__((constructor)) do_qemu_init_ ## function(void) \
{ \
    register_module_init(function, type); \
} \
#endif
```

它前面的**修饰是\_\_attribute\_\_((constructor**)),这个导致**machine\_init(！！！**)或者**type\_init(！！！**)等会在**main()之前就被执行(！！！**)。

#### 1.2.2.1 type\_init

\[kvm\-all.c\]

所有**type\_init**(**kvm\_type\_init**）\-\> **kvm\_accel\_type** \-\> **kvm\_accel\_class\_init** \-\> **kvm\_init**依次完成了**函数注册**.

所以说**module\_call\_init(MODULE\_INIT\_QOM**)函数**已经完成了kvm\_init的执行(！！！**)，所以这样就**清楚KVM调用关系**了。

#### 1.2.2.2 kvm\_init()

如此就先去看**kvm\_init**函数，

前面主要干了一件事，**填充KVMState \*s**结构体，

然后通过**kvm\_ioctl**(s, KVM\_GET\_API\_VERSION, 0)判断**内核KVM**驱动和**当前QEMU版本**是否兼容，

下面则是执行*kvm\_ioctl*(s, **KVM\_CREATE\_VM**, type)进行**虚拟机的创建**活动，**创建了KVM虚拟机**，获取虚拟机句柄。具体KVM\_CREATE\_VM在内核态做了什么，ioctl的工作等另外再说.

现在假定KVM\_CREATE\_VM所代表的虚拟机创建成功，下面通过**检查kvm\_check\_extension**结果填充KVMState，**kvm\_arch\_init初始化KVMState**，其中有IDENTITY\_MAP\_ADDR，TSS\_ADDR，NR\_MMU\_PAGES等，**cpu\_register\_phys\_memory\_client**注册qemu对**内存管理的函数集**

kvm\_create\_irqchip创建kvm**中断管理内容**，通过kvm\_vm\_ioctl(s, **KVM\_CREATE\_IRQCHIP**)实现，具体内核态的工作内容后面分析。

到此kvm\_init的工作就完成了，最主要的工作就是创建的虚拟机。
 
## 1.3 machine\_class

这样绕了这么大圈，重新回到vl.c上面来，前面刚说了**module\_call\_init(MODULE\_INIT\_MACHINE**)本质就是把**pc\_init1**赋值给了**mc\-\>init**，然后machine\_class = **find\_default\_machine**()，如此可以看到**machine\_class的init**函数一定会执行**pc\_init1(！！！**)。

### 1.3.1 current\_machine

下面涉及对OPT入参的解析过程略过不提。 qemu准备模拟的[机器的类型](http://oenhan.com/cgroup-src-1)从下面语句获得:

```c
current_machine = MACHINE(object_new(object_class_get_name(
                    OBJECT_CLASS(machine_class))));
```

machine\_class则是通过**入参传入**的

```c
            case QEMU_OPTION_machine:
                olist = qemu_find_opts("machine");
                opts = qemu_opts_parse_noisily(olist, optarg, true);
                if (!opts) {
                    exit(1);
                }
                break;
```

man qemu

```c
       -machine [type=]name[,prop=value[,...]]
           Select the emulated machine by name.
           Use "-machine help" to list available machines
```

## 1.4 pc\_init1函数

下面有**cpu\_exec\_init\_all**就是执行了qemu的**内存结构体的初始化**而已，**cpudef\_init**()则提供了**VCPU的不同型号的模拟**，qemu\_set\_log设置日志输出，**kvm对外的日志**是从**这里配置**的。

中间的乱七八糟的就忽略掉即可，然后直接到了**machine\_class\->init(current\_machine**)函数，其实就是执行了**pc\_init1**。暂且记下来，先看下面的，**cpu\_synchronize\_all\_post\_init**就是**内核和qemu数据不一致同步**一下。下面的函数没有重要的了，只有vm\_start()函数需要记一下，后面会用到。

在**pc\_init1**中重点看**两个函数**，**pc\_cpus\_init**和**pc\_memory\_init**，顾名思义，**CPU和内存的初始化**，中断，vga等函数的初始化先忽略掉，先看这两个。

### 1.4.1 CPU初始化

```c
void pc_cpus_init(const charchar *cpu_model, DeviceState *icc_bridge)  
{  
    int i;  
    X86CPU *cpu = NULL;  
    Error *error = NULL;  
  
    /* init CPUs */  
    if (cpu_model == NULL) {  
#ifdef TARGET_X86_64  
        cpu_model = "qemu64";  
#else  
        cpu_model = "qemu32";  
#endif  
    }  
    current_cpu_model = cpu_model;  
  
    for (i = 0; i < smp_cpus; i++) {  
        //对每一个CPU进行初始化及创建
        cpu = pc_new_cpu(cpu_model, x86_cpu_apic_id_from_index(i),  
                         icc_bridge, &error);  
        if (error) {  
            fprintf(stderr, "%s\n", error_get_pretty(error));  
            error_free(error);  
            exit(1);  
        }  
    }  
  
    /* map APIC MMIO area if CPU has APIC */  
    if (cpu && cpu->env.apic_state) {  
        /* XXX: what if the base changes? */  
        sysbus_mmio_map_overlap(SYS_BUS_DEVICE(icc_bridge), 0,  
                                APIC_DEFAULT_ADDRESS, 0x1000);  
    }  
}  
```

**pc\_cpus\_init**入参是**cpu\_model**，前面说过这是**具体的CPU模型**，所有**X86的CPU模型**都在**builtin\_x86\_defs**中定义，取其中一个看看

```c
[target-i386/cpu.c]
    {
        .name = "SandyBridge",
        .level = 0xd,
        .vendor = CPUID_VENDOR_INTEL,
        .family = 6,
        .model = 42,
        .stepping = 1,
        .features[FEAT_1_EDX] =
            CPUID_VME | CPUID_SSE2 | CPUID_SSE | CPUID_FXSR | CPUID_MMX |
            CPUID_CLFLUSH | CPUID_PSE36 | CPUID_PAT | CPUID_CMOV | CPUID_MCA |
            CPUID_PGE | CPUID_MTRR | CPUID_SEP | CPUID_APIC | CPUID_CX8 |
            CPUID_MCE | CPUID_PAE | CPUID_MSR | CPUID_TSC | CPUID_PSE |
            CPUID_DE | CPUID_FP87,
        .features[FEAT_1_ECX] =
            CPUID_EXT_AVX | CPUID_EXT_XSAVE | CPUID_EXT_AES |
            CPUID_EXT_TSC_DEADLINE_TIMER | CPUID_EXT_POPCNT |
            CPUID_EXT_X2APIC | CPUID_EXT_SSE42 | CPUID_EXT_SSE41 |
            CPUID_EXT_CX16 | CPUID_EXT_SSSE3 | CPUID_EXT_PCLMULQDQ |
            CPUID_EXT_SSE3,
        .features[FEAT_8000_0001_EDX] =
            CPUID_EXT2_LM | CPUID_EXT2_RDTSCP | CPUID_EXT2_NX |
            CPUID_EXT2_SYSCALL,
        .features[FEAT_8000_0001_ECX] =
            CPUID_EXT3_LAHF_LM,
        .features[FEAT_XSAVE] =
            CPUID_XSAVE_XSAVEOPT,
        .features[FEAT_6_EAX] =
            CPUID_6_EAX_ARAT,
        .xlevel = 0x80000008,
        .model_id = "Intel Xeon E312xx (Sandy Bridge)",
    },
```

你可以cat一个**本地的/proc/cpuinfo**，**builtin\_x86\_defs定义的就是这些参数**。

然后是**for循环**中针对**每个CPU初始化(！！！**)，即**pc\_new\_cpu**()，直接进入**cpu\_x86\_create**函数，主要就是把**CPUX86State填充**了一下，涉及到CPUID和其他的feature。

下面是**x86\_cpu\_realize**，即**唤醒CPU**，重点是**qemu\_init\_vcpu**，MCE忽略掉，走到**qemu\_kvm\_start\_vcpu**()，qemu创建VCPU，如下：

```c
    //创建VPU对于的qemu线程，线程函数是qemu_kvm_cpu_thread_fn
    qemu_thread_create(cpu->thread, thread_name, qemu_kvm_cpu_thread_fn,
                       cpu, QEMU_THREAD_JOINABLE);
    //如果线程没有创建成功，则一直在此处循环阻塞。说明多核vcpu的创建是顺序的
    while (!cpu->created) {
        qemu_cond_wait(&qemu_cpu_cond, &qemu_global_mutex);
    }
```

**线程创建完成**，具体任务支线提，回到**主流程**上，**qemu\_init\_vcpu执行完成**后，下面就是**cpu\_reset**，此处的作用是什么呢？答案是无用，本质是一个空函数，它的主要功能就是CPUClass的reset函数，reset在cpu\_class\_init里面注册的，注册的是cpu\_common\_reset，这是一个空函数，没有任何作用。**cpu\_class\_init**则是被**cpu\_type\_info即TYPE\_CPU**使用，而**cpu\_type\_info**则由type\_init(cpu\_register\_types)完成，type\_init则是前面提到的和machine\_init对应的注册关系。根据下句完成工作

```c
#define type_init(function) module_init(function, MODULE_INIT_QOM)
```

#### 1.4.1.1 vcpu线程

从上面看，**pc\_cpus\_init**函数过程已经理顺了，下面看一下，**vcpu所在的线程**对应的**qemu\_kvm\_cpu\_thread\_fn**中：

```c
//初始化VCPU
    r = kvm_init_vcpu(env);
//初始化KVM中断
    qemu_kvm_init_cpu_signals(env);

//标志VCPU创建完成，和上面判断是对应的
    cpu->created = true;
    qemu_cond_signal(&qemu_cpu_cond);
    while (1) {
        if (cpu_can_run(env)) {
          //CPU进入执行状态
            r = kvm_cpu_exec(env);
            if (r == EXCP_DEBUG) {
                cpu_handle_guest_debug(env);
            }
        }
        qemu_kvm_wait_io_event(env);
    }
```

**CPU进入执行状态**的时候我们看到**其他的VCPU**包括**内存可能还没有初始化**，关键是**此处有一个开关**，**qemu\_cpu\_cond**, **打开这个开关才能进入到CPU执行状态(！！！**)，谁来打开这个开关，后面再说。

```c
int kvm_init_vcpu(CPUState *cpu)  
{  
    KVMState *s = kvm_state;  
    long mmap_size;  
    int ret;  
  
    DPRINTF("kvm_init_vcpu\n");  
    //通过ioctl调用向内核发起创建CPU请求，内核完成相关工作
    ret = kvm_vm_ioctl(s, KVM_CREATE_VCPU, (voidvoid *)kvm_arch_vcpu_id(cpu));  
    if (ret < 0) {  
        DPRINTF("kvm_create_vcpu failed\n");  
        goto err;  
    }  
  
    cpu->kvm_fd = ret;  
    cpu->kvm_state = s;  
    cpu->kvm_vcpu_dirty = true;  
    //通过ioctl调用获取内核与用户层共享的内存大小，这部分内存以内存映射的方式进行共享
    mmap_size = kvm_ioctl(s, KVM_GET_VCPU_MMAP_SIZE, 0);  
    if (mmap_size < 0) {  
        ret = mmap_size;  
        DPRINTF("KVM_GET_VCPU_MMAP_SIZE failed\n");  
        goto err;  
    }  
    //获取内存大小后，用户层进行共享内存映射
    cpu->kvm_run = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED,  
                        cpu->kvm_fd, 0);  
    if (cpu->kvm_run == MAP_FAILED) {  
        ret = -errno;  
        DPRINTF("mmap'ing vcpu state failed\n");  
        goto err;  
    }  
    //mmio相关的一部分共享内存设置
    if (s->coalesced_mmio && !s->coalesced_mmio_ring) {    
        s->coalesced_mmio_ring =  
            (voidvoid *)cpu->kvm_run + s->coalesced_mmio * PAGE_SIZE;  
    }  
  
    ret = kvm_arch_init_vcpu(cpu);//相关初始化  
    if (ret == 0) {  
        qemu_register_reset(kvm_reset_vcpu, cpu);  
        kvm_arch_reset_vcpu(cpu);  
    }  
err:  
    return ret;  
}
```

先看**kvm\_init\_vcpu**，通过kvm\_vm\_ioctl，KVM\_CREATE\_VCPU**创建VCPU**，用KVM\_GET\_VCPU\_MMAP\_SIZE获取**env\-\>kvm\_run对应的内存映射**，**kvm\_arch\_init\_vcpu**则**填充对应的kvm\_arch**内容，具体内核部分，后面单独写。

kvm\_init\_vcpu就是**获取了vcpu**，将**相关内容填充了env**。

继续qemu\_kvm\_cpu\_thread\_fn, **qemu\_kvm\_init\_cpu\_signals**则是将**中断组合掩码**传递给kvm\_set\_signal\_mask，最终给**内核KVM\_SET\_SIGNAL\_MASK**。kvm\_cpu\_exec此时还在阻塞过程中，先挂起来，看内存的初始化。

### 1.4.2 内存初始化

**内存初始化**函数是**pc\_memory\_init**, **memory\_region\_init\_ram**传入了**高端内存**和**低端内存**的值，memory\_region\_init负责**填充mr**，重点在**qemu\_ram\_alloc**，即**qemu\_ram\_alloc\_from\_ptr**，首先有**RAMBlock**，ram\_list，那就直接借助find\_ram\_offset函数一起看一下qemu的内存分布模型。

![config](./images/6.png)

qemu模拟了**普通内存分布模型**，内存的线性也是**分块被使用**的，每个块称为**RAMBlock**，由ram\_list统领，RAMBlock.offset则是区块的线性地址，即相对于开始的偏移位，RAMBlock.length(size)则是区块的大小，find\_ram\_offset则是在线性区间内找到没有使用的一段空间，可以完全容纳新申请的ramblock length大小，代码就是进行了所有区块的遍历，找到满足新申请length的最小区间，把ramblock安插进去即可，返回的offset即是新分配区间的开始地址。

而RAMBlock的物理则是在RAMBlock.host,由kvm_vmalloc(size)分配真正物理内存，内部qemu_vmalloc使用qemu_memalign页对齐分配内存。后续的都是对RAMBlock的插入等处理。

从上面看，memory_region_init_ram已经将qemu内存模型和实际的物理内存初始化了。

vmstate_register_ram_global这个函数则是负责将前面提到的ramlist中的ramblock和memory region的初始地址对应一下，将mr->name填充到ramblock的idstr里面，就是让二者有确定的对应关系，如此mr就有了物理内存使用。

后面则是subregion的处理，memory_region_init_alias初始化，其中将ram传递给mr->owner确定了隶属关系，memory_region_add_subregion则是大头，memory_region_add_subregion_common前面的判断忽略，QTAILQ_INSERT_TAIL(&mr->subregions, subregion, subregions_link)就是插入了链表而已，主要内容在memory_region_transaction_commit。

memory_region_transaction_commit中引入了新的结构address_spaces（AS），注释里面提到“AddressSpace: describes a mapping of addresses to #MemoryRegion objects”，就是内存地址的映射关系，因为内存有不同的应用类型，address_spaces以链表形式存在，commit函数则是对所有AS执行address_space_update_topology，先看AS在哪里注册的，就是前面提到的kvm_init里面，执行memory_listener_register，注册了address_space_memory和address_space_io两个，涉及的另外一个结构体则是MemoryListener，有kvm_memory_listener和kvm_io_listener，就是用于监控内存映射关系发生变化之后执行回调函数。

下面进入到address_space_update_topology函数，FlatView则是“Flattened global view of current active memory hierarchy”，address_space_get_flatview直接获取当前的，generate_memory_topology则根据前面已经变化的mr重新生成FlatView,然后通过address_space_update_topology_pass比较，简单说address_space_update_topology_pass就是两个FlatView逐条的FlatRange进行对比，以后一个FlatView为准，如果前面FlatView的FlatRange和后面的不一样，则对前面的FlatView的这条FlatRange进行处理，差别就是3种情况，如代码：

```c
 while (iold < old_view->nr || inew < new_view->nr) {
        if (iold < old_view->nr) {
            frold = &old_view->ranges[iold];
        } else {
            frold = NULL;
        }
        if (inew < new_view->nr) {
            frnew = &new_view->ranges[inew];
        } else {
            frnew = NULL;
        }

        if (frold
            && (!frnew
                || int128_lt(frold->addr.start, frnew->addr.start)
                || (int128_eq(frold->addr.start, frnew->addr.start)
                    && !flatrange_equal(frold, frnew)))) {
            /* In old but not in new, or in both but attributes changed. */

            if (!adding) { //这个判断代码添加的无用，可以直接删除,
                //address_space_update_topology里面的两个pass也可以删除一个
                MEMORY_LISTENER_UPDATE_REGION(frold, as, Reverse, region_del);
            }

            ++iold;
        } else if (frold && frnew && flatrange_equal(frold, frnew)) {
            /* In both and unchanged (except logging may have changed) */

            if (adding) {
                MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_nop);
                if (frold->dirty_log_mask && !frnew->dirty_log_mask) {
                    MEMORY_LISTENER_UPDATE_REGION(frnew, as, Reverse, log_stop);
                } else if (frnew->dirty_log_mask && !frold->dirty_log_mask) {
                    MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, log_start);
                }
            }

            ++iold;
            ++inew;
        } else {
            /* In new */

            if (adding) {
                MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_add);
            }

            ++inew;
        }
    }
```

重点在MEMORY_LISTENER_UPDATE_REGION函数上，将变化的FlatRange构造一个MemoryRegionSection，然后遍历所有的memory_listeners，如果memory_listeners监控的内存区域和MemoryRegionSection一样，则执行第四个入参函数，如region_del函数，即kvm_region_del函数，这个是在kvm_init中初始化的。kvm_region_del主要是kvm_set_phys_mem函数，主要是将MemoryRegionSection有效值转换成KVMSlot形式，在kvm_set_user_memory_region中使用kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem)传递给kernel。
我们看内存初始化真正需要做的是什么？就是qemu申请内存，把申请物理地址传递给kernel进行映射，那我们直接就可以KVMSlot申请内存，然后传递给kvm_vm_ioctl，这样也是OK的，之所以有这么多代码，因为qemu本身是一个软件虚拟机，mr涉及的地址已经是vm的地址，对于KVM是多余的，只是方便函数复用而已。

内存初始化之后还是pci等处理先跳过，如此pc\_init就完成了，但是前面VM线程已经初始化成功，在qemu\_kvm\_cpu\_thread\_fn函数中等待运行：

```c
    while (1) {
        if (cpu_can_run(cpu)) {
            r = kvm_cpu_exec(cpu);
            if (r == EXCP_DEBUG) {
                cpu_handle_guest_debug(cpu);
            }
        }
        qemu_kvm_wait_io_event(cpu);
    }
```

判断条件就是cpu\_can\_run函数，即cpu\-\>stop && cpu\-\>stopped && current\_run\_state ！= running 都是false，而这几个参数都是由**vm\_start函数决定**的

```c
void vm_start(void)
{
    if (!runstate_is_running()) {
        cpu_enable_ticks();
        runstate_set(RUN_STATE_RUNNING);
        vm_state_notify(1, RUN_STATE_RUNNING);
        resume_all_vcpus();
        monitor_protocol_event(QEVENT_RESUME, NULL);
    }
}
```

如此kvm\_cpu\_exec就真正进入执行阶段，即通过kvm\_vcpu\_ioctl传递KVM\_RUN给内核。







**e\->init**()函数是在**machine\_init(pc\_machine\_init)函数注册**时注册到**ModuleTypeList**的**ModuleEntry**上的，module\_call\_init()针对**X86架构时调用machine\_init**(),随即调用**pc\_machine\_init**()函数，代码如下：

```c
static void pc_machine_init(void)   //在hw/pc_piix.h文件中
{  
    qemu_register_machine(&pc_i440fx_machine_v1_5);  
    qemu_register_machine(&pc_i440fx_machine_v1_4);  
    qemu_register_machine(&pc_machine_v1_3);  
    qemu_register_machine(&pc_machine_v1_2);  
    qemu_register_machine(&pc_machine_v1_1);  
    qemu_register_machine(&pc_machine_v1_0);  
    qemu_register_machine(&pc_machine_v0_15);  
    qemu_register_machine(&pc_machine_v0_14);  
    qemu_register_machine(&pc_machine_v0_13);  
    qemu_register_machine(&pc_machine_v0_12);  
    qemu_register_machine(&pc_machine_v0_11);  
    qemu_register_machine(&pc_machine_v0_10);  
    qemu_register_machine(&isapc_machine);  
#ifdef CONFIG_XEN  
    qemu_register_machine(&xenfv_machine);  
#endif  
}  
  
machine_init(pc_machine_init);  
```

注意这里注册的第一个为**pc\_i440fx\_machine\_v1\_5**，这个结构体定义为下：

```c
static QEMUMachine pc_i440fx_machine_v1_5 = {  
    .name = "pc-i440fx-1.5",  
    .alias = "pc",  
    .desc = "Standard PC (i440FX + PIIX, 1996)",  
    .init = pc_init_pci,  
    .hot_add_cpu = pc_hot_add_cpu,  
    .max_cpus = 255,  
    .is_default = 1,  
    DEFAULT_MACHINE_OPTIONS,  
};  
```

\.init=pc\_init\_pci, **pc\_init\_pci**()即为**初始化时候调用的函数**，一路跟下去，其实最终调到**pc\_init1**()这个函数。再看pc\_init1()这个函数，这里面进行了**内存（pc\_memory\_init**）、**cpu（pc\_cpus\_init**）、**中断**等等多种的初始化，这里不细说，重点看cpu的初始化。

```c
void pc_cpus_init(const charchar *cpu_model, DeviceState *icc_bridge)  
{  
    int i;  
    X86CPU *cpu = NULL;  
    Error *error = NULL;  
  
    /* init CPUs */  
    if (cpu_model == NULL) {  
#ifdef TARGET_X86_64  
        cpu_model = "qemu64";  
#else  
        cpu_model = "qemu32";  
#endif  
    }  
    current_cpu_model = cpu_model;  
  
    for (i = 0; i < smp_cpus; i++) {  
        cpu = pc_new_cpu(cpu_model, x86_cpu_apic_id_from_index(i),//对每一个CPU进行初始化及创建  
                         icc_bridge, &error);  
        if (error) {  
            fprintf(stderr, "%s\n", error_get_pretty(error));  
            error_free(error);  
            exit(1);  
        }  
    }  
  
    /* map APIC MMIO area if CPU has APIC */  
    if (cpu && cpu->env.apic_state) {  
        /* XXX: what if the base changes? */  
        sysbus_mmio_map_overlap(SYS_BUS_DEVICE(icc_bridge), 0,  
                                APIC_DEFAULT_ADDRESS, 0x1000);  
    }  
}  
```

该函数内调用**pc\_new\_cpu**()函数对**每一个CPU进行初始化**，其后依次调用关系为：

![调用关系](images/3.png)

下面来看kvm\_init\_vcpu()函数

```c
int kvm_init_vcpu(CPUState *cpu)  
{  
    KVMState *s = kvm_state;  
    long mmap_size;  
    int ret;  
  
    DPRINTF("kvm_init_vcpu\n");  
    //通过ioctl调用向内核发起创建CPU请求，内核完成相关工作
    ret = kvm_vm_ioctl(s, KVM_CREATE_VCPU, (voidvoid *)kvm_arch_vcpu_id(cpu));  
    if (ret < 0) {  
        DPRINTF("kvm_create_vcpu failed\n");  
        goto err;  
    }  
  
    cpu->kvm_fd = ret;  
    cpu->kvm_state = s;  
    cpu->kvm_vcpu_dirty = true;  
    //通过ioctl调用获取内核与用户层共享的内存大小，这部分内存以内存映射的方式进行共享
    mmap_size = kvm_ioctl(s, KVM_GET_VCPU_MMAP_SIZE, 0);  
    if (mmap_size < 0) {  
        ret = mmap_size;  
        DPRINTF("KVM_GET_VCPU_MMAP_SIZE failed\n");  
        goto err;  
    }  
    //获取内存大小后，用户层进行共享内存映射
    cpu->kvm_run = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED,  
                        cpu->kvm_fd, 0);  
    if (cpu->kvm_run == MAP_FAILED) {  
        ret = -errno;  
        DPRINTF("mmap'ing vcpu state failed\n");  
        goto err;  
    }  
    //mmio相关的一部分共享内存设置
    if (s->coalesced_mmio && !s->coalesced_mmio_ring) {    
        s->coalesced_mmio_ring =  
            (voidvoid *)cpu->kvm_run + s->coalesced_mmio * PAGE_SIZE;  
    }  
  
    ret = kvm_arch_init_vcpu(cpu);//相关初始化  
    if (ret == 0) {  
        qemu_register_reset(kvm_reset_vcpu, cpu);  
        kvm_arch_reset_vcpu(cpu);  
    }  
err:  
    return ret;  
}  
```

回到上面一些列调用中的**qemu\_kvm\_cpu\_thread\_fn**()函数中，在调用了kvm\_init\_vcpu()函数**完成cpu的初始化**之后，又调用**kvm\_cpu\_exec**()函数**运行cpu**，也就是运行了整个虚拟机。

我们来看kvm\_cpu\_exec()这个函数

```c
int kvm_cpu_exec(CPUArchState *env)  
{  
    .......  
    do{   
        run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);  
  
       .......  
        trace_kvm_run_exit(cpu->cpu_index, run->exit_reason);  
        switch (run->exit_reason) {  
        case KVM_EXIT_IO:  
            ......  
            break;  
        case KVM_EXIT_MMIO:  
            ......  
            break;  
        case KVM_EXIT_IRQ_WINDOW_OPEN:  
            ......  
            break;  
        case KVM_EXIT_SHUTDOWN:  
            ......  
            break;  
        case KVM_EXIT_UNKNOWN:  
            ......  
            break;  
        case KVM_EXIT_INTERNAL_ERROR:  
           ......  
            break;  
        default:  
            ......  
            break;  
        }  
    } while (ret == 0);  
  
    .......  
    return ret;  
}  
```

首先一个**kvm\_vcpu\_ioctl系统调用**，向内核请求****运行虚拟机****，然后内核运行虚拟机，kvm\_cpu\_exec()内是有一个**while循环**，只要是没有错误就会不断运行不会终止，后面的switch语句实际上是**接收内核传来的退出原因**，因为**I/O等是需要Qemu即用户层来完成**的，这样虚拟机运行时内核层遇到I/O等就需要退出到Qemu层并记录退出原因，Qemu根据退出原因执行相关操作，完成后再次执行ioctl操作转到内核层继续运行虚拟机。关于异常退出的具体流程后面文章再详细讲解。

关于**内存初始化**相关工作，在之前提到过的**kvm初始化主函数kvm\_init**()函数里，依次调用：

```c
memory_listener_register(&kvm_memory_listener, &address_space_memory);

listener_add_address_space(listener, as);

listener->region_add(listener, &section);

.region_add = kvm_region_add,

kvm_set_phys_mem(section, true);

err = kvm_set_user_memory_region(s, mem);

return kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem);
```

这些函数依次调用，基本完成内存初始化过程，这里最后的**ioctl调用**是设置**影子页表信息**以及设置**页面访问权限**等。

最终，在内核与用户层的配合下完成整个虚拟机的创建和初始化工作，并运行虚拟机。