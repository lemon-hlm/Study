
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 相关配置参数](#1-相关配置参数)
* [2 与内存相关的数据结构](#2-与内存相关的数据结构)
	* [2.1 PCDIMMDevice和 HostMemoryBackend](#21-pcdimmdevice和-hostmemorybackend)
	* [2.2 RAMBlock和RAMList](#22-ramblock和ramlist)
* [跟踪脏页](#跟踪脏页)
	* [AddressSpace和MemoryRegion](#addressspace和memoryregion)

<!-- /code_chunk_output -->

QEMU在虚拟机启动的初始化阶段，为客户机分配了物理内存，那么客户机的物理内存如何工作呢？

本篇文档，为大家介绍客户机物理内存的工作原理、相关数据结构，但不会涉及其实现细节，客户机物理内存的实现细节，会在后面的代码分析中讲述。

# 1 相关配置参数

QEMU的命令行中有参数：

```
-m [size=]megs[,slots=n,maxmem=size] 
```

用于指定客户机初始运行时的内存大小以及客户机最大内存大小，以及内存芯片槽的数量（DIMM）。

之所以QEMU可以指定最大内存、槽等参数，是因为QEMU可以模拟DIMM的热插拔，客户机操作系统可以和在真实的系统上一样，检测新内存被插入或者拔出。也就是说，内存热插拔的粒度是DIMM槽（或者说DIMM集合），而不是最小的byte。

# 2 与内存相关的数据结构

![](./images/2019-06-05-18-53-12.png)

## 2.1 PCDIMMDevice和 HostMemoryBackend

**PCDIMMDevice**和**HostMemoryBackend对象**都是在QEMU中**用户可见**的**客户机内存**。它们能通过**QEMU命令行**或者**QMP监控器接口**来管理。

**PCDIMMDevice数据结构**是使用QEMU中的面向对象编程模型**QOM定义**的，对应的对象和类的数据结构如下。通过**在QEMU进程中**创建一个**新的PCDIMMDevice对象**，就可以实现**内存的热插拔**。

值得注意的是，**客户机启动时**的**初始化内存**，可能**不会被模拟成PCDIMMDevice设备**，也就是说，这部分**初始化内存不能进行热插拔**。PCDIMMDevice的定义在include/hw/mem/pc\-dimm.h中。

```c
// include/hw/mem/pc-dimm.h
typedef struct PCDIMMDevice {
    /* private */
    DeviceState parent_obj;

    /* public */
    uint64_t addr;
    uint32_t node; //numa node
    int32_t slot; //slot编号
    HostMemoryBackend *hostmem;
} PCDIMMDevice;

typedef struct PCDIMMDeviceClass {
    /* private */
    DeviceClass parent_class;

    /* public */
    void (*realize)(PCDIMMDevice *dimm, Error **errp);
    MemoryRegion *(*get_vmstate_memory_region)(PCDIMMDevice *dimm,
                                               Error **errp);
} PCDIMMDeviceClass;
```

**每个PCDIMMDevice对象**都与 **HostMemoryBackend对象相关联**。

HostMemoryBackend也是使用QEMU中的面向对象编程模型QOM定义的。HostMemoryBackend定义在include/sysemu/hostmem.h中。HostMemoryBackend对象包含了**客户机内存对应的真正的主机内存**，这些内存可以是**匿名映射的内存**，也可以是**文件映射内存**。

**文件映射的客户机内存**允许Linux在**物理主机**上**透明大页机制**的使用（hugetlbfs），并且能够**共享内存**，从而使**其他进程可以访问客户机内存**。

```c
// include/sysemu/hostmem.h
struct HostMemoryBackendClass {
    ObjectClass parent_class;

    void (*alloc)(HostMemoryBackend *backend, Error **errp);
};

struct HostMemoryBackend {
    /* private */
    Object parent;

    /* protected */
    uint64_t size;
    bool merge, dump, use_canonical_path;
    bool prealloc, force_prealloc, is_mapped, share;
    DECLARE_BITMAP(host_nodes, MAX_NODES + 1);
    HostMemPolicy policy;

    MemoryRegion mr;
};
```

## 2.2 RAMBlock和RAMList

**HostMemoryBackend对象中的内存**被**实际映射**到通过**qemu\_ram\_alloc**()函数（代码定义在**exec.c**中）**RAMBlock数据结构**中。

**每个RAMBlock**都有一个**指针**指向**被映射内存的起始位置**，同时包含一个**ram\_addr\_t的位移变量**。ram\_addr\_t位于**全局的命名空间**中，因此RAMBlock能够通过**offset来查找**。

RAMBlock定义在include/exec/ram\_addr.h中。RAMBlock受**RCU机制保护**，所谓RCU，即Read\-COPY\-Update，

```c
// include/exec/ram_addr.h
typedef uint64_t ram_addr_t;

struct RAMBlock {
    struct rcu_head rcu; //该数据结构受rcu机制保护
    struct MemoryRegion *mr;
    uint8_t *host;  //RAMBlock的内存起始位置
    uint8_t *colo_cache; /* For colo, VM's ram cache */
    ram_addr_t offset;  //在所有的RAMBlock中offset
    ram_addr_t used_length; //已使用长度
    ram_addr_t max_length;  //最大分配内存
    void (*resized)(const char*, uint64_t length, void *host);
    uint32_t flags;
    /* Protected by iothread lock.  */
    char idstr[256];    //RAMBlock的ID
    /* RCU-enabled, writes protected by the ramlist lock */
    QLIST_ENTRY(RAMBlock) next;
    QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
    int fd; //映射文件的描述符
    size_t page_size;
    /* dirty bitmap used during migration */
    unsigned long *bmap;
    unsigned long *unsentmap;
    /* bitmap of already received pages in postcopy */
    unsigned long *receivedmap;
};
```

所有的**RAMBlock**保存在**全局的RAMBlock的链表**中，名为**RAMList**，它有专门的**数据结构定义**。

**RAMList数据结构**定义在include/exec/ramlist.h中，而全局的ram\_list变量则定义在exec.c中。因此**这个链表**保存了**客户机的内存**对应的**所有的物理机！！！的实际内存信息！！！**。

```cpp
// include/exec/ramlist.h
typedef struct RAMList {
    QemuMutex mutex;
    //最近最常使用的RAMBlock，将其保存，从而能够迅速访问
    RAMBlock *mru_block;
    /* RCU-enabled, writes protected by the ramlist lock. */
    //ram_list的链表
    QLIST_HEAD(, RAMBlock) blocks;
    //用于保存脏页信息的bitmap，有三种bitmap，一种用于VGA，一种用于TCG编程中，一种用于热迁移中。
    DirtyMemoryBlocks *dirty_memory[DIRTY_MEMORY_NUM];
    //全局的ram_list，每更改一次，version+1
    uint32_t version;
    QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
} RAMList;
extern RAMList ram_list;

// exec.c
RAMList ram_list = { .blocks = QLIST_HEAD_INITIALIZER(ram_list.blocks) };
```

# 跟踪脏页

当**客户机CPU**或者**DMA**将**数据**保存到**客户机内存**时，需要通知下列一些用户： 

1. **热迁移**特性依赖于**跟踪脏页**，因此他们能够在**被改变之后重新传输**。 
2. **图形卡模拟**依赖于**跟踪脏的视频内存**，用于**重画某些界面**。

## AddressSpace和MemoryRegion

**所有的CPU架构**都有**内存地址空间**、有些CPU架构又有一个**IO地址空间**。它们在QEMU中被表示为**AddressSpace数据结构**，它定义在include/exec/memory.h中。

而**每个地址空间**都包含一个**MemoryRegion的树状结构**，所谓树状结构，指的是**每个MemoryRegion**的内部可以含有**MemoryRegion**，这样的包含所形成的树状结构。

**MemoryRegion**是联系**客户机内存**和包含这一部分内存的**RAMBlock**。**每个MemoryRegion**都包含一个在**RAMBlock**中ram\_addr\_t类型的**offset**，**每个RAMBlock**也有一个**MemoryRegion的指针**。

**MemoryRegion**不仅可以表示**RAM**，也可以表示**I/O映射内存**，在访问时可以调用read/write回调函数。这也是硬件从客户机CPU注册的访问被分派到相应的模拟设备的方法。








参考

https://blog.csdn.net/u011364612/article/details/51345110