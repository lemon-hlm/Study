
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 相关配置参数](#1-相关配置参数)
* [2 与内存相关的数据结构](#2-与内存相关的数据结构)
	* [2.1 PCDIMMDevice和 HostMemoryBackend](#21-pcdimmdevice和-hostmemorybackend)
	* [2.2 RAMBlock和RAMList](#22-ramblock和ramlist)

<!-- /code_chunk_output -->

QEMU在虚拟机启动的初始化阶段，为客户机分配了物理内存，那么客户机的物理内存如何工作呢？

本篇文档，为大家介绍客户机物理内存的工作原理、相关数据结构，但不会涉及其实现细节，客户机物理内存的实现细节，会在后面的代码分析中讲述。

# 1 相关配置参数

QEMU的命令行中有参数：

```
-m [size=]megs[,slots=n,maxmem=size] 
```

用于指定客户机初始运行时的内存大小以及客户机最大内存大小，以及内存芯片槽的数量（DIMM）。

之所以QEMU可以指定最大内存、槽等参数，是因为QEMU可以模拟DIMM的热插拔，客户机操作系统可以和在真实的系统上一样，检测新内存被插入或者拔出。也就是说，内存热插拔的粒度是DIMM槽（或者说DIMM集合），而不是最小的byte。

# 2 与内存相关的数据结构

![](./images/2019-06-05-18-53-12.png)

## 2.1 PCDIMMDevice和 HostMemoryBackend

**PCDIMMDevice**和**HostMemoryBackend对象**都是在QEMU中**用户可见**的**客户机内存**。它们能通过**QEMU命令行**或者**QMP监控器接口**来管理。

**PCDIMMDevice数据结构**是使用QEMU中的面向对象编程模型**QOM定义**的，对应的对象和类的数据结构如下。通过**在QEMU进程中**创建一个**新的PCDIMMDevice对象**，就可以实现**内存的热插拔**。

值得注意的是，**客户机启动时**的**初始化内存**，可能**不会被模拟成PCDIMMDevice设备**，也就是说，这部分**初始化内存不能进行热插拔**。PCDIMMDevice的定义在include/hw/mem/pc\-dimm.h中。

```c
typedef struct PCDIMMDevice {
    /* private */
    DeviceState parent_obj;

    /* public */
    uint64_t addr;
    uint32_t node; //numa node
    int32_t slot; //slot编号
    HostMemoryBackend *hostmem;
} PCDIMMDevice;

typedef struct PCDIMMDeviceClass {
    /* private */
    DeviceClass parent_class;

    /* public */
    void (*realize)(PCDIMMDevice *dimm, Error **errp);
    MemoryRegion *(*get_vmstate_memory_region)(PCDIMMDevice *dimm,
                                               Error **errp);
} PCDIMMDeviceClass;
```

**每个PCDIMMDevice对象**都与 **HostMemoryBackend对象相关联**。

HostMemoryBackend也是使用QEMU中的面向对象编程模型QOM定义的。HostMemoryBackend定义在include/sysemu/hostmem.h中。HostMemoryBackend对象包含了**客户机内存对应的真正的主机内存**，这些内存可以是**匿名映射的内存**，也可以是**文件映射内存**。

**文件映射的客户机内存**允许Linux在**物理主机**上**透明大页机制**的使用（hugetlbfs），并且能够**共享内存**，从而使**其他进程可以访问客户机内存**。

```c
struct HostMemoryBackendClass {
    ObjectClass parent_class;

    void (*alloc)(HostMemoryBackend *backend, Error **errp);
};

struct HostMemoryBackend {
    /* private */
    Object parent;

    /* protected */
    uint64_t size;
    bool merge, dump, use_canonical_path;
    bool prealloc, force_prealloc, is_mapped, share;
    DECLARE_BITMAP(host_nodes, MAX_NODES + 1);
    HostMemPolicy policy;

    MemoryRegion mr;
};
```

## 2.2 RAMBlock和RAMList

**HostMemoryBackend对象中的内存**被**实际映射**到通过**qemu\_ram\_alloc**()函数（代码定义在**exec.c**中）**RAMBlock数据结构**中。

**每个RAMBlock**都有一个**指针**指向**被映射内存的起始位置**，同时包含一个**ram\_addr\_t的位移变量**。ram\_addr\_t位于**全局的命名空间**中，因此RAMBlock能够通过**offset来查找**。

RAMBlock定义在include/exec/ram\_addr.h中。RAMBlock受**RCU机制保护**，所谓RCU，即Read\-COPY\-Update，

```c
typedef uint64_t ram_addr_t;

struct RAMBlock {
    struct rcu_head rcu; //该数据结构受rcu机制保护
    struct MemoryRegion *mr;
    uint8_t *host;  //RAMBlock的内存起始位置
    uint8_t *colo_cache; /* For colo, VM's ram cache */
    ram_addr_t offset;  //在所有的RAMBlock中offset
    ram_addr_t used_length; //已使用长度
    ram_addr_t max_length;  //最大分配内存
    void (*resized)(const char*, uint64_t length, void *host);
    uint32_t flags;
    /* Protected by iothread lock.  */
    char idstr[256];    //RAMBlock的ID
    /* RCU-enabled, writes protected by the ramlist lock */
    QLIST_ENTRY(RAMBlock) next;
    QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
    int fd;
    size_t page_size;
    /* dirty bitmap used during migration */
    unsigned long *bmap;
    /* bitmap of pages that haven't been sent even once
     * only maintained and used in postcopy at the moment
     * where it's used to send the dirtymap at the start
     * of the postcopy phase
     */
    unsigned long *unsentmap;
    /* bitmap of already received pages in postcopy */
    unsigned long *receivedmap;
};
```



参考

https://blog.csdn.net/u011364612/article/details/51345110