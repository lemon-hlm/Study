
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 磁盘I/O性能测试工具](#1-磁盘io性能测试工具)
	* [1.1 DD](#11-dd)
	* [1.2 fio](#12-fio)
	* [1.3 Bonnie\+\+](#13-bonnie)
	* [1.4 hdparm](#14-hdparm)
* [2 测试环境配置](#2-测试环境配置)
* [1. 使用IDE磁盘￼](#1-使用ide磁盘)

<!-- /code_chunk_output -->

# 1 磁盘I/O性能测试工具

在一个计算机系统中，CPU获取**自身缓存数据**的速度非常快，读写内存的速度也比较快，内部局域网速度也比较快（特别是使用万兆以太网），但是磁盘I/O的速度是相对比较慢的。

很多的日常软件的运行都会读写磁盘，而且大型的数据库应用（如Oracle、MySQL等）都是磁盘I/O密集型的应用，所以在**KVM虚拟化**中磁盘I/O的性能也是比较关键的。

测试磁盘I/O性能的工具有很多，如DD、Bonnie\+\+、fio、iometer、hdparm等。下面简单介绍其中几个工具。

## 1.1 DD

DD（命令为dd）是Linux系统上一个非常流行的**文件复制工具**，在复制文件的同时可以根据其具体选项进行转换和格式化等操作。

通过DD工具复制同一个文件（相同数据量）所需要的时间长短即可粗略评估磁盘I/O的性能。

一般的Linux系统中都自带这个工具，用man dd命令即可查看DD工具的使用手册。

## 1.2 fio

fio是一个被广泛使用的进行**磁盘性能及压力测试的工具**。它功能强大而灵活，可以用它定义（模拟）出**各种工作负载（workload**），模拟**真实使用场景**，以更准确地衡量磁盘的性能。

除了测试磁盘**读写的带宽**以外，它还统计**IOPS**并且以**不同的延迟时间**分布表示；

除了**总的延迟时间**，它还分别统计**I/O递交的时间延迟**和**I/O完成的时间延迟**。

fio的主页是：http://git.kernel.dk/?p=fio.git;a=summary 。它可以运行在Linux、UNIX、Windows等多个操作系统平台上，多数Linux发行版也包含它的安装包。

## 1.3 Bonnie\+\+

Bonnie\+\+是以Bonnie￼的代码为基础编写而成的软件，它使用一系列对硬盘驱动器和文件系统的简单测试来衡量其性能。

Bonnie\+\+可以模拟像数据库那样去访问一个单一的大文件，也可以模拟像Squid那样创建、读取和删除许许多多的小文件。

它可以实现有序地读写一个文件，也可以随机地查找一个文件中的某个部分，而且支持按字符方式和按块方式读写。

## 1.4 hdparm

hdparm是一个用于**获取和设置SATA和IDE设备参数**的工具，在RHEL 7.3中可以用yum install hdparm命令来安装hdparm工具。

hdparm也可以**粗略地测试磁盘的I/O性能**，通过如下的命令即可粗略评估sdb这个磁盘的**读性能**。

```
hdparm -tT /dev/sdb
```

本节选择了DD、FIO、Bonnie\+\+这3种工具用于KVM虚拟化的磁盘I/O性能测试。

# 2 测试环境配置

对KVM的磁盘I/O虚拟化性能测试的环境配置，与前面CPU、内存、网络性能测试的环境配置基本相同，下面仅说明一下不同之处和需要强调的地方。

在本次对磁盘I/O的性能测试中，**客户机第2块硬盘**专门用于磁盘性能测试，这块硬盘是**宿主机的一个LVM分区**。

我们不选择前面通常使用的raw image文件，是为了**避免宿主机文件系统这一层的消耗**，从而使得客户机与宿主机直接的磁盘性能比较更加公平。

如图10\-10所示，因为笔者的宿主机环境是LVM分区，所以本节采用中间的一种方式。当然，最右边采用直接的物理硬盘分区的方式层次更简洁，可以预见其绝对性能应该更好。但如同前面几节的测试目标一样，我们这里注重测量虚拟化层的性能损耗，也就是客户机环境中基准测试结果，与同样资源、软件堆栈层次的原生系统中同样基准测试的结果进行对比。

![](./images/2019-05-11-23-01-25.png)

另外，磁盘性能测试与磁盘分区（包括物理分区和LVM分区）时候的参数设定、在硬盘分区上创建文件系统时候的参数设定，以及关联与具体磁盘的IO Scheduler都有密切关系。但这些不是我们这里测试的关注点，我们只要注意原始环境的参数设置和客户机里的参数设置一致即可。具体来说，我们这里的IO Scheduler、分区设置（LVM）、文件系统（XFS）的参数都选用最简单的默认值，这也覆盖了大多数用户的使用场景。

[root@kvm-host current]# lvcreate --size 32G --name perf-test-lvm rhel￼ [root@kvm-host current]# lvdisplay /dev/rhel/perf-test-lvm￼     --- Logical volume ---￼     LV Path                /dev/rhel/perf-test-lvm￼     LV Name                perf-test-lvm￼     VG Name                rhel￼     LV UUID                x8hVqL-U0Z8-i8fl-rFz1-tqrR-sAxW-vHV0d2￼     LV Write Access        read/write￼     LV Creation host, time kvm-host, 2017-04-29 21:43:12 +0800￼     LV Status              available￼     # open                 1￼     LV Size                32.00 GiB￼     Current LE             8192￼     Segments               1￼     Allocation             inherit￼     Read ahead sectors     auto￼     - currently set to     8192￼     Block device           253:2￼ [root@kvm-guest current]# mkfs -t xfs -f /dev/sda

在实验中使用的希捷2 TB大小的SATA硬盘，型号如下：

ST2000DL003-9VT166

本次测试评估了QEMU/KVM中的纯软件模拟的IDE磁盘和使用virtio-blk驱动的磁盘（见6.1.5节），启动客户机的QEMU命令行分别如下（注意，客户机有两块硬盘，一块是系统盘，一直是virtio-blk接口，第二块才是我们的测试硬盘，/dev/rhel/perf-test-lvm）：

#1. 使用IDE磁盘￼ qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=none,media=disk,id=virtio_drive0 -device virtio-blk-pci,drive=virtio_drive0,bootindex=0 -drive file=/dev/rhel/perf-test-lvm,media=disk,if=ide,format=raw,id=ide_drive,cache=none -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -name perf_test -display sdl￼ ￼ #2. 使用virtio-blk磁盘￼ qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=none,media=disk,id=virtio_drive0 -device virtio-blk-pci,drive=virtio_drive0,bootindex=0 -drive file=/dev/rhel/perf-test-lvm,media=disk,if=none,format=raw,id=virtio_drive1,cache=none -device virtio-blk-pci,drive=virtio_drive1 -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -name perf_test -display sdl

从上面的命令行可以看出，测试磁盘镜像文件是raw格式的，并且配置有“cache=none”来绕过页面缓存。配置为“cache=none”绕过了页面缓存，但是没有绕过磁盘自身的磁盘缓存；如果要在宿主机中彻底绕过这两种缓存，可以在启动客户机时配置为“cache=directsync”。不过由于“cache=directsync”配置会让客户机中磁盘I/O效率比较低，所以这种配置用得比较少，常用的配置一般为“cache=writethrough”“cache=none”等。关于“cache=xx”选项的配置，可以参考5.4.1节。
由于启动客户机时使用的磁盘配置选项“cache=xx”的设置对磁盘I/O测试结果的影响非常大，所以本次结果仅能代表“cache=none”这样配置下的一次基准测试。
10.5.3　性能测试方法
对非虚拟化的原生系统和KVM客户机都执行相同的磁盘I/O基准测试，然后对比其测试结果。
1.DD
用DD工具对读取磁盘文件进行测试，测试4种不同的块大小。使用的命令如下：

dd if=file.dat of=/dev/null iflag=direct bs=1K count=100K￼ dd if=file.dat of=/dev/null iflag=direct bs=8K count=100K￼ dd if=file.dat of=/dev/null iflag=direct bs=1M count=10K￼ dd if=file.dat of=/dev/null iflag=direct bs=8M count=2K

在上面命令中，if=xx表示输入文件（即被读取的文件），of=xx表示输出文件（即写入的文件）。这里为了测试读磁盘的速度，所以读取一个磁盘上的文件，然后将其写到/dev/null￼这个空设备中。iflag=xx表示打开输入文件时的标志，此处设置为direct是为了绕过页面缓存，以得到更真实的读取磁盘的性能。bs=xx表示一次读写传输的数据量大小，count=xx表示执行多少次数据的读写。
用DD工具向磁盘上写入文件的测试，也测试4种不同的块大小。使用的命令如下：
