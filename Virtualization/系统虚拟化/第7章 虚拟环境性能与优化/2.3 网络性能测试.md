
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 网络性能测试工具](#1-网络性能测试工具)
	* [1.1 Netperf](#11-netperf)
	* [1.2 Iperf](#12-iperf)
	* [1.3 NETIO](#13-netio)
	* [1.4 SCP](#14-scp)
* [2　测试环境配置](#2-测试环境配置)

<!-- /code_chunk_output -->

# 1 网络性能测试工具

只要是需要**快速**而且**大量**的**网络数据传输**的应用都可以作为**网络性能基准测试工具**，可以是**专门用于测试网络带宽**的**Netperf**、**Iperf**、**NETIO**、**Ttcp**等，也可以是常用的Linux上的**文件传输工具SCP**。下面简单介绍几种常用的网络性能测试工具。

## 1.1 Netperf

Netperf是由HP公司开发的一个网络性能基准测试工具，它是非常流行网络性能测试工具，其官方主页是http://www.netperf.org/netperf。Netperf工具可以运行在UNIX、Linux和Windows操作系统中。Netperf的源代码是开放的，不过它和普通开源软件使用的许可证协议不完全一样，如果想使用完全的开源软件，则可以考虑采用GNU GPLv2许可证发布的netperf4工具（http://www.netperf.org/svn/netperf4）。

Netperf可以测试网络性能的多个方面，主要包括使用TCP、UDP等协议的单向批量数据传输模式和请求-响应模式的传输性能。Netperf主要测试的项目包括：使用BSD Sockets的TCP和UDP连接（IPv4和IPv6）、使用DLPI接口的链路级别的数据传输、Unix Domain Socket、SCTP协议的连接（IPv4和IPv6）。Netperf采用客户机/服务器（Client/Server）的工作模式：服务端是netserver，用来侦听来自客户端的连接，客户端是netperf，用来向服务端发起网络测试。在客户端与服务端之间，首先建立一个控制连接，用于传递有关测试配置的信息和测试完成后的结果；在控制连接建立并传递了测试配置信息以后，客户端与服务端之间会另外再建立一个测试数据连接，用来传递指定测试模式的所有数据；当测试完成后数据连接就断开，控制连接会收集好客户端和服务端的测试结果，然后让客户端展示给用户。为了尽可能地模拟更多真实的网络传输场景，Netperf有非常多的测试模式供选择，包括：TCP_STREAM、TCP_MAERTS、TCP_SENDFILE、TCP_RR、TCP_CRR、TCP_CC、UDP_STREAM、UDP_RR等。

## 1.2 Iperf

Iperf是一个常用的网络性能测试工具，它是用C++编写的跨平台的开源软件，可以在Linux、UNIX和Windows系统上运行，其项目主页是：http://sourceforge.net/projects/iperf。Iperf支持TCP和UDP的数据流模式的测试，用于衡量其吞吐量。与Netperf类似，Iperf也实现了客户机/服务器模式，Iperf有一个客户端和一个服务端，可以测量两端的单向和双向数据吞吐量。当使用TCP功能时，Iperf测量有效载荷的吞吐带宽；当使用UDP功能时，Iperf允许用户自定义数据包大小，并最终提供一个数据包吞吐量值和丢包值。另外，有一个项目叫Iperf3（项目主页为http://code.google.com/p/iperf），它完全重新实现了Iperf，其目的是使用更小、更简单的源代码来实现相同的功能，同时也开发了可用于其他程序的一个函数库。

## 1.3 NETIO

NETIO也是个跨平台的、源代码公开的网络性能测试工具，它支持UNIX、Linux和Windows平台，其作者关于NETIO的主页是：http://www.ars.de/ars/ars.nsf/docs/netio . NETIO也是基于客户机/服务器的架构，它可以使用不同大小的数据报文来测试TCP和UDP网络连接的吞吐量。

## 1.4 SCP

SCP是Linux系统上最常用的远程文件复制程序，它可以作为实际的应用来测试网络传输的效率。用SCP远程传输同等大小的一个文件，根据其花费时间的长短可以粗略评估出网络性能的好坏。

在本次网络性能测试中，采用Netperf基准测试工具来评估KVM虚拟化中客户机的网络性能。

# 2　测试环境配置

对KVM的网络虚拟化性能测试的测试环境配置与10.2.2节的环境配置基本相同，具体可以参考表10\-1和表10\-2。略有不同的是，本节中，宿主机内存设置得比较大，40G。这是因为笔者发现，在以VT\-d分配VF给客户机时，客户机会预分配16G内存（笔者也不确定这是否是bug），如果只分配20G内存给宿主机，就会因内存吃紧而频繁交换内存页（Swap），从而造成系统性能急剧下降。所以，为了避免这种情况，也为了公平比较，所有的客户机类型测试中，宿主机都设置40G内存。

测试中用到的网卡为Intel X520 SR2（其以太网控制器为82599ES），10G的光纤网卡，驱动为ixgbe。

在测试SR\-IOV类型的网络时，要打开SR\-IOV的功能，具体方法参照6.2.5节。通过lspci命令查看已经打开SR\-IOV功能后的网卡具体信息，示例如下：

```
[root@kvm-host ~]# lspci | grep -i eth￼
//省略其他网卡设备￼
05:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)￼
05:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)￼
05:10.1 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)￼
05:10.3 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
```

在本次测试中，分别对KVM客户机中使用默认e1000网卡的网桥网络、使用virtio\-net模式（QEMU做后端驱动）的网桥网络、使用vhost\-net模式（vhost\-net做后端驱动）的网桥网络、VT\-d直接分配SR\-IOV VF这4种模式进行测试。为了实现这4种模式，启动客户机的qemu命令示例如下：