
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 网络性能测试工具](#1-网络性能测试工具)
	* [1.1 Netperf](#11-netperf)
	* [1.2 Iperf](#12-iperf)
	* [1.3 NETIO](#13-netio)
	* [1.4 SCP](#14-scp)
* [2 测试环境配置](#2-测试环境配置)
* [3 性能测试方法](#3-性能测试方法)

<!-- /code_chunk_output -->

# 1 网络性能测试工具

只要是需要**快速**而且**大量**的**网络数据传输**的应用都可以作为**网络性能基准测试工具**，可以是

- **专门用于测试网络带宽**的**Netperf**、**Iperf**、**NETIO**、**Ttcp**等，
- 也可以是常用的Linux上的**文件传输工具SCP**。

下面简单介绍几种常用的网络性能测试工具。

## 1.1 Netperf

Netperf是由**HP公司**开发的一个**网络性能基准测试工具**，它是非常流行**网络性能测试工具**，其官方主页是 http://www.netperf.org/netperf . 

Netperf工具可以运行在UNIX、Linux和Windows操作系统中。Netperf的源代码是开放的，不过它和普通开源软件使用的许可证协议不完全一样，如果想**使用完全的开源软件**，则可以考虑采用GNU GPLv2许可证发布的**netperf4工具**（ http://www.netperf.org/svn/netperf4 ）。

Netperf可以测试**网络性能的多个方面**，主要包括使用**TCP**、**UDP**等协议的**单向批量数据传输模式**和**请求\-响应模式**的传输性能。

Netperf主要测试的项目包括：使用**BSD Sockets**的**TCP和UDP连接**（IPv4和IPv6）、使用**DLPI接口**的链路级别的数据传输、Unix Domain Socket、SCTP协议的连接（IPv4和IPv6）。

Netperf采用**客户机/服务器（Client/Server）的工作模式**：

- **服务端**是**netserver**，用来**侦听来自客户端的连接**，
- **客户端**是**netperf**，用来**向服务端发起网络测试**。

测试流程:

1. 在**客户端与服务端之间**，首先**建立一个控制连接**，用于传递有关**测试配置的信息**和**测试完成后的结果**；
2. 在控制连接建立并传递了测试配置信息以后，客户端与服务端之间会另外**再建立一个测试数据连接**，用来传递指定测试模式的**所有数据**；
3. 当**测试完成**后数据连接就**断开**，控制连接会收集好客户端和服务端的测试结果，然后让客户端展示给用户。

为了尽可能地模拟更多真实的**网络传输场景**，Netperf有非常多的**测试模式供选择**，包括：TCP\_STREAM、TCP\_MAERTS、TCP\_SENDFILE、TCP\_RR、TCP\_CRR、TCP\_CC、UDP\_STREAM、UDP\_RR等。

## 1.2 Iperf

Iperf是一个常用的**网络性能测试工具**，它是用C++编写的跨平台的**开源软件**，可以在Linux、UNIX和Windows系统上运行，其项目主页是：http://sourceforge.net/projects/iperf . 

Iperf支持**TCP和UDP**的数据流模式的测试，用于衡量其**吞吐量**。

与Netperf类似，**Iperf**也实现了**客户机/服务器模式**，Iperf有一个客户端和一个服务端，可以测量**两端的单向和双向数据吞吐量**。

- 当使用**TCP**功能时，Iperf测量**有效载荷的吞吐带宽**；
- 当使用**UDP**功能时，Iperf允许用户**自定义数据包大小**，并最终提供一个数据包吞吐量值和丢包值。

另外，有一个项目叫Iperf3（项目主页为 http://code.google.com/p/iperf ），它完全重新实现了Iperf，其目的是使用**更小、更简单的源代码**来实现**相同的功能**，同时也开发了**可用于其他程序的一个函数库**。

## 1.3 NETIO

NETIO也是个**跨平台**的、源代码公开的网络性能测试工具，它支持UNIX、Linux和Windows平台，其作者关于NETIO的主页是：http://www.ars.de/ars/ars.nsf/docs/netio . 

NETIO也是基于**客户机/服务器**的架构，它可以使用**不同大小的数据报文**来测试**TCP和UDP网络连接**的吞吐量。

## 1.4 SCP

SCP是Linux系统上最常用的远程文件复制程序，它可以作为实际的应用来测试网络传输的效率。用SCP远程传输同等大小的一个文件，根据其花费时间的长短可以粗略评估出网络性能的好坏。

在本次网络性能测试中，采用Netperf基准测试工具来评估KVM虚拟化中客户机的网络性能。

# 2 测试环境配置

对KVM的网络虚拟化性能测试的测试环境配置与之前的环境配置基本相同，具体可以参考表10\-1和表10\-2。

略有不同的是，本节中，**宿主机内存设置得比较大**，**40G**。这是因为笔者发现，在**以VT\-d分配VF**给**客户机**时，**客户机**会**预分配16G内存！！！**（笔者也不确定这是否是bug），如果**只分配20G内存**给**宿主机**，就会因内存吃紧而**频繁交换内存页（Swap**），从而造成系统性能急剧下降。所以，为了避免这种情况，也为了公平比较，所有的客户机类型测试中，**宿主机都设置40G内存**。

测试中用到的网卡为Intel X520 SR2（其以太网控制器为82599ES），10G的光纤网卡，驱动为ixgbe。

在测试SR\-IOV类型的网络时，要**打开SR\-IOV**的功能，具体方法见其他文章。通过lspci命令查看已经打开SR\-IOV功能后的网卡具体信息，示例如下：

```
[root@kvm-host ~]# lspci | grep -i eth￼
//省略其他网卡设备￼
05:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)￼
05:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)￼
05:10.1 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)￼
05:10.3 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
```

在本次测试中，分别对**KVM客户机**中使用默认**e1000网卡**的网桥网络、使用**virtio\-net**模式（**QEMU做后端驱动**）的网桥网络、使用**vhost\-net**模式（**vhost\-net做后端驱动**）的网桥网络、VT\-d直接分配**SR\-IOV VF**这4种模式进行测试。为了实现这4种模式，启动客户机的qemu命令示例如下：

```
# 1. 默认e1000网卡的网桥网络￼
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=virtio,media=disk -drive file=./raw_disk.img,format=raw,if=virtio,media=disk -net nic,netdev=nic0 -netdev bridge,id=nic0,br=virbr1 -daemonize -name perf_test -display vnc=:1 ￼

# 2. virtio-net模式的网桥网络（vhost=off）￼
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=virtio,media=disk -drive file=./raw_disk.img,format=raw,if=virtio,media=disk -device virtio-net-pci,netdev=nic0,vhost=off -netdev bridge,id=nic0,br=virbr1 -daemonize -name perf_test -display vnc=:1￼

# 3. virtio-net模式的网桥网络（vhost=on）
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=virtio,media=disk -drive file=./raw_disk.img,format=raw,if=virtio,media=disk -device virtio-net-pci,netdev=nic0,vhost=on -netdev bridge,id=nic0,br=virbr1 -daemonize -name perf_test -display vnc=:1￼

# 4. VT-d直接分配VF￼
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=virtio,media=disk -drive file=./raw_disk.img,format=raw,if=virtio,media=disk -net none -device vfio-pci,host=05:10.1 -daemonize -name perf_test -display vnc=:1
```

在本次测试中，网桥网络在宿主机中绑定在X520的PF上。Netperf的客户端在测试的客户机里，服务器端在另一台物理主机上（配备的也是X520 SR网卡），它们之间通过一台思科10G光纤交换机相连，如图10\-7所示。

![](./images/2019-05-11-22-33-31.png)

KVM客户机网络配置的具体方法可参考其他.

# 3 性能测试方法

在本次网络性能测试中，将被测试的原生系统或者KVM客户机作为客户端，用**netperf**工具分别测试客户端在各种配置方式下的网络性能。

首先下载netperf最新的2.7.0版本￼，然后进行配置、编译、安装（configure、make、make install），即可得到Netperf的客户端和服务端测试工具netperf和netserver（注意，宿主机、客户机和远端作为netperf服务器端的物理主机上都要编译安装netperf）。


在**远端的物理主机上服务器端**运行**netserver**程序即可。命令行操作如下：

```
[root@kvm-host2 ~]# netserver￼
Starting netserver with host 'IN(6)ADDR_ANY' port '12865' and family AF_UNSPEC
```

然后，在**客户端**运行**netperf**程序即可**对服务端进行网络性能测试**。

我们分别以原生系统、默认网桥模式的客户机、virtio\-net网桥模式的客户机、virtio\-net\-vhost网桥模式的客户机以及VT\-d直接分配SR\-IOV VF模式的客户机作为netperf的客户端，来测量不同模式下的网络吞吐（TCP Stream）以及TCP响应时间（TCP RR）￼。

如图10\-7所示拓扑，笔者测试环境中netperf服务器端IP为192.168.0.3，在客户端发起的Netperf测试的命令示例如下：

```
#以原生环境为例￼
[root@kvm-host ~]# netperf -t tcp_stream -H 192.168.0.3 -l 60￼
MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.0.3 () port 0 AF_INET￼
Recv   Send    Send￼
Socket Socket  Message  Elapsed￼
Size   Size    Size     Time     Throughput￼
bytes  bytes   bytes    secs.    10^6bits/sec￼

87380  16384  16384    60.00    9408.19￼

[root@kvm-host ~]# netperf -t tcp_rr -H 192.168.0.3 -l 60￼
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.0.3 () port 0 AF_INET : first burst 0￼
Local /Remote￼
Socket Size   Request  Resp.   Elapsed  Trans.￼
Send   Recv   Size     Size    Time     Rate￼
bytes  Bytes  bytes    bytes   secs.    per sec￼
16384  87380  1        1       60.00    35782.91￼
16384  87380
```

在netperf命令中，-H name|IP表示连接到远程服务端的主机名或IP地址，-l testlen表示测试持续的时间长度（单位是秒），-t testname表示执行的测试类型（这里指定为TCP_STREAM这个很典型的类型）。可以使用netperf-h（或man netperf）命令查看netperf程序的帮助文档，更详细的解释可以参考netperf 2.7.0的官方在线文档￼。
在Netperf TCP_Stream测试过程的60秒时间中，同时使用sar命令记录KVM宿主机在40秒时间内的所有CPU的平均使用率。命令行操作示例如下（该命令表示每2秒采样一次CPU使用率，总共采样20次）：

[root@kvm-host native]# sar -u 2 20 > native-sar3.log

得到的CPU利用率的采样日志sar-1.log的信息大致如下：

[root@kvm-host native]# cat native-sar3.log ￼ ......￼ Average:        all      0.32      0.00      3.23      0.00      0.00     96.46

在本次实验中，主要观测了CPU空闲（idle）的百分比的平均值（如上面信息的最后一行的最后一个数值），而将其余部分都视为已经被占用的CPU资源。在带宽能达到基本相同的情况下，CPU空闲的百分比越高，说明netperf服务端消耗的CPU资源越少，这样的系统性能就越高。
10.4.4　性能测试数据
本次KVM虚拟化网络性能测试都是分别收集了3次测试数据后计算的平均值。
使用Intel 82599网卡对KVM网络虚拟化的性能进行测试得到的数据，与非虚拟化原生系统测试数据的对比，分别如图10-8和图10-9所示。图10-8中显示的吞吐量（throughput）越大越好，在吞吐量相近时，CPU空闲百分比也是越大越好。图10-9中显示的是每秒完成的TCP请求（Request）、应答（Response）的对数（Transaction），也是数值越大越好，表示每秒内TCP请求、应答的频率越高，也就是时延（latency）越小。
