
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1 基本配置](#1-基本配置)
- [2 docker安装](#2-docker安装)
- [3 修改docker cgroup driver为systemd](#3-修改docker-cgroup-driver为systemd)
- [3 kube\-proxy开启ipvs的前置条件](#3-kube-proxy开启ipvs的前置条件)

<!-- /code_chunk_output -->

Kubernetes系统由一组可执行程序组成，用户可以通过GitHub上的Kubernetes项目页**下载编译好的二进制包**，或者**下载源代码并编译后进行安装**。

安装Kubernetes对软件和硬件的系统要求如表2.1所示。

![2019-08-14-15-29-47.png](./images/2019-08-14-15-29-47.png)

Kubernetes需要**容器运行时（Container Runtime Interface，CRI**）的支持，目前官方支持的容器运行时包括：**Docker**、**Containerd**、**CRI\-O**和**frakti**。容器运行时CRI的原理详见3.9节的说明。本节以Docker作为容器运行环境.

宿主机操作系统以CentOS Linux 7为例，使用**Systemd系统**完成对Kubernetes服务的配置。其他Linux发行版的服务配置请参考相关的系统管理手册。为了便于管理，常见的做法是将Kubernetes服务程序配置为Linux系统开机自启动的服务。

需要注意的是，CentOS Linux 7默认启动了防火墙服务（firewalld），而Kubernetes的**Master**与**工作Node**之间会有大量的**网络通信**，安全的做法是在**防火墙上配置各组件需要相互通信的端口号**，具体要配置的端口号详见2.8节各服务启动参数中监听的端口号。

# 1 基本配置

修改主机名, 添加/etc/hosts, 添加ssh\-key

```
# hostnamectl set-hostname 02master01
# ssh-keygen
# cat /root/.ssh/id_rsa.pub
# vim 
```


如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口，可以查看[Installing kubeadm](https://kubernetes.io/docs/setup/independent/install-kubeadm/)中的”Check required ports”一节。 

这里简单起见可以关闭防火墙服务

```
# systemctl disable firewalld
# systemctl stop firewalld
```

禁用SELinux, 让容器可以读取主机文件系统:

```
# setenforce 0
```

或修改系统文件/etc/sysconfig/selinux, 将SELINUX=enforcing改成SELINUX=disabled, 然后重启Linux.

将桥接的IPV4流量传递到iptables的链, 创建/etc/sysctl.d/k8s.conf文件

```
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
```

使修改生效

```
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
```

```
sysctl --system
```

# 2 docker安装

Kubernetes从1.6开始使用CRI(Container Runtime Interface)容器运行时接口。默认的容器运行时仍然是Docker，使用的是kubelet中内置**dockershim CRI**实现。

具体可以查看文档[Install Docker](https://docs.docker.com/install/)。也可以查看国内镜像源的安装方式, 比如[清华镜像源](https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/)

先卸载旧版本

```
# sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
```

安装依赖包. `yum\-utils`提供`yum\-config\-manager`组件, `device\-mapper\-persistent\-data` 和 `lvm2`是被devicemapper存储驱动依赖.

```
# sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
```

下载repo文件

```
# wget -O /etc/yum.repos.d/docker-ce.repo http://mirrors.tencent.com/docker-ce/linux/centos/docker-ce.repo
# sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
```

将仓库地址修改为国内源, 比如tuna或腾讯

```
# sudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo
# sudo sed -i 's+download.docker.com+mirrors.tencent.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo
```

更新yum缓存

```
# sudo yum makecache
```

查看最新的Docker版本：

```
[root@master01 ~]# yum list docker-ce.x86_64  --showduplicates |sort -r
docker-ce.x86_64            3:19.03.1-3.el7                    docker-ce-stable
docker-ce.x86_64            3:19.03.0-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.8-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.7-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.7-3.el7                    @docker-ce-stable
docker-ce.x86_64            3:18.09.6-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.5-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.4-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.3-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.2-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.1-3.el7                    docker-ce-stable
docker-ce.x86_64            3:18.09.0-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.3.ce-3.el7                   docker-ce-stable
docker-ce.x86_64            18.06.2.ce-3.el7                   docker-ce-stable
docker-ce.x86_64            18.06.1.ce-3.el7                   docker-ce-stable
docker-ce.x86_64            18.06.0.ce-3.el7                   docker-ce-stable
docker-ce.x86_64            18.03.1.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            18.03.0.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.12.1.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.12.0.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.09.1.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.09.0.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.06.2.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.06.1.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.06.0.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.03.3.ce-1.el7                   docker-ce-stable
docker-ce.x86_64            17.03.2.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.03.1.ce-1.el7.centos            docker-ce-stable
docker-ce.x86_64            17.03.0.ce-1.el7.centos            docker-ce-stable
```

Kubernetes 1.15当前支持的docker版本列表是1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09。 这里在各节点安装docker的18.09.7版本。

安装docker engine

```
# yum install -y --setopt=obsoletes=0 docker-ce-18.09.7-3.el7
# systemctl start docker
# systemctl enable docker
```

Docker 需要用户具有 sudo 权限，为了避免每次命令都输入sudo，可以把用户加入 Docker 用户组

```
# sudo usermod -aG docker $USER
```


修改iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT 

```
# iptables -P FORWARD ACCEPT
# iptables -nvL
Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */
    0     0 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */
    0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
    0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  *      virbr0  0.0.0.0/0            192.168.122.0/24     ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  virbr0 *       192.168.122.0/24     0.0.0.0/0
    0     0 ACCEPT     all  --  virbr0 virbr0  0.0.0.0/0            0.0.0.0/0
    0     0 REJECT     all  --  *      virbr0  0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable
    0     0 REJECT     all  --  virbr0 *       0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable
```

# 3 修改docker cgroup driver为systemd

根据文档[CRI installation](https://kubernetes.io/docs/setup/cri/)中的内容，对于使用systemd作为init system的Linux的发行版，使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定，因此这里修改各个节点上docker的cgroup driver为systemd。

创建或修改/etc/docker/daemon.json：

```
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
```

重启docker：

```
# systemctl restart docker

# docker info | grep Cgroup
Cgroup Driver: systemd
```


# 3 kube\-proxy开启ipvs的前置条件

由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块

```
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
```

在所有的Kubernetes节点执行下面脚本

```
# cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用`lsmod | grep -e ip_vs -e nf_conntrack_ipv4`命令查看是否已经正确加载所需的内核模块。

接下来还需要确保各个节点上已经安装了ipset软件包yum install ipset。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm yum install ipvsadm。

如果以上前提条件如果不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式。








```
# kubeadm init --apiserver-advertise-address=192.168.56.11 --image-repository registry.aliyuncs.com/google_containers --service-cidr=192.168.0.0/16 --pod-network-cidr=192.168.0.0/16

# kubeadm init --apiserver-advertise-address=192.168.56.11 --service-cidr=192.168.0.0/16 --pod-network-cidr=192.168.0.0/16
```


```
journalctl -xeu kubelet
```




```
https://www.jianshu.com/p/745c96476a32
```

