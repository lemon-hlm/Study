
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1 Deployment或RC：全自动调度](#1-deployment或rc全自动调度)

<!-- /code_chunk_output -->

在Kubernetes平台上，我们**很少会直接创建一个Pod**，在大多数情况下会通过**RC**、**Deployment**、**DaemonSet**、**Job**等**控制器**完成对一组Pod副本的创建、调度及全生命周期的自动控制任务。

在**最早的Kubernetes版本**里是**没有这么多Pod副本控制器**的，**只有一个Pod副本控制器RC**（Replication Controller），这个控制器是这样设计实现的：**RC独立于所控制的Pod**，并通过**Label标签**这个松耦合关联关系**控制目标Pod实例的创建和销毁**，随着Kubernetes的发展，RC也出现了新的继任者——**Deployment**，用于更加自动地完成Pod副本的部署、版本更新、回滚等功能。

严谨地说，**RC的继任者**其实并**不是Deployment**，而是**ReplicaSet**，因为 ReplicaSet进一步**增强了 RC标签选择器的灵活性**。之前**RC的标签选择器只能选择一个标签**，而ReplicaSet拥有**集合式的标签选择器**，可以选择**多个Pod标签**，如下所示：

```yaml
selector:
  matchLabels:
    tier: frontend
  matchExpressions:
    - {key: tier, operator: In, values: [frontend]}
```

**与RC不同**，**ReplicaSet**被设计成能**控制多个不同标签的Pod副本**。一种**常见的应用场景**是，**应用MyApp**目前发布了**v1与v2两个版本**，用户希望MyApp的**Pod副本数保持为3个**，可以**同时包含v1和v2版本的Pod**，就可以用**ReplicaSet来实现这种控制**，写法如下：

```yaml
selector:
  matchLabels:
    version: v2
  matchExpressions:
    - {key: version, operator: In, values: [v1, v2]}
```

其实，Kubernetes的**滚动升级**就是巧妙运用ReplicaSet的这个特性来实现的，同时，**Deployment**也是通过**ReplicaSet**来实现**Pod副本自动控制功能**的。

我们**不应该！！！直接使用底层的ReplicaSet**来控制Pod副本，而应该使用**管理ReplicaSet**的**Deployment对象！！！** 来控制副本，这是来自官方的建议。

在大多数情况下，我们希望**Deployment**创建的**Pod副本**被成功调度到集群中的**任何一个可用节点**，而不关心具体会调度到哪个节点。但是，在真实的生产环境中的确也存在一种需求：希望**某种Pod的副本**全部在指定的**一个或者一些节点上运行**，比如希望将MySQL数据库调度到一个具有SSD磁盘的目标节点上，此时**Pod模板**中的**NodeSelector属性**就开始发挥作用了，上述MySQL定向调度案例的实现方式可分为以下两步。

（1）把具有SSD磁盘的**Node**都打上**自定义标签**“**disk=ssd**”。

（2）在**Pod模板**中设定**NodeSelector**的值为“**disk: ssd**”。

如此一来，Kubernetes在**调度Pod副本**的时候，就会**先**按照**Node的标签**过滤出**合适的目标节点**，然后选择**一个最佳节点**进行调度。

上述逻辑看起来既简单又完美，但在真实的生产环境中可能面临以下令人尴尬的问题。

（1）如果**NodeSelector**选择的**Label不存在**或者**不符合条件**，比如这些目标节点此时宕机或者资源不足，该怎么办？

（2）如果要选择**多种合适**的目标节点，比如**SSD磁盘的节点**或者**超高速硬盘的节点**，该怎么办？Kubernates引入了**NodeAffinity（节点亲和性设置**）来解决该需求。

在真实的生产环境中还存在如下所述的**特殊需求**。

（1）**不同Pod**之间的**亲和性（Affinity**）。比如MySQL数据库与Redis中间件不能被调度到同一个目标节点上，或者两种不同的Pod必须被调度到同一个Node上，以实现本地文件共享或本地网络通信等特殊需求，这就是PodAffinity要解决的问题。

（2）**有状态集群的调度**。对于ZooKeeper、Elasticsearch、MongoDB、Kafka等有状态集群，虽然集群中的每个Worker节点看起来都是相同的，但每个Worker节点都必须有明确的、不变的唯一ID（主机名或IP地址），这些节点的启动和停止次序通常有严格的顺序。此外，由于集群需要持久化保存状态数据，所以集群中的Worker节点对应的Pod不管在哪个Node上恢复，都需要挂载原来的Volume，因此这些Pod还需要捆绑具体的PV。针对这种复杂的需求，Kubernetes提供了StatefulSet这种特殊的副本控制器来解决问题，在Kubernetes 1.9版本发布后，StatefulSet才可用于正式生产环境中。

（3）在**每个Node**上**调度**并且**仅仅创建一个Pod副本**。这种调度通常用于系统监控相关的Pod，比如主机上的日志采集、主机性能采集等进程需要被部署到集群中的每个节点，并且只能部署一个副本，这就是DaemonSet这种特殊Pod副本控制器所解决的问题。

（4）对于**批处理作业**，需要创建**多个Pod副本**来协同工作，当这些Pod副本都完成自己的任务时，整个批处理作业就结束了。这种Pod运行且仅运行一次的特殊调度，用常规的RC或者Deployment都无法解决，所以Kubernates引入了新的Pod调度控制器Job来解决问题，并继续延伸了定时作业的调度控制器CronJob。

与单独的Pod实例不同，由RC、ReplicaSet、Deployment、DaemonSet等控制器创建的Pod副本实例都是归属于这些控制器的，这就产生了一个问题：控制器被删除后，归属于控制器的Pod副本该何去何从？在Kubernates 1.9之前，在RC等对象被删除后，它们所创建的Pod副本都不会被删除；在Kubernates 1.9以后，这些Pod副本会被一并删除。如果不希望这样做，则可以通过kubectl命令的\-\-cascade=false参数来取消这一默认特性：

```
# kubectl delete replicaset my-repset --cascade=false
```

# 1 Deployment或RC：全自动调度

Deployment或RC的主要功能之一就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。

下面是一个Deployment配置的例子，使用这个配置文件可以创建一个ReplicaSet，这个ReplicaSet会创建3个Nginx应用的Pod：

```yaml
# nginx-deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80 
```

运行kubectl create命令创建这个Deployment：

```
# kubectl create -f nginx-deployment.yaml
deployment "nginx-deployment" created
```

查看Deployment的状态:

```
# kubectl get deployment
NAME              DESIRED   CURRENT   UP-TO-DATE  AVAILABLE AGE
nginx-deployment  3         3         3           3         18s
```

该状态说明Deployment已创建好所有3个副本，并且所有副本都是最新的可用的。

通过运行kubectl get rs和kubectl get pods可以查看已创建的ReplicaSet（RS）和Pod的信息。

```
#kubectl get rs
```