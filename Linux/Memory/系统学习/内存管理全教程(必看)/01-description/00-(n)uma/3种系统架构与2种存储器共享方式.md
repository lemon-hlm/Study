# 1. 3种系统架构与2种存储器共享方式

## 1.1 架构概述

从**系统架构**来看，目前的商用服务器大体可以分为三类

- 对称多处理器结构(SMP：Symmetric Multi\-Processor)

- 非一致存储访问结构(NUMA：Non\-Uniform Memory Access)

- 海量并行处理结构(MPP：Massive Parallel Processing)。

**共享存储型**多处理机有两种模型

- 均匀存储器存取（Uniform\-Memory\-Access，简称UMA）模型

- 非均匀存储器存取（Nonuniform\-Memory\-Access，简称NUMA）模型

而我们后面所提到的**COMA和ccNUMA**都是**NUMA结构的改进**

## 1.2 SMP(Symmetric Multi-Processor)

所谓对称多处理器结构，是指服务器中**多个CPU对称工作**，**无主次或从属关系**。

各CPU共享相同的物理内存，每个CPU访问内存中的任何地址所需时间是相同的，因此SMP也被称为**一致存储器访问结构(UMA：Uniform Memory Access)**

对SMP服务器进行扩展的方式包括增加内存、使用更快的CPU、增加CPU、扩充I/O(槽口数与总线数)以及添加更多的外部设备(通常是磁盘存储)。

SMP服务器的主要特征是**共享**，系统中**所有资源**(CPU、内存、I/O等)都是**共享**的。也正是由于这种特征，导致了SMP服务器的主要问题，那就是它的扩展能力非常有限。

对于SMP服务器而言，每一个共享的环节都可能造成SMP服务器扩展时的瓶颈，而最受限制的则是**内存**。由于**每个CPU**必须通过**相同的内存总线**访问**相同的内存资源**，因此随着CPU数量的增加，**内存访问冲突**(涉及**内存一致性模型**)将迅速增加，最终会造成CPU资源的浪费，使CPU性能的有效性大大降低。实验证明，SMP服务器**CPU利用率**最好的情况是**2至4个CPU**

![UMA多处理机模型如图所示](./images/uma.gif)

图中，物理存储器被所有处理机均匀共享。所有处理机对所有存储字具有相同的存取时间，这就是为什么称它为均匀存储器存取的原因。每台处理机可以有私用高速缓存,外围设备也以一定形式共享。

## 1.3 NUMA(Non-Uniform Memory Access)

由于SMP在扩展能力上的限制，人们开始探究如何进行有效地扩展从而构建大型系统的技术，NUMA就是这种努力下的结果之一

利用NUMA技术，可以把几十个CPU(甚至上百个CPU)组合在一个服务器内.

![NUMA多处理机模型如图所示](./images/numa.png)

NUMA多处理机模型如图所示，其访问时间随存储字的位置不同而变化。其**共享存储器**物理上是分布在**所有处理机的本地存储器上**。**所有本地存储器**的集合组成了**全局地址空间**，可被所有的处理机访问。处理机访问本地存储器是比较快的，但访问属于另一台处理机的远程存储器则比较慢，因为通过互连网络会产生附加时延。

NUMA服务器的基本特征是具有**多个CPU模块**，**每个CPU模块由多个CPU(如4个)组成**，并且具有独立的**本地内存、I/O槽口等**(这些**资源都是以CPU模式划分本地的，一个CPU模块可能有多个CPU！！！**)。

![NUMA多处理机模型如图所示](./images/numa.gif)

由于其节点之间可以通过**互联模块**(如称为**Crossbar Switch**)进行连接和信息交互，因此**每个CPU**可以访问**整个系统的内存**(**这是NUMA系统与MPP系统的重要差别**)。显然，**访问本地内存**的速度将远远高于**访问远地内存**(系统内其它节点的内存)的速度，这也是非一致存储访问NUMA的由来。

由于这个特点，为了更好地发挥系统性能，开发应用程序时需要**尽量减少不同CPU模块之间的信息交互**。利用NUMA技术，可以较好地解决**原来SMP系统的扩展问题**，在一个物理服务器内**可以支持上百个CPU**。比较典型的NUMA服务器的例子包括HP的Superdome、SUN15K、IBMp690等。

但NUMA技术同样有一定**缺陷**，由于**访问远地内存的延时**远远超过**本地内存**，因此**当CPU数量增加**时，系统**性能无法线性增加**。如HP公司发布Superdome服务器时，曾公布了它与HP其它UNIX服务器的相对性能值，结果发现，64路CPU的Superdome(NUMA结构)的相对性能值是20，而8路N4000(共享的SMP结构)的相对性能值是6.3. 从这个结果可以看到，8倍数量的CPU换来的只是3倍性能的提升.

## 1.4 MPP(Massive Parallel Processing)

和NUMA不同，MPP提供了另外一种进行系统扩展的方式，它由**多个SMP服务器**通过一定的**节点互联网络**进行连接，协同工作，**完成相同的任务**，从用户的角度来看是一个服务器系统。其基本特征是由多个SMP服务器(**每个SMP服务器称节点**)通过**节点互联网络**连接而成，**每个节点只访问自己的本地资源**(内存、存储等)，是一种完全**无共享(Share Nothing)结构**，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现512个节点互联，数千个CPU。目前业界对**节点互联网络暂无标准**，如NCR的Bynet，IBM的SPSwitch，它们都采用了不同的内部实现机制。但节点互联网仅供MPP服务器内部使用，对用户而言是透明的。

**在MPP系统中，每个SMP节点也可以运行自己的操作系统、数据库等**。但和NUMA不同的是，它不存在异地内存访问的问题。换言之，**每个节点内的CPU不能访问另一个节点的内存**。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为**数据重分配**(Data Redistribution)。

但是MPP服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。目前一些基于MPP技术的服务器往往通过系统级软件(如数据库)来屏蔽这种复杂性。举例来说，NCR的Teradata就是基于MPP技术的一个关系数据库软件，基于此数据库来开发应用时，不管后台服务器由多少个节点组成，开发人员所面对的都是同一个数据库系统，而不需要考虑如何调度其中某几个节点的负载。

# 2. 三种体系架构之间的差异

## 2.1 NUMA、MPP、SMP之间性能的区别

**NUMA的节点互联机制**是在**同一个物理服务器内部实现**的，当某个CPU需要进行远地内存访问时，它必须等待，这也是NUMA服务器**无法实现CPU增加时性能线性扩展**。
 
**MPP的节点互联机制**是在**不同的SMP服务器外部通过I/O实现**（**所以Linux不用考虑这种！！！**）的，每个节点只访问本地内存和存储，节点之间的信息交互与节点本身的处理是并行进行的。因此MPP在增加节点时性能**基本上可以实现线性扩展**。
 
SMP所有的**CPU资源是共享的**，因此**完全实现线性扩展**。
 
## 2.2 NUMA、MPP、SMP之间扩展的区别

NUMA理论上可以无限扩展，目前技术比较成熟的能够支持上百个CPU进行扩展。如HP的SUPERDOME。

MPP理论上也可以实现无限扩展，目前技术比较成熟的能够支持512个节点，数千个CPU进行扩展。

**SMP扩展能力很差**，目前2个到4个CPU的利用率最好，但是IBM的BOOK技术，能够将CPU扩展到8个。

MPP是由多个SMP构成，多个SMP服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务。
 
## 2.3 MPP和SMP、NUMA应用之间的区别

**MPP的优势**

**MPP系统不共享资源**，因此对它而言，资源比SMP要多，**当需要处理的事务达到一定规模**时，**MPP的效率要比SMP好**。由于MPP系统因为要在不同处理单元之间传送信息，在通讯时间少的时候，那MPP系统可以充分发挥资源的优势，达到高效率。也就是说：**操作相互之间没有什么关系，处理单元之间需要进行的通信比较少，那采用MPP系统就要好**。因此，MPP系统在决策支持和数据挖掘方面显示了优势。

**SMP的优势**

MPP系统因为要在不同处理单元之间传送信息，所以它的效率要比SMP要差一点。**在通讯时间多的时候**，那MPP系统可以充分发挥资源的优势。因此当前使用的OTLP程序中，用户访问一个中心数据库，如果采用SMP系统结构，它的效率要比采用MPP结构要快得多。

**NUMA架构的优势**

NUMA架构来看，它可以在一个物理服务器内集成许多CPU，使系统具有较高的事务处理能力，由于远地内存访问时延远长于本地内存访问，因此需要**尽量减少不同CPU模块之间的数据交互**。显然，NUMA架构更适用于OLTP事务处理环境，当用于数据仓库环境时，由于大量复杂的数据处理必然导致大量的数据交互，将使CPU的利用率大大降低。

## 3. 总结

传统的多核运算是使用SMP(Symmetric Multi-Processor )模式：将多个处理器与一个集中的存储器和I/O总线相连。所有处理器只能访问同一个物理存储器，因此SMP系统有时也被称为一致存储器访问(UMA)结构体系，一致性意指无论在什么时候，处理器只能为内存的每个数据保持或共享唯一一个数值。很显然，SMP的缺点是可伸缩性有限，因为在存储器和I/O接口达到饱和的时候，增加处理器并不能获得更高的性能，与之相对应的有AMP架构，不同核之间有主从关系，如一个核控制另外一个核的业务，可以理解为多核系统中控制平面和数据平面。

NUMA模式是一种分布式存储器访问方式，处理器可以同时访问不同的存储器地址，大幅度提高并行性。 NUMA模式下，处理器被划分成多个"节点"（node）， 每个节点被分配有的本地存储器空间。 所有节点中的处理器都可以访问全部的系统物理存储器，但是访问本节点内的存储器所需要的时间，比访问某些远程节点内的存储器所花的时间要少得多。

NUMA 的主要优点是伸缩性。NUMA 体系结构在设计上已超越了 SMP 体系结构在伸缩性上的限制。通过 SMP，所有的内存访问都传递到相同的共享内存总线。这种方式非常适用于 CPU 数量相对较少的情况，但不适用于具有几十个甚至几百个 CPU 的情况，因为这些 CPU 会相互竞争对共享内存总线的访问。NUMA 通过限制任何一条内存总线上的 CPU 数量并依靠高速互连来连接各个节点，从而缓解了这些瓶颈状况。


名词解释

| 概念 | 描述 |
|:----:|:---:|
| SMP | 称为共享存储型多处理机(Shared Memory mulptiProcessors), 也称为对称型多处理机（Symmetry MultiProcessors) |
| UMA | 称为均匀存储器存取（Uniform-Memory-Access) |
| NUMA | 非均匀存储器存取（Nonuniform-Memory-Access） |
| COMA | 只用高速缓存的存储器结构（Cache-Only Memory Architecture） |
| ccNUMA | 高速缓存相关的非一致性内存访问（CacheCoherentNon-UniformMemoryAccess） |

**UMA**

物理存储器被所有处理机均匀共享。所有处理机对所有存储字具有相同的存取时间，这就是为什么称它为均匀存储器存取的原因。每台处理机可以有私用高速缓存,外围设备也以一定形式共享。

**NUMA**

其访问时间随存储字的位置不同而变化。其共享存储器物理上是分布在所有处理机的本地存储器上。所有本地存储器的集合组成了全局地址空间，可被所有的处理机访问。处理机访问本地存储器是比较快的，但访问属于另一台处理机的远程存储器则比较慢，因为通过互连网络会产生附加时延。 

**COMA**


一种只用高速缓存的多处理机。COMA模型是NUMA机的一种特例，只是将后者中分布主存储器换成了高速缓存, 在每个处理机结点上没有存储器层次结构,全部高速缓冲存储器组成了全局地址空间。远程高速缓存访问则借助于分布高速缓存目录进行。

是CC-NUMA体系结构的竞争者，两者拥有相同的目标，但实现方式不同。COMA节点不对内存部件进行分布，也不通过互连设备使整个系统保持一致性。COMA节点没有内存，只在每个Quad中配置大容量的高速缓存

**CCNUMA**

在CC-NUMA系统中，分布式内存相连接形成单一内存，内存之间没有页面复制或数据复制，也没有软件消息传送。CC-NUMA只有一个内存映象，存储部件利用铜缆和某些智能硬件进行物理连接。CacheCoherent是指不需要软件来保持多个数据拷贝的一致性，也不需要软件来实现操作系统与应用系统的数据传输。如同在SMP模式中一样，单一操作系统和多个处理器完全在硬件级实现管理。