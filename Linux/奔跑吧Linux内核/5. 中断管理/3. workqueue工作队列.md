[TOC]

思考题:

- workqueue是运行在中断上下文，还是进程上下文？其回调函数允许睡眠吗？
- 旧版本（Linux 2.6.25)的 workqueue机制在实际过程中遇到了哪些问题和挑战？
- CMWQ机制如何动态管理工作线程池的线程呢？
- 如果有多个work挂入一个工作线程中执行，当某个work的回调函数执行了阻塞操作，那么剩下的work该怎么办？

# 0 历史和原理概述

工作队列机制(workqueue)是除了软中断和tasklet以外最常用的一种下半部机制。工作队列的**基本原理**是把**work(需要推迟执行的函数**）交由一个**内核线程**来执行，它总是在**进程上下文**中执行。工作队列的优点是利用**进程上下文**来执行**中断下半部操作**，因此工作队列允许**重新调度**和**睡眠**，是异步执行的**进程上下文**，另外它还能解决**软中断**和**tasklet**执行时间过长导致**系统实时性下降**等问题。

当驱动程序或者内核子系统在进程上下文中有异步执行的工作任务时，可以使用**work item**来描述工作任务，包括该工作任务的执行回调函数，**把work item添加到一个队列**中，然后**一个内核线程**会去执行这个**工作任务的回调函数**。这里**work item被称为工作**，**队列被称为workqueue**，即工作队列，**内核线程被称为worker**。

工作队列最早是在Linux 2.5.x内核开发期间被引入的机制，早期的工作队列的设计比较简单，由**多线程（Multi threaded，每个CPU默认一个工作线程**）和**单线程（Single threaded, 用户可以自行创建工作线程**）组成。在长期测试中发现如下问题:

- **内核线程数量太多**。虽然系统中有默认的一套工作线程（kevents)，但是有很多驱动和子系统喜欢自行创建工作线程，例如调用create\_workqueue()函数，这样在大型系统(CPU数量比较多的机器)中可能内核启动结束之后就耗尽了系统PID资源。
- **并发性比较差**。Multi threaded的工作线程和CPU是一一绑定的，例如CPU0上的某个工作线程有A 、B 和 C 三个work。假设执行work A上回调函数时发生了睡眠和调度，CPU0就会调度出去执行其他的进程，对 于 B 和 C 来说，它们只能等待CPU0重新调度执行该工作线程，尽管其他CPU比较空闲，也没有办法迁移到其他CPU上执行。
- **死锁问题**。系统有一个默认的工作队列kevents, 如果有很多work运行在默认的工作队列kevents上，并且它们有一些数据上依赖关系，那么很有可能会产生死锁。解决办法是为每一个有可能产生死锁的work创建一个专职的工作线程，这样又回到问题1了。

为此社区专家Tejun Heo在Linux 2.6.36中提出了一套解决方案**concurrency\-managed workqueues(CMWQ**)。

执行**work任务的线程**称为**worker**或**工作线程**。**工作线程**会**串行化地执行**挂入到队列中**所有的work**。如果队列中**没有work**, 那么该**工作线程**就会变成**idle状态**。

为了管理众多**工作线程**，CMWQ提出了**工作线程池(worker\-pool**)概念，worker\-pool有**两种**，一是**BOUND类型**的，可以理解为**Per\-CPU类型**，每个CPU都有worker\-pool; 另一种是**UNBOUND类型**的，即不和具体CPU绑定。这**两种worker\-pool**都会定义**两个线程池**，一个给**普通优先级的work**使用，另一个给**高优先级的work**使用。这些工作线程池中的**线程数量**是**动态分配**和管理的，而不是固定的。当**工作线程睡眠**时，会去检查是否需要唤醒更多的工作线程，如有需要，会去**唤醒同一个工作线程池中idle状态**的工作线程。

# 1 初始化工作队列

## 1.1 工作任务struct work\_struct

**workqueue**机制**最小的调度单元是work item**, 有的书中称为工作任务，由struct work\_struct数据结构来抽象和描述，本章简称为work或工作任务。

```c
[include/linux/workqueue.h]
struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
};
```

struct work\_struct数据结构定义比较简单。

- data成员包括**两部分**，**低比特位部分**是work的**标志位**，**剩余的比特位**通常用于存放**上一次运行的worker\_pool**的**ID号**或**pool\_workqueue的指针**,存放的内容由**WORK\_STRUCT\_PWQ标志位来决定**。
- func是工作任务的处理函数
- entry用于把**work挂到其他队列**上。

## 1.2 工作线程struct worker

**work**运行在**内核线程**中，这个**内核线程在代码中被称为worker**, 类似流水线中的工人，work类似工人的工作，本章简称为**工作线程或worker**。

**工作线程**用**struct worker**数据结构来描述：

```c
[kernel/workqueue_internal.h]
struct worker {
	/* on idle list while idle, on busy hash table while busy */
	union {
		struct list_head	entry;	/* L: while idle */
		struct hlist_node	hentry;	/* L: while busy */
	};

	struct work_struct	    *current_work;	/* L: work being processed */
	work_func_t		        current_func;	/* L: current_work's fn */
	struct pool_workqueue	*current_pwq;   /* L: current_work's pwq */
	struct list_head	    scheduled;	    /* L: scheduled works */
    struct list_head	    node;		    /* A: anchored at pool->workers */
						                    /* A: runs through worker->node */
	struct task_struct	    *task;		    /* I: worker task */
	struct worker_pool	    *pool;		    /* I: the associated pool */
	int			            id;		        /* I: worker id */
    ...
};
```

- current\_work: 当前**正在处理的work**。
- current\_func: 当前**正在执行的work回调函数**。
- current\_pwq：当前**work所属的pool\_workqueue**。
- scheduled: 所有被调度并正**准备执行的work都挂入该链表**中。
- task: 该**工作线程**的task\_struct数据结构。
- pool: 该工作线程所属的**worker\_pool**。
- id: 工作线程的**ID号**。
- node: 可以把该worker挂入到**worker\_pool\->workers链表**中。

## 1.3 工作线程池struct worker\_pool

CMWQ提出了**工作线程池**概念，代码中使用struct worker\_pool数据结构来抽象和描述，本章简称worker\-pool或者工作线程池。

简化后的**struct worker\_pool**数据结构如下：

```c
[kernel/workqueue.c]
struct worker_pool {
	spinlock_t		lock;		/* the pool lock */
	int			    cpu;		/* I: the associated cpu */
	int			    node;		/* I: the associated node ID */
	int			    id;		    /* I: pool ID */
	unsigned int	flags;		/* X: flags */

	struct list_head	worklist;	/* L: list of pending works */
	int			        nr_workers;	/* L: total number of workers */
	int			        nr_idle;	/* L: currently idle ones */

	struct list_head	idle_list;	/* X: list of idle workers */
	struct list_head	workers;	/* A: attached workers */
	struct workqueue_attrs	*attrs;		/* I: worker attributes */
	atomic_t		nr_running ____cacheline_aligned_in_smp;
	struct rcu_head		rcu;
	...
} ____cacheline_aligned_in_smp;
```

- lock: 用于**保护worker\-pool的自旋锁**。
- cpu: 对应**BOUND类型**的**workqueue**来说，cpu表示**绑定的CPU ID**, 对应**UNBOUND类型**，该**值为\-1**。
- node: 对于**UNBOUND类型的workqueue**，node表示该**worker\-pool所属内存节点的ID**编号。
- id: 该**worker\-pool的ID号**。
- worklist: **pending状态**的**work**会挂入**该链表**中。
- nr\_workers: **工作线程的数量**。
- nr\_idle: 处于**idle状态**的**工作线程的数量**。
- idle\_list: 处于**idle状态**的**工作线程(！！！**)会挂入**该链表**中。
- workers: 该worker\-pool管理的**工作线程**会挂入**该链表**中。
- attrs: **工作线程的属性**。
- nr\_running: **统计计数**，用于管理**worker**的**创建和销毁**，表示**正在运行中的worker数量**。在**进程调度器**中**唤醒进程时(try\_to\_wake\_up**())，**其他CPU**有可能会**同时访问该成员**，该成员**频繁在多核之间读写**，因此让**该成员独占一个缓冲行(！！！**)，避免**多核CPU**在**读写该成员**时引发其他临近的成员“颠簸”现象，这也是所谓的“**缓存行伪共享**”的问题。
- rcu: RCU锁。

**worker\-pool是Per\-CPU**概念，每个CPU都有worker\-pool, 准确来说**每个CPU有两个worker\-pool**, 一个用于**普通优先级的工作线程**，另一个用于**高优先级的工作线程**。

```c
[kernel/workqueue.c]
/* the per-cpu worker pools */
static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS],
				     cpu_worker_pools);
```

## 1.4 连接workqueue(工作队列)和worker\-pool(工作线程池)的桥梁strct pool\_workqueue

CMWQ还定义了一个**pool\_workqueue**的数据结构，它是**连接workqueue和worker\-pool的枢纽**。

```c
[kernel/workqueue.c]
struct pool_workqueue {
	struct worker_pool	    *pool;		/* I: the associated pool */
	struct workqueue_struct *wq;		/* I: the owning workqueue */
	int			            nr_active;	/* L: nr of active works */
	int			            max_active;	/* L: max active works */
	struct list_head	    delayed_works;	/* L: delayed works */
	struct rcu_head		    rcu;
	...
} __aligned(1 << WORK_STRUCT_FLAG_BITS);
```

其中，**WORK\_STRUCT\_FLAG\_BITS为8**, 因此pool\_workqueue数据结构是按照**256Byte对齐**的，这样方便把该**数据结构指针的bit[8:31]位存放到work\->data**中，work\->data字段的**低8位**用于存放一些**标志位**，见set\_work\_pwq()和get\_work\_pwq()函数。

- pool: 指向**worker\-pool指针**。
- wq: 指向**所属的工作队列**。
- nr\_active: **活跃的work数量**。
- max\_active: **活跃的work最大数量**。
- delayed\_works: 链表头，被**延迟执行的works可以挂入该链表**。
- rcu: rcu锁。

## 1.5 工作队列workqueue

系统中**所有的工作队列**，包括系统**默认的工作队列**，例如system\_wq或system\_highpri\_wq等，以及驱动开发者新创建的工作队列，它们**共享一组worker\-pool**。而对于**BOUND类型的工作队列**，**每个CPU**只有**两个工作线程池**，**每个工作线程池**可以和**多个workqueue**对应，**每个workqueue**也**只能对应这几个工作线程池**。

**工作队列**由struct **workqueue\_struct**数据结构来描述：

```c
[kernel/workqueue.c]
struct workqueue_struct {
	struct list_head	pwqs;		/* WR: all pwqs of this wq */
	struct list_head	list;		/* PL: list of all workqueues */

	struct list_head	maydays;	/* MD: pwqs requesting rescue */
	struct worker		*rescuer;	/* I: rescue worker */

	struct workqueue_attrs	*unbound_attrs;	/* WQ: only for unbound wqs */
	struct pool_workqueue	*dfl_pwq;	/* WQ: only for unbound wqs */

	char			name[WQ_NAME_LEN]; /* I: workqueue name */

	/* hot fields used during command issue, aligned to cacheline */
	unsigned int		flags ____cacheline_aligned; /* WQ: WQ_* flags */
	struct pool_workqueue __percpu *cpu_pwqs; /* I: per-cpu pwqs */
	...
};
```

- pwqs: **所有的pool\-workqueue**数据结构都**挂入链表**中。
- list: **链表节点**。系统定义一个**全局的链表workqueues**，**所有的workqueue**挂入**该链表**。
- maydays: **所有rescue状态**下的**pool\-workqueue**数据结构**挂入该链表**。
- rescuer: **rescue内核线程**。**内存紧张**时**创建新的工作线程**可能会失败，如果**创建workqueue**时设置了**WQ\_MEM\_RECLAIM**标志位，那么**rescuer线程会接管这种情况**。
- unbound attrs: **UNBOUND类型属性**。
- dfl\_pwq: 指向**UNBOUND类型的pool\_workqueue**.
- name: 该**workqueue的名字**。
- flags: 标志位经常被**不同CPU访问**，因此要和**cache line对齐**。标志位包括WQ\_UNBOUND、WQ\_HIGHPRI、WQ\_FREEZABLE等。
- cpu\_pwqs: 指向**Per\-CPU类型**的**pool workqueue**。

## 1.6 数据结构关系图

**一个work挂入workqueue**中，最终还要**通过worker\-pool**中的**工作线程来处理其回调函数**，worker-pool是**系统共享的(！！！**)，因此**workqueue**需要查找到一个**合适的worker\-pool**，然后从worker\-pool中分派一个**合适的工作线程**，pool\_workqueue数据结构在其中起到**桥梁**作用。这有些类似IT类公司的人力资源池的概念，具体关系如图5.7所示。

![config](./images/11.png)

## 1.7 系统初始化几个默认的workqueue

总结:

(1) 创建一个pool\_workqueue结构的slab缓存对象

(2) 为所有可用CPU创建两个工作线程池struct worker\_pool(普通优先级的和高优先级的)并初始化

(3) 为每个在线CPU的每个工作线程池分别创建一个工作线程(调用create\_worker())

(4) 创建UNBOUND类型和ordered类型的workqueue属性

(5) 创建几个默认的workqueue, 调用alloc\_workqueue()

在**系统启动**时，会通过**init\_workqueues**()函数来**初始化几个系统默认的workqueue**。

```c
[kernel/workqueue.c]
static int __init init_workqueues(void)
{
	int std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };
	int i, cpu;
    // 位置1
	pwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);

	cpu_notifier(workqueue_cpu_up_callback, CPU_PRI_WORKQUEUE_UP);
	hotcpu_notifier(workqueue_cpu_down_callback, CPU_PRI_WORKQUEUE_DOWN);
    // 位置2
	wq_numa_init();

	/* initialize CPU pools */
	// 位置3
	for_each_possible_cpu(cpu) {
		struct worker_pool *pool;

		i = 0;
		// 位置4
		for_each_cpu_worker_pool(pool, cpu) {
			BUG_ON(init_worker_pool(pool));
			pool->cpu = cpu;
			cpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));
			pool->attrs->nice = std_nice[i++];
			pool->node = cpu_to_node(cpu);

			/* alloc pool ID */
			mutex_lock(&wq_pool_mutex);
			BUG_ON(worker_pool_assign_id(pool));
			mutex_unlock(&wq_pool_mutex);
		}
	}

	/* create the initial worker */
	// 位置5
	for_each_online_cpu(cpu) {
		struct worker_pool *pool;

		for_each_cpu_worker_pool(pool, cpu) {
			pool->flags &= ~POOL_DISASSOCIATED;
			BUG_ON(!create_worker(pool));
		}
	}
    // 位置6
	/* create default unbound and ordered wq attrs */
	for (i = 0; i < NR_STD_WORKER_POOLS; i++) {
		struct workqueue_attrs *attrs;

		BUG_ON(!(attrs = alloc_workqueue_attrs(GFP_KERNEL)));
		attrs->nice = std_nice[i];
		unbound_std_wq_attrs[i] = attrs;

		BUG_ON(!(attrs = alloc_workqueue_attrs(GFP_KERNEL)));
		attrs->nice = std_nice[i];
		attrs->no_numa = true;
		ordered_wq_attrs[i] = attrs;
	}
    // 位置7
	system_wq = alloc_workqueue("events", 0, 0);
	system_highpri_wq = alloc_workqueue("events_highpri", WQ_HIGHPRI, 0);
	system_long_wq = alloc_workqueue("events_long", 0, 0);
	system_unbound_wq = alloc_workqueue("events_unbound", WQ_UNBOUND,
					    WQ_UNBOUND_MAX_ACTIVE);
	system_freezable_wq = alloc_workqueue("events_freezable",
					      WQ_FREEZABLE, 0);
	system_power_efficient_wq = alloc_workqueue("events_power_efficient",
					      WQ_POWER_EFFICIENT, 0);
	system_freezable_power_efficient_wq = alloc_workqueue("events_freezable_power_efficient",
					      WQ_FREEZABLE | WQ_POWER_EFFICIENT,
					      0);
	return 0;
}
early_initcall(init_workqueues);
```

位置1, 创建一个**pool\_workqueue**数据结构的**slab缓存对象**。

位置2, **workqueue**考虑了**NUMA系统**情况的一些特殊处理。

位置3, 为系统中**所有可用的CPU**(cpu\_possible\_mask) 分别**创建struct worker\_pool数据结构**. 

位置4, for\_each\_cpu\_worker\_pool()为**每个CPU**创建**两个worker\_pool**, 一个是**普通优先级**的工作线程池, 另一个是**高优先级**的工作线程池, **init\_worker\_pool**()函数用于**初始化一个worker\_pool**. 

位置4的**for\_each\_cpu\_worker\_pool**宏**遍历CPU中两个worker\_pool**:

```c
[kernel/workqueue.c]
#define for_each_cpu_worker_pool(pool, cpu)				\
	for ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];		\
	     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \
	     (pool)++)
```

位置5, 为系统每一个**在线(online)CPU**中的**每个worker\_pool**分别**创建一个工作线程**。

位置6, 创建**UNBOUND类型**和**ordered类型的workqueue属性**，**ordered类型**的**workqueue**表示**同一个时刻只能有一个work item在运行(！！！**)。

位置7到最后, **创建系统默认的workqueue**，这里使用**创建工作队列**的API函数**alloc\_workqueue**().

- **普通优先级BOUND类型**的**工作队列system\_wq**, 名称为“**events**”，可以理解为**默认工作队列**。
- **高优先级BOUND类型**的工作队列**system\_highpri\_wq** ，名称为“**events\_highpri**”。
- **UNBOUND类型**的工作队列**system\_unbound\_wq**，名称为“**system\_unbound\_wq**”。
- **Freezable类型**的工作队列**system\_freezable\_wq**，名称为“**events\_freezable**”。
- **省电类型**的工作队列**system\_freezable\_wq**，名称为 “**events\_power\_efficient**”。

### 1.7.1 create\_worker()创建工作线程

总结:

(1) 获取一个ID

(2) 工作线程池对应的内存节点分配一个worker

(3) 给worker设置名字

(4) 在工作线程池对应的内存节点上创建一个**内核线程**, 执行函数位worker\_thread, 参数为worker(struct worker)

(5) 设置线程(worker\->task\->flags)的PF\_NO\_SETAFFINITY标志位, 防止修改CPU亲和性

(6) 将创建的worker挂到worker\_pool: 线程池如果有绑定到某个CPU, 那么设置worker不绑定CPU; 将worker加到工作线程池的workers链表

(7) 使worker进入idle状态

(8) 唤醒worker的内核线程

(9) 返回该worker

上面位置5, 会为**每个online的CPU**的**每个worker\_pool**分别创建**一个工作线程**.

下面来看**create\_worker**()函数是如何创建工作线程的。

```c
[init_workqueues()->create_worker()]
static struct worker *create_worker(struct worker_pool *pool)
{
	struct worker *worker = NULL;
	int id = -1;
	char id_buf[16];
    // 位置1
	/* ID is needed to determine kthread name */
	id = ida_simple_get(&pool->worker_ida, 0, 0, GFP_KERNEL);
    // 位置2
	worker = alloc_worker(pool->node);

	worker->pool = pool;
	worker->id = id;
    // 位置3
	if (pool->cpu >= 0)
		snprintf(id_buf, sizeof(id_buf), "%d:%d%s", pool->cpu, id,
			 pool->attrs->nice < 0  ? "H" : "");
	else
		snprintf(id_buf, sizeof(id_buf), "u%d:%d", pool->id, id);
    // 位置4
	worker->task = kthread_create_on_node(worker_thread, worker, pool->node,
					      "kworker/%s", id_buf);

	set_user_nice(worker->task, pool->attrs->nice);

	/* prevent userland from meddling with cpumask of workqueue workers */
	// 位置5
	worker->task->flags |= PF_NO_SETAFFINITY;

	/* successful, attach the worker to the pool */
	// 位置6
	worker_attach_to_pool(worker, pool);

	/* start the newly created worker */
	spin_lock_irq(&pool->lock);
	// 位置7
	worker->pool->nr_workers++;
	// 位置8
	worker_enter_idle(worker);
	// 位置9
	wake_up_process(worker->task);
	spin_unlock_irq(&pool->lock);

	return worker;
}
```

位置1, 通过**IDA子系统**获取一个**ID号**。

位置2, 在**worker\_pool**对应的**内存节点中分配一个worker**数据结构。

位置3到位置4之间，**pool\->cpu \>= 0**, 表示**BOUND类型的工作线程**。worker的名字一般是 “**kworker/ \+ CPU\_ID \+ worker\_id**”，如果属于**高优先级**类型的workqueue，即**nice值小于 0**，那么还要**加上“H**”。 **pool\->cpu \< 0**，表示**UNBOUND类型的工作线程**，名字为“**kworker/u + CPU\_ID + worker\_id**”。

位置4，通过**kthread\_create\_on\_node**()函数在**工作线程池对应的node！！！**中**创建一个内核线程用于worker**，在这个内存节点上分配该内核线程相关的struct task\_struct等数据结构。

注意, **线程执行函数为worker\_thread！！！worker(struct worker)是执行函数的参数, 在工作线程池对应的node上创建, 线程名是位置3设置的！！！**

位置5，设置**工作线程(task的flags！！！**)的**PF\_NO\_SETAFFINITY**标志位，**防止用户程序修改其CPU亲和性**。在**位置6**代码中会设置**这个worker允许运行的cpumask(！！！**)。

位置6，**worker\_attach\_to\_pool**()函数把刚分配的**工作线程**挂入**worker\_pool**中。

```c
[create_worker() ->worker_attach_to_pool()]
static void worker_attach_to_pool(struct worker *worker,
				   struct worker_pool *pool)
{
	mutex_lock(&pool->attach_mutex);
	set_cpus_allowed_ptr(worker->task, pool->attrs->cpumask);

	if (pool->flags & POOL_DISASSOCIATED)
		worker->flags |= WORKER_UNBOUND;

	list_add_tail(&worker->node, &pool->workers);
	mutex_unlock(&pool->attach_mutex);
}
```

**worker\_attach\_to\_pool**()函数最主要的工作是将**该worker工作线程**加入**worker\_pool\->workers链表**中。

**POOL\_DISASSOCIATED**是 **worker\-pool(工作线程池使用的！！！)内部使用的标志位**，**一个线程池**可以是**associated**状态或**disassociated**状态。associated状态的**线程池**表示有**绑定到某个CPU**上，disassociated状态的**线程池**表示**没有绑定某个CPU**, 也有可能是**绑定的CPU被offline(！！！**)了，因此可以在**任意CPU上运行(！！！**)。

回到create\_worker()函数中，位置7代码中的**nr\_workers**统计该**worker\_pool中的工作线程的个数**。注意这里nr\_workers变量需要用**spinlock锁**来保护，因为**每个worker\_pool**定义了一个**timer**，用于**动态删除过多的空闲的worker(！！！**)，见**idle\_worker\_timeout**()函数。

位置8，worker\_enter\_idle()函数**让该工作线程进入idle状态**。

位置9, wake\_up\_process()函数**唤醒该工作线程**。

# 2 创建工作队列

