[TOC]

思考题:

- workqueue是运行在中断上下文，还是进程上下文？其回调函数允许睡眠吗？
- 旧版本（Linux 2.6.25)的 workqueue机制在实际过程中遇到了哪些问题和挑战？
- CMWQ机制如何动态管理工作线程池的线程呢？
- 如果有多个work挂入一个工作线程中执行，当某个work的回调函数执行了阻塞操作，那么剩下的work该怎么办？

# 0 历史和原理概述

工作队列机制(workqueue)是除了软中断和tasklet以外最常用的一种下半部机制。工作队列的**基本原理**是把**work(需要推迟执行的函数**）交由一个**内核线程**来执行，它总是在**进程上下文**中执行。工作队列的优点是利用**进程上下文**来执行**中断下半部操作**，因此工作队列允许**重新调度**和**睡眠**，是异步执行的**进程上下文**，另外它还能解决**软中断**和**tasklet**执行时间过长导致**系统实时性下降**等问题。

当驱动程序或者内核子系统在进程上下文中有异步执行的工作任务时，可以使用**work item**来描述工作任务，包括该工作任务的执行回调函数，**把work item添加到一个队列**中，然后**一个内核线程**会去执行这个**工作任务的回调函数**。这里**work item被称为工作**，**队列被称为workqueue**，即工作队列，**内核线程被称为worker**。

工作队列最早是在Linux 2.5.x内核开发期间被引入的机制，早期的工作队列的设计比较简单，由**多线程（Multi threaded，每个CPU默认一个工作线程**）和**单线程（Single threaded, 用户可以自行创建工作线程**）组成。在长期测试中发现如下问题:

- **内核线程数量太多**。虽然系统中有默认的一套工作线程（kevents)，但是有很多驱动和子系统喜欢自行创建工作线程，例如调用create\_workqueue()函数，这样在大型系统(CPU数量比较多的机器)中可能内核启动结束之后就耗尽了系统PID资源。
- **并发性比较差**。Multi threaded的工作线程和CPU是一一绑定的，例如CPU0上的某个工作线程有A 、B 和 C 三个work。假设执行work A上回调函数时发生了睡眠和调度，CPU0就会调度出去执行其他的进程，对 于 B 和 C 来说，它们只能等待CPU0重新调度执行该工作线程，尽管其他CPU比较空闲，也没有办法迁移到其他CPU上执行。
- **死锁问题**。系统有一个默认的工作队列kevents, 如果有很多work运行在默认的工作队列kevents上，并且它们有一些数据上依赖关系，那么很有可能会产生死锁。解决办法是为每一个有可能产生死锁的work创建一个专职的工作线程，这样又回到问题1了。

为此社区专家Tejun Heo在Linux 2.6.36中提出了一套解决方案**concurrency\-managed workqueues(CMWQ**)。

执行**work任务的线程**称为**worker**或**工作线程**。**工作线程**会**串行化地执行**挂入到队列中**所有的work**。如果队列中**没有work**, 那么该**工作线程**就会变成**idle状态**。

为了管理众多**工作线程**，CMWQ提出了**工作线程池(worker\-pool**)概念，worker\-pool有**两种**，一是**BOUND类型**的，可以理解为**Per\-CPU类型**，每个CPU都有worker\-pool; 另一种是**UNBOUND类型**的，即不和具体CPU绑定。这**两种worker\-pool**都会定义**两个线程池**，一个给**普通优先级的work**使用，另一个给**高优先级的work**使用。这些工作线程池中的**线程数量**是**动态分配**和管理的，而不是固定的。当**工作线程睡眠**时，会去检查是否需要唤醒更多的工作线程，如有需要，会去**唤醒同一个工作线程池中idle状态**的工作线程。

# 1 初始化工作队列

## 1.1 工作任务struct work\_struct

**workqueue**机制**最小的调度单元是work item**, 有的书中称为工作任务，由struct work\_struct数据结构来抽象和描述，本章简称为work或工作任务。

```c
[include/linux/workqueue.h]
struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
};
```

struct work\_struct数据结构定义比较简单。

- data成员包括**两部分**，**低比特位部分**是work的**标志位**，**剩余的比特位**通常用于存放**上一次运行的worker\_pool**的**ID号**或**pool\_workqueue的指针**,存放的内容由**WORK\_STRUCT\_PWQ标志位来决定**。
- func是工作任务的处理函数
- entry用于把**work挂到其他队列**上。

## 1.2 工作线程struct worker

**work**运行在**内核线程**中，这个**内核线程在代码中被称为worker**, 类似流水线中的工人，work类似工人的工作，本章简称为**工作线程或worker**。

**工作线程**用**struct worker**数据结构来描述：

```c
[kernel/workqueue_internal.h]
struct worker {
	/* on idle list while idle, on busy hash table while busy */
	union {
		struct list_head	entry;	/* L: while idle */
		struct hlist_node	hentry;	/* L: while busy */
	};

	struct work_struct	    *current_work;	/* L: work being processed */
	work_func_t		        current_func;	/* L: current_work's fn */
	struct pool_workqueue	*current_pwq;   /* L: current_work's pwq */
	struct list_head	    scheduled;	    /* L: scheduled works */
    struct list_head	    node;		    /* A: anchored at pool->workers */
						                    /* A: runs through worker->node */
	struct task_struct	    *task;		    /* I: worker task */
	struct worker_pool	    *pool;		    /* I: the associated pool */
	int			            id;		        /* I: worker id */
    ...
};
```

- current\_work: 当前**正在处理的work**。
- current\_func: 当前**正在执行的work回调函数**。
- current\_pwq：当前**work所属的pool\_workqueue**。
- scheduled: 所有被调度并正**准备执行的work都挂入该链表**中。
- task: 该**工作线程**的task\_struct数据结构。
- pool: 该工作线程所属的**worker\_pool**。
- id: 工作线程的**ID号**。
- node: 可以把该worker挂入到**worker\_pool\->workers链表**中。

## 1.3 工作线程池struct worker\_pool

CMWQ提出了**工作线程池**概念，代码中使用struct worker\_pool数据结构来抽象和描述，本章简称worker\-pool或者工作线程池。

简化后的**struct worker\_pool**数据结构如下：

```c
[kernel/workqueue.c]
struct worker_pool {
	spinlock_t		lock;		/* the pool lock */
	int			    cpu;		/* I: the associated cpu */
	int			    node;		/* I: the associated node ID */
	int			    id;		    /* I: pool ID */
	unsigned int	flags;		/* X: flags */

	struct list_head	worklist;	/* L: list of pending works */
	int			        nr_workers;	/* L: total number of workers */
	int			        nr_idle;	/* L: currently idle ones */

	struct list_head	idle_list;	/* X: list of idle workers */
	struct list_head	workers;	/* A: attached workers */
	struct workqueue_attrs	*attrs;		/* I: worker attributes */
	atomic_t		nr_running ____cacheline_aligned_in_smp;
	struct rcu_head		rcu;
	...
} ____cacheline_aligned_in_smp;
```

- lock: 用于**保护worker\-pool的自旋锁**。
- cpu: 对应**BOUND类型**的**workqueue**来说，cpu表示**绑定的CPU ID**, 对应**UNBOUND类型**，该**值为\-1**。
- node: 对于**UNBOUND类型的workqueue**，node表示该**worker\-pool所属内存节点的ID**编号。
- id: 该**worker\-pool的ID号**。
- worklist: **pending状态**的**work**会挂入**该链表**中。
- nr\_workers: **工作线程的数量**。
- nr\_idle: 处于**idle状态**的**工作线程的数量**。
- idle\_list: 处于**idle状态**的**工作线程(！！！**)会挂入**该链表**中。
- workers: 该worker\-pool管理的**工作线程**会挂入**该链表**中。
- attrs: **工作线程的属性**。
- nr\_running: **统计计数**，用于管理**worker**的**创建和销毁**，表示**正在运行中的worker数量**。在**进程调度器**中**唤醒进程时(try\_to\_wake\_up**())，**其他CPU**有可能会**同时访问该成员**，该成员**频繁在多核之间读写**，因此让**该成员独占一个缓冲行(！！！**)，避免**多核CPU**在**读写该成员**时引发其他临近的成员“颠簸”现象，这也是所谓的“**缓存行伪共享**”的问题。
- rcu: RCU锁。

**worker\-pool是Per\-CPU**概念，每个CPU都有worker\-pool, 准确来说**每个CPU有两个worker\-pool**, 一个用于**普通优先级的工作线程**，另一个用于**高优先级的工作线程**。

```c
[kernel/workqueue.c]
/* the per-cpu worker pools */
static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS],
				     cpu_worker_pools);
```

## 1.4 连接workqueue(工作队列)和worker\-pool(工作线程池)的桥梁strct pool\_workqueue

CMWQ还定义了一个**pool\_workqueue**的数据结构，它是**连接workqueue和worker\-pool的枢纽**。

```c
[kernel/workqueue.c]
struct pool_workqueue {
	struct worker_pool	    *pool;		/* I: the associated pool */
	struct workqueue_struct *wq;		/* I: the owning workqueue */
	int			            nr_active;	/* L: nr of active works */
	int			            max_active;	/* L: max active works */
	struct list_head	    delayed_works;	/* L: delayed works */
	struct rcu_head		    rcu;
	...
} __aligned(1 << WORK_STRUCT_FLAG_BITS);
```

其中，**WORK\_STRUCT\_FLAG\_BITS为8**, 因此pool\_workqueue数据结构是按照**256Byte对齐**的，这样方便把该**数据结构指针的bit[8:31]位存放到work\->data**中，work\->data字段的**低8位**用于存放一些**标志位**，见set\_work\_pwq()和get\_work\_pwq()函数。

- pool: 指向**worker\-pool指针**。
- wq: 指向**所属的工作队列**。
- nr\_active: **活跃的work数量**。
- max\_active: **活跃的work最大数量**。
- delayed\_works: 链表头，被**延迟执行的works可以挂入该链表**。
- rcu: rcu锁。

## 1.5 工作队列struct workqueue\_struct

系统中**所有的工作队列**，包括系统**默认的工作队列**，例如system\_wq或system\_highpri\_wq等，以及驱动开发者新创建的工作队列，它们**共享一组worker\-pool**。而对于**BOUND类型的工作队列**，**每个CPU**只有**两个工作线程池**，**每个工作线程池**可以和**多个workqueue**对应，**每个workqueue**也**只能对应这几个工作线程池**。

**工作队列**由struct **workqueue\_struct**数据结构来描述：

```c
[kernel/workqueue.c]
struct workqueue_struct {
	struct list_head	pwqs;		/* WR: all pwqs of this wq */
	struct list_head	list;		/* PL: list of all workqueues */

	struct list_head	maydays;	/* MD: pwqs requesting rescue */
	struct worker		*rescuer;	/* I: rescue worker */

	struct workqueue_attrs	*unbound_attrs;	/* WQ: only for unbound wqs */
	struct pool_workqueue	*dfl_pwq;	/* WQ: only for unbound wqs */

	char			name[WQ_NAME_LEN]; /* I: workqueue name */

	/* hot fields used during command issue, aligned to cacheline */
	unsigned int		flags ____cacheline_aligned; /* WQ: WQ_* flags */
	struct pool_workqueue __percpu *cpu_pwqs; /* I: per-cpu pwqs */
	...
};
```

- pwqs: **所有的pool\-workqueue**数据结构都**挂入链表**中。
- list: **链表节点**。系统定义一个**全局的链表workqueues**，**所有的workqueue**挂入**该链表**。
- maydays: **所有rescue状态**下的**pool\-workqueue**数据结构**挂入该链表**。
- rescuer: **rescue内核线程**。**内存紧张**时**创建新的工作线程**可能会失败，如果**创建workqueue**时设置了**WQ\_MEM\_RECLAIM**标志位，那么**rescuer线程会接管这种情况**。
- unbound attrs: **UNBOUND类型属性**。
- dfl\_pwq: 指向**UNBOUND类型的pool\_workqueue**.
- name: 该**workqueue的名字**。
- flags: 标志位经常被**不同CPU访问**，因此要和**cache line对齐**。标志位包括WQ\_UNBOUND、WQ\_HIGHPRI、WQ\_FREEZABLE等。
- cpu\_pwqs: 指向**Per\-CPU类型**的**pool workqueue**。

## 1.6 数据结构关系图

**一个work挂入workqueue**中，最终还要**通过worker\-pool**中的**工作线程来处理其回调函数**，worker-pool是**系统共享的(！！！**)，因此**workqueue**需要查找到一个**合适的worker\-pool**，然后从worker\-pool中分派一个**合适的工作线程**，pool\_workqueue数据结构在其中起到**桥梁**作用。这有些类似IT类公司的人力资源池的概念，具体关系如图5.7所示。

![config](./images/11.png)

## 1.7 系统初始化几个默认的workqueue

总结:

(1) 创建一个**pool\_workqueue结构的slab缓存对象**

(2) 为**所有可用CPU**创建**两个工作线程池**struct worker\_pool(**普通优先级**的和**高优先级**的)并初始化

(3) 为**每个在线CPU**的**每个工作线程池**分别创建**一个工作线程**(调用**create\_worker**(), 详细见下面)

(4) 创建**UNBOUND类型**和**ordered类型**的**workqueue属性**

(5) 创建几个**默认的workqueue**, 调用alloc\_workqueue()

在**系统启动**时，会通过**init\_workqueues**()函数来**初始化几个系统默认的workqueue**。

```c
[kernel/workqueue.c]
static int __init init_workqueues(void)
{
	int std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };
	int i, cpu;
    // 位置1
	pwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);

	cpu_notifier(workqueue_cpu_up_callback, CPU_PRI_WORKQUEUE_UP);
	hotcpu_notifier(workqueue_cpu_down_callback, CPU_PRI_WORKQUEUE_DOWN);
    // 位置2
	wq_numa_init();

	/* initialize CPU pools */
	// 位置3
	for_each_possible_cpu(cpu) {
		struct worker_pool *pool;

		i = 0;
		// 位置4
		for_each_cpu_worker_pool(pool, cpu) {
			BUG_ON(init_worker_pool(pool));
			pool->cpu = cpu;
			cpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));
			pool->attrs->nice = std_nice[i++];
			pool->node = cpu_to_node(cpu);

			/* alloc pool ID */
			mutex_lock(&wq_pool_mutex);
			BUG_ON(worker_pool_assign_id(pool));
			mutex_unlock(&wq_pool_mutex);
		}
	}

	/* create the initial worker */
	// 位置5
	for_each_online_cpu(cpu) {
		struct worker_pool *pool;

		for_each_cpu_worker_pool(pool, cpu) {
			pool->flags &= ~POOL_DISASSOCIATED;
			BUG_ON(!create_worker(pool));
		}
	}
    // 位置6
	/* create default unbound and ordered wq attrs */
	for (i = 0; i < NR_STD_WORKER_POOLS; i++) {
		struct workqueue_attrs *attrs;

		BUG_ON(!(attrs = alloc_workqueue_attrs(GFP_KERNEL)));
		attrs->nice = std_nice[i];
		unbound_std_wq_attrs[i] = attrs;

		BUG_ON(!(attrs = alloc_workqueue_attrs(GFP_KERNEL)));
		attrs->nice = std_nice[i];
		attrs->no_numa = true;
		ordered_wq_attrs[i] = attrs;
	}
    // 位置7
	system_wq = alloc_workqueue("events", 0, 0);
	system_highpri_wq = alloc_workqueue("events_highpri", WQ_HIGHPRI, 0);
	system_long_wq = alloc_workqueue("events_long", 0, 0);
	system_unbound_wq = alloc_workqueue("events_unbound", WQ_UNBOUND,
					    WQ_UNBOUND_MAX_ACTIVE);
	system_freezable_wq = alloc_workqueue("events_freezable",
					      WQ_FREEZABLE, 0);
	system_power_efficient_wq = alloc_workqueue("events_power_efficient",
					      WQ_POWER_EFFICIENT, 0);
	system_freezable_power_efficient_wq = alloc_workqueue("events_freezable_power_efficient",
					      WQ_FREEZABLE | WQ_POWER_EFFICIENT,
					      0);
	return 0;
}
early_initcall(init_workqueues);
```

位置1, 创建一个**pool\_workqueue**数据结构的**slab缓存对象**。

位置2, **workqueue**考虑了**NUMA系统**情况的一些特殊处理。

位置3, 为系统中**所有可用的CPU**(cpu\_possible\_mask) 分别**创建struct worker\_pool数据结构**. 

位置4, for\_each\_cpu\_worker\_pool()为**每个CPU**创建**两个worker\_pool**, 一个是**普通优先级**的工作线程池, 另一个是**高优先级**的工作线程池, **init\_worker\_pool**()函数用于**初始化一个worker\_pool**. 

位置4的**for\_each\_cpu\_worker\_pool**宏**遍历CPU中两个worker\_pool**:

```c
[kernel/workqueue.c]
#define for_each_cpu_worker_pool(pool, cpu)				\
	for ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];		\
	     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \
	     (pool)++)
```

位置5, 为系统每一个**在线(online)CPU**中的**每个worker\_pool**分别**创建一个工作线程**。

位置6, 创建**UNBOUND类型**和**ordered类型的workqueue属性**，**ordered类型**的**workqueue**表示**同一个时刻只能有一个work item在运行(！！！**)。

位置7到最后, **创建系统默认的workqueue**，这里使用**创建工作队列**的API函数**alloc\_workqueue**().

- **普通优先级BOUND类型**的**工作队列system\_wq**, 名称为“**events**”，可以理解为**默认工作队列**。
- **高优先级BOUND类型**的工作队列**system\_highpri\_wq** ，名称为“**events\_highpri**”。
- **UNBOUND类型**的工作队列**system\_unbound\_wq**，名称为“**system\_unbound\_wq**”。
- **Freezable类型**的工作队列**system\_freezable\_wq**，名称为“**events\_freezable**”。
- **省电类型**的工作队列**system\_freezable\_wq**，名称为 “**events\_power\_efficient**”。

### 1.7.1 create\_worker()创建工作线程

总结:

(1) 获取一个**ID**

(2) **工作线程池对应的内存节点**分配一个**worker**

(3) 给**worker设置名字**

(4) 在**工作线程池对应的内存节点**上创建一个**内核线程给分配的worker**, 执行函数为**worker\_thread**, 参数为**worker(struct worker**), 内核线程名字是"**kworker/*worker名字***"

(5) 设置**线程(worker\->task\->flags**)的**PF\_NO\_SETAFFINITY**标志位, **防止修改CPU亲和性**

(6) 将创建的**worker挂到worker\_pool**: **线程池**如果**没有绑定到某个CPU**, 那么设置**worker不绑定CPU**, 可在任意CPU上运行; 将**worker**加到**工作线程池的workers链表**

(7) 使**worker进入idle状态**

(8) **唤醒worker的内核线程**

(9) 返回该worker

上面位置5, 会为**每个online的CPU**的**每个worker\_pool**分别创建**一个工作线程**.

下面来看**create\_worker**()函数是如何创建工作线程的。

```c
[init_workqueues()->create_worker()]
static struct worker *create_worker(struct worker_pool *pool)
{
	struct worker *worker = NULL;
	int id = -1;
	char id_buf[16];
    // 位置1
	/* ID is needed to determine kthread name */
	id = ida_simple_get(&pool->worker_ida, 0, 0, GFP_KERNEL);
    // 位置2
	worker = alloc_worker(pool->node);

	worker->pool = pool;
	worker->id = id;
    // 位置3
	if (pool->cpu >= 0)
		snprintf(id_buf, sizeof(id_buf), "%d:%d%s", pool->cpu, id,
			 pool->attrs->nice < 0  ? "H" : "");
	else
		snprintf(id_buf, sizeof(id_buf), "u%d:%d", pool->id, id);
    // 位置4
	worker->task = kthread_create_on_node(worker_thread, worker, pool->node,
					      "kworker/%s", id_buf);

	set_user_nice(worker->task, pool->attrs->nice);

	/* prevent userland from meddling with cpumask of workqueue workers */
	// 位置5
	worker->task->flags |= PF_NO_SETAFFINITY;

	/* successful, attach the worker to the pool */
	// 位置6
	worker_attach_to_pool(worker, pool);

	/* start the newly created worker */
	spin_lock_irq(&pool->lock);
	// 位置7
	worker->pool->nr_workers++;
	// 位置8
	worker_enter_idle(worker);
	// 位置9
	wake_up_process(worker->task);
	spin_unlock_irq(&pool->lock);

	return worker;
}
```

位置1, 通过**IDA子系统**获取一个**ID号**。

位置2, 在**worker\_pool**对应的**内存节点中分配一个worker**数据结构。

位置3到位置4之间，**pool\->cpu \>= 0**, 表示**BOUND类型的工作线程**。worker的名字一般是 “**kworker/ \+ CPU\_ID \+ worker\_id**”，如果属于**高优先级**类型的workqueue，即**nice值小于 0**，那么还要**加上“H**”。 **pool\->cpu \< 0**，表示**UNBOUND类型的工作线程**，名字为“**kworker/u + CPU\_ID + worker\_id**”。

位置4，通过**kthread\_create\_on\_node**()函数在**工作线程池对应的node！！！**中**创建一个内核线程用于worker**，在这个内存节点上分配该内核线程相关的struct task\_struct等数据结构。

注意, **线程执行函数为worker\_thread！！！worker(struct worker)是执行函数的参数, 在工作线程池对应的node上创建, 线程名是位置3设置的！！！**

位置5，设置**工作线程(task的flags！！！**)的**PF\_NO\_SETAFFINITY**标志位，**防止用户程序修改其CPU亲和性**。在**位置6**代码中会设置**这个worker允许运行的cpumask(！！！**)。

位置6，**worker\_attach\_to\_pool**()函数把刚分配的**工作线程**挂入**worker\_pool**中。

```c
[create_worker() ->worker_attach_to_pool()]
static void worker_attach_to_pool(struct worker *worker,
				   struct worker_pool *pool)
{
	mutex_lock(&pool->attach_mutex);
	set_cpus_allowed_ptr(worker->task, pool->attrs->cpumask);

	if (pool->flags & POOL_DISASSOCIATED)
		worker->flags |= WORKER_UNBOUND;

	list_add_tail(&worker->node, &pool->workers);
	mutex_unlock(&pool->attach_mutex);
}
```

**worker\_attach\_to\_pool**()函数最主要的工作是将**该worker工作线程**加入**worker\_pool\->workers链表**中。

**POOL\_DISASSOCIATED**是**worker\-pool(工作线程池使用的！！！)内部使用的标志位**，**一个线程池**可以是**associated**状态或**disassociated**状态。associated状态的**线程池**表示有**绑定到某个CPU**上, disassociated状态的**线程池**表示**没有绑定某个CPU**, 也有可能是**绑定的CPU被offline(！！！**)了，因此可以在**任意CPU上运行(！！！**)。

回到create\_worker()函数中，位置7代码中的**nr\_workers**统计该**worker\_pool中的工作线程的个数**。注意这里nr\_workers变量需要用**spinlock锁**来保护，因为**每个worker\_pool**定义了一个**timer**，用于**动态删除过多的空闲的worker(！！！**)，见**idle\_worker\_timeout**()函数。

位置8，worker\_enter\_idle()函数**让该工作线程进入idle状态**。

位置9, wake\_up\_process()函数**唤醒该工作线程**。

# 2 创建工作队列workqueue

创建工作队列API有很多，并且基本上和旧版本的workqueue兼容。

```c
[include/linux/workqueue.h]
#define alloc_workqueue(fmt, flags, max_active, args...)		\
	__alloc_workqueue_key((fmt), (flags), (max_active),		\
			      NULL, NULL, ##args)

#define alloc_ordered_workqueue(fmt, flags, args...)			\
	alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)

#define create_workqueue(name)						\
	alloc_workqueue("%s", WQ_MEM_RECLAIM, 1, (name))
#define create_freezable_workqueue(name)				\
	alloc_workqueue("%s", WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, \
			1, (name))
#define create_singlethread_workqueue(name)				\
	alloc_ordered_workqueue("%s", WQ_MEM_RECLAIM, name)
```

最常见是**alloc\_workqueue**(), 有3个参数, 分别是**name, flags和max\_active**. 其他API和该API类似, **只是调用的flags不相同(！！！**).

(1) **WQ\_UNBOUND**: **工作任务work**会加入**UNBOUND工作队列**中，UNBOUND工作队列的**工作线程没有绑定到具体的CPU**上。UNBOUND类型的work**不需要额外的同步管理**，UNBOUND工作线程池会尝试尽快执行它的work。**这类work会牺牲一部分性能**（局部原理带来的性能提升），但是比较适用于如下场景。

- **一些应用**会**在不同的CPU上跳跃**，这样如果**创建Bound类型的工作队列**，会创建**很多没用的工作线程**。
- **长时间运行**的**CPU消耗类型的应用**（标记**WQ\_CPU\_INTENSIVE标志位**）通常会创建UNBOUND类型的workqueue, 进程调度器会管理这类工作线程在哪个CPU上运行。

(2) **WQ\_FREEZABLE**: 一个标记着WQ\_FREEZABLE的工作队列会参与到**系统的suspend过程**中，这会让**工作线程**处理完成**当前所有的work**才完成**进程冻结**，并且这个过程**不会再新开始一个work**的执行，直到**进程被解冻**。

(3) **WQ\_MEM\_RECLAIM**: 当**内存紧张**时，创建**新的工作线程可能会失败**，系统还有一个**rescuer内核线程**会去接管这种情况。

(4) **WQ\_HIGHPRI**: 属于**高优先级的worker\-pool**, 即比较**低的nice值**。

(5) **WQ\_CPU\_INTENSIVE**: 属于**特别消耗CPU资源**的一类work, 这类work的执行会得到**系统进程调度器的监管**。排在这类work后面的**non\-CPU\-intensive类型**的work可能会**推迟执行**。

(6) \_\_**WQ\_ORDERED**: 表示**同一个时间只能执行一个work item**。

参数**max\_active**也值得关注，它决定**每个CPU最多可以有多少个work**挂入一个**工作队列**中。例如**max\_active=16**，说明**每个CPU最多可以有16个work**挂入到**工作队列中执行**。通常对于BOUND类型的工作队列，max\_active最大可以是512，如果max\_active参数传入0，则表示指定为256。对于UNBOUND类型工作队列，max\_active可以取512和4 \* num\_possible\_cpus()之间的最大值。通常建议驱动开发者使用max\_active=0作为参数，有些驱动开发者希望使用一个严格串行执行的工作队列，alloc\_ordered\_workqueue() API可以满足这方面的需求，这里使用max\_active=1和WQ\_UNBOUND的组合，同一时刻只有一个work可以执行。

```c
[kernel/workqueue.c]
struct workqueue_struct *__alloc_workqueue_key(const char *fmt,
					       unsigned int flags,
					       int max_active,
					       struct lock_class_key *key,
					       const char *lock_name, ...)
{
	size_t tbl_size = 0;
	va_list args;
	struct workqueue_struct *wq;
	struct pool_workqueue *pwq;

	/* see the comment above the definition of WQ_POWER_EFFICIENT */
	// 位置1
	if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)
		flags |= WQ_UNBOUND;

	/* allocate wq and format name */
	if (flags & WQ_UNBOUND)
		tbl_size = nr_node_ids * sizeof(wq->numa_pwq_tbl[0]);

	wq = kzalloc(sizeof(*wq) + tbl_size, GFP_KERNEL);
	if (!wq)
		return NULL;

	if (flags & WQ_UNBOUND) {
		wq->unbound_attrs = alloc_workqueue_attrs(GFP_KERNEL);
		if (!wq->unbound_attrs)
			goto err_free_wq;
	}

	va_start(args, lock_name);
	vsnprintf(wq->name, sizeof(wq->name), fmt, args);
	va_end(args);

	max_active = max_active ?: WQ_DFL_ACTIVE;
	max_active = wq_clamp_max_active(max_active, flags, wq->name);

	/* init wq */
	wq->flags = flags;
	wq->saved_max_active = max_active;
	mutex_init(&wq->mutex);
	atomic_set(&wq->nr_pwqs_to_flush, 0);
	INIT_LIST_HEAD(&wq->pwqs);
	INIT_LIST_HEAD(&wq->flusher_queue);
	INIT_LIST_HEAD(&wq->flusher_overflow);
	INIT_LIST_HEAD(&wq->maydays);

	lockdep_init_map(&wq->lockdep_map, lock_name, key, 0);
	INIT_LIST_HEAD(&wq->list);

	if (alloc_and_link_pwqs(wq) < 0)
		goto err_free_wq;
```

位置1, **WQ\_POWER\_EFFICIENT**标志位考虑系统的功耗问题。对于BOUND类型的workqueue, 它是Per\-CPU类型的，会利用cache的局部性原理来提高性能。也就是说，它不会从这个CPU迁移到另外一个CPU, 也不希望进程调度器来打扰它们。设置成UNBOUND类型的workqueue后，宄竟选择哪个CPU上唤醒交由进程调度器决定。Per\-CPU类型的workqueue会让idle状态的CPU从idle状态唤醒，从而增加了功耗。如果系统配置了CONNG\_WQ\_POWER\_EFFICIENT\_DEFAULT选项，那么创建workqueue会把标记了WQ\_POWER\_EFFIOENT的workqueue设置成UNBOUND类型，这样进程调度器就可以参与选择CPU来执行.

接下来是分配一个workqueue\_struct数据结构并初始化。

```c
[alloc_workqueue() ->alloc_and_link_pwqs()]
[kernel/workqueue.c]
static int alloc_and_link_pwqs(struct workqueue_struct *wq)
{
	bool highpri = wq->flags & WQ_HIGHPRI;
	int cpu, ret;
    // 位置1
	if (!(wq->flags & WQ_UNBOUND)) {
		wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);
		
		for_each_possible_cpu(cpu) {
			struct pool_workqueue *pwq =
				per_cpu_ptr(wq->cpu_pwqs, cpu);
			struct worker_pool *cpu_pools =
				per_cpu(cpu_worker_pools, cpu);

			init_pwq(pwq, wq, &cpu_pools[highpri]);

			mutex_lock(&wq->mutex);
			link_pwq(pwq);
			mutex_unlock(&wq->mutex);
		}
		return 0;
	// 位置2
	} else if (wq->flags & __WQ_ORDERED) {
		ret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);
		/* there should only be single pwq for ordering guarantee */
		WARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||
			      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),
		     "ordering guarantee broken for workqueue %s\n", wq->name);
		return ret;
	// 位置3
	} else {
		return apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);
	}
}
```

位置1的整个if, 处理BOUND类型的workqueue。cpu\_pwqs是一个Per\-CPU类型的指针，alloc\_percpu()为每一个CPU分配一个pool\_workqueue数据结构。cpu\_worker\_pools是系统静态定义的Per\-CPU类型的worker\_pool数据结构，wq\->cpu\_pwqs是动态分配的Per\-CPU类型的pool\_workqueue数据结构。init\_pwq()函数把这两个数据结构连接起来，即pool\_workqueue-〉pool指向worker\_pool数据结构，pool\_workqueue-〉wq指向workqueue\_struct数据结构。link\_pwq()函数主要是把pool\_workqueue添加到workqueue\_struct\-> pwqs链表中。

位置2和位置3处理ORDERED类型和UNBOUND类型的workqueue, 都通过调用apply\_workqueue\_attrs()函数来实现, 代码片段如下:

```c
[alloc_workqueue() -> alloc_and_link_pwqs -> apply_workqueue_attrs()]
int apply_workqueue_attrs(struct workqueue_struct *wq,
			  const struct workqueue_attrs *attrs)
{
	struct workqueue_attrs *new_attrs, *tmp_attrs;
	struct pool_workqueue **pwq_tbl, *dfl_pwq;
	int node, ret;

	pwq_tbl = kzalloc(nr_node_ids * sizeof(pwq_tbl[0]), GFP_KERNEL);

	mutex_lock(&wq_pool_mutex);

	dfl_pwq = alloc_unbound_pwq(wq, new_attrs);

	for_each_node(node) {
		dfl_pwq->refcnt++;
		pwq_tbl[node] = dfl_pwq;
	}

	mutex_unlock(&wq_pool_mutex);

    mutex_lock(&wq->mutex);

	copy_workqueue_attrs(wq->unbound_attrs, new_attrs);

	/* save the previous pwq and install the new one */
	for_each_node(node)
		pwq_tbl[node] = numa_pwq_tbl_install(wq, node, pwq_tbl[node]);

	/* @dfl_pwq might not have been used, ensure it's linked */
	link_pwq(dfl_pwq);
	swap(wq->dfl_pwq, dfl_pwq);

	mutex_unlock(&wq->mutex);

	/* put the old pwqs */
	for_each_node(node)
		put_pwq_unlocked(pwq_tbl[node]);
	put_pwq_unlocked(dfl_pwq);

	put_online_cpus();
	ret = 0;
	return ret;
}
```

首先分配一个pool\_workqueue数据结构, 然后调用alloc\_unbound\_pwq()来查找或新建一个pool\_workqueue.

```c
[apply_workqueue_attrs() ->alloc_unbound_pwq()]
[kernel/workqueue.c]
static struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,
					const struct workqueue_attrs *attrs)
{
	struct worker_pool *pool;
	struct pool_workqueue *pwq;

	pool = get_unbound_pool(attrs);

	pwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);
	init_pwq(pwq, wq, pool);
	return pwq;
}
```

首先通过get\_unbound\_pool()去系统中查找有没有相同属性的worker\_pool。

```c
[kernel/workqueue.c]
static struct worker_pool *get_unbound_pool(const struct workqueue_attrs *attrs)
{
	u32 hash = wqattrs_hash(attrs);
	struct worker_pool *pool;
	int node;

	/* do we already have a matching pool? */
	hash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {
		if (wqattrs_equal(pool->attrs, attrs)) {
			pool->refcnt++;
			return pool;
		}
	}

	/* nope, create a new one */
	pool = kzalloc(sizeof(*pool), GFP_KERNEL);

	if (worker_pool_assign_id(pool) < 0)
		goto fail;

	/* create and start the initial worker */
	if (!create_worker(pool))
		goto fail;

	/* install */
	hash_add(unbound_pool_hash, &pool->hash_node, hash);

	return pool;
fail:
	if (pool)
		put_unbound_pool(pool);
	return NULL;
}
```

系统定义了一个哈希表unbound\_pool\_hash，用于管理系统中所有的UNBOUND类型的worker\_pool，通过wqattrs\_equal()判断系统中是否己经有了类型相关的worker\_pool, 如果没有，那就重新分配和初始化一个。wqattrs\_equal()函数首先会比较nice值，然后比较cpumask位图是否一致。

回到 alloc\_unbound\_pwq()函数中，找到worker\_pool后还需要一个连接器pool\_workqueue，最后通过 init\_pwq()函数把 worker\_pool 和 workqueue\_struct 串联起来。

回到 apply\_workqueue\_attrs()函数中的numa\_pwq\_tbl\_install()函数。

```c
[kernel/workqueue.c]
static struct pool_workqueue *numa_pwq_tbl_install(struct workqueue_struct *wq,
						   int node,
						   struct pool_workqueue *pwq)
{
	struct pool_workqueue *old_pwq;

	lockdep_assert_held(&wq->mutex);

	/* link_pwq() can handle duplicate calls */
	link_pwq(pwq);

	old_pwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);
	rcu_assign_pointer(wq->numa_pwq_tbl[node], pwq);
	return old_pwq;
}
```

link\_pwq()把找到的 pool\_workqueue 添加到 workqueue\_struct\-〉pwqs链表中。接下来利用RCU锁机制来保护pool\_workqueue数据结构，首先old\_pwq和pwq\_tbl\[node\]指向wq\->numa\_pwq\_tbl\[node\]中旧的数据，rcu\_assign\_pointer()之后wq\-〉numa\_pwq\_tbl[node]指针指向新的数据。那RCU什么时候会删除旧数据呢？看apply\_workqueue\_attrs()函数的第33行代码，其中参数pwq\_tbl\[node\]指向旧数据。

```c
[put_pwq_unlocked() ->put_pwq()]
[kernel/workqueue.c]
static void put_pwq(struct pool_workqueue *pwq)
{
	lockdep_assert_held(&pwq->pool->lock);
	if (likely(--pwq->refcnt))
		return;
	if (WARN_ON_ONCE(!(pwq->wq->flags & WQ_UNBOUND)))
		return;

	schedule_work(&pwq->unbound_release_work);
}
```

当pool\_workqueue\->refcnt成员计数小于0时，会通过schedule\_work()调度一个系统默认的work，每个pool\_workqueue有初始化一个work，见init\_pwq()函数。

```c
[kernel/workqueue.c]
static void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,
		     struct worker_pool *pool)
{
	BUG_ON((unsigned long)pwq & WORK_STRUCT_FLAG_MASK);

	memset(pwq, 0, sizeof(*pwq));

	pwq->pool = pool;
	pwq->wq = wq;
	pwq->flush_color = -1;
	pwq->refcnt = 1;
	INIT_LIST_HEAD(&pwq->delayed_works);
	INIT_LIST_HEAD(&pwq->pwqs_node);
	INIT_LIST_HEAD(&pwq->mayday_node);
	INIT_WORK(&pwq->unbound_release_work, pwq_unbound_release_workfn);
}
```

直接看该 work 的回调函数pwq\_unbound\_release\_workfn()

```c
[put_pwq_unlocked() ->put_pwq() ->pwq_unbound_release_workfn()]
[kernel/workqueue.c]
static void pwq_unbound_release_workfn(struct work_struct *work)
{
	struct pool_workqueue *pwq = container_of(work, struct pool_workqueue,
						  unbound_release_work);
	struct workqueue_struct *wq = pwq->wq;
	struct worker_pool *pool = pwq->pool;
	bool is_last;

	if (WARN_ON_ONCE(!(wq->flags & WQ_UNBOUND)))
		return;

	mutex_lock(&wq_pool_mutex);
	put_unbound_pool(pool);
	mutex_unlock(&wq_pool_mutex);

	call_rcu_sched(&pwq->rcu, rcu_free_pwq);

	if (is_last) {
		free_workqueue_attrs(wq->unbound_attrs);
		kfree(wq);
	}
}
```

首先从work中找到pool\_workqueue数据结构指针pwq，注意该work只对UNBOUND类型的workqueue有效。当有需要释放pool\_workqueue数据结构时，会调用call\_rcu\_sched()来对旧数据进行保护，让所有访问该旧数据的读临界区都经历过了Grace Period之后才会释放旧数据。

# 3 调度一个work

Linux内核推荐驱动开发者使用默认的workqueue, 而不是新创建workqueue。要使用系统默认的workqueue，首先需要初始化一个work, 内核提供了相应的宏INIT\_WORK()。

```c
[include/linux/workqueue.h]
#define INIT_WORK(_work, _func)						\
	__INIT_WORK((_work), (_func), 0)

#define __INIT_WORK(_work, _func, _onstack)				\
	do {								\
		__init_work((_work), _onstack);				\
		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
		INIT_LIST_HEAD(&(_work)->entry);			\
		(_work)->func = (_func);				\
	} while (0)
```

struct work\_struct数据结构不复杂，主要是对data、entry和回调函数fUnc的赋值。Data成员被划分成两个域，低比特位域用于存放work相关的flags, 高比特位域用于存放上次执行该work的worker\_pool的ID号或保存上一次pool\_workqueue数据结构指针。

```c
[include/linux/workqueue.h]
enum {
	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
	WORK_STRUCT_DELAYED_BIT	= 1,	/* work item is delayed */
	WORK_STRUCT_PWQ_BIT	= 2,	/* data points to pwq */
	WORK_STRUCT_LINKED_BIT	= 3,	/* next work is linked to this one */
	WORK_STRUCT_COLOR_SHIFT	= 4,	/* color for workqueue flushing */
	WORK_STRUCT_COLOR_BITS	= 4,
    ...
	WORK_OFFQ_FLAG_BITS	= 1,
	...
};
```

以32bit的CPU来说，当data字段包含WORK\_STRUCT\_PWQ\_BIT标志位时，表示高比特位域保存着上一次pool\_workqueue数据结构指针，这时低8位用于存放一些标志位。当data字段没有包含WORK\_STRUCT\_PWQ\_BIT标志位时，表示其高比特位域存放上次执行该work的worker\_pool的ID号，低5位用于存放一些标志位，见get\_work\_pool()函数。

常见的标志位如下。

- WORK\_STRUCT\_PENDING\_BIT: 表示该work正在pending执行。
- WORK\_STRUCT\_DELAYED\_BIT: 表示该work被延迟执行了。
- WORK\_STRUCT\_PWQ\_BIT: 表示work的data成员指向pwqs数据结构的指针，其中pwqs需要按照256Byte对齐，这样pwqs指针的低8位可以忽略，只需要其余的比特位就可以找回pwqs指针。 struct pool\_workqueue数据结构按照256Byte对齐。
- WORK\_STRUCT\_LINKED\_BIT：表示下一个work连接到该work上。

初始化完一个work后，就可以调用schedule\_work()函数来把work挂入系统的默认的workqueue中。

```c
[include/linux/workqueue.h]
static inline bool schedule_work(struct work_struct *work)
{
	return queue_work(system_wq, work);
}
```

schedule\_work()函数把work挂入系统默认BOUND类型的工作队列system\_wq中，该工作队列是在init\_workqueues()时创建的。

```c
[schedule_work() - >queue_work()]
[include/linux/workqueue.h]
static inline bool queue_work(struct workqueue_struct *wq,
			      struct work_struct *work)
{
	return queue_work_on(WORK_CPU_UNBOUND, wq, work);
}
```

queue\_work()有3个参数，其中WORK\_CPU\_UNBOUND表示不绑定到任何CPU上，建议使用本地CPU。WORK\_CPU\_UNBOUND宏容易让人产生混淆，其定义为NR\_CPUS。wq指工作队列，work是新创建的工作。

```c
[schedule_work() ->queue_work() ->queue_work_on()]
[kernel/workqueue.c]
bool queue_work_on(int cpu, struct workqueue_struct *wq,
		   struct work_struct *work)
{
	bool ret = false;
	unsigned long flags;

	local_irq_save(flags);

	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
		__queue_work(cpu, wq, work);
		ret = true;
	}

	local_irq_restore(flags);
	return ret;
}
EXPORT_SYMBOL(queue_work_on);
```

把 work加入工作队列中是在关闭本地中断下运行的。如果开中断，那么有可能在处理中断返回时调度其他进程，其他进程有可能调用cancel\_delayed\_work()把PENDING位偷走，这种情况在稍后介绍cancel\_delayed\_work()时再详细描述。如果该work己经设置WORK\_STRUCT\_PENDING\_BIT标志位，说明该work己经在工作队列中，不需要重复添加。test\_and\_set\_bit()函数设置WORK\_STRUCT\_PENDING\_BIT标志位并返回旧值。

```c
[schedule_work() ->queue_work() ->queue_work_on() ->__queue_work()]
[kernel/workqueue.c]
static void __queue_work(int cpu, struct workqueue_struct *wq,
			 struct work_struct *work)
{
	struct pool_workqueue *pwq;
	struct worker_pool *last_pool;
	struct list_head *worklist;
	unsigned int work_flags;
	unsigned int req_cpu = cpu;

	WARN_ON_ONCE(!irqs_disabled());

	debug_work_activate(work);

	/* if draining, only works from the same workqueue are allowed */
	if (unlikely(wq->flags & __WQ_DRAINING) &&
	    WARN_ON_ONCE(!is_chained_work(wq)))
		return;
```

第 9 行代码要判断当前运行状态是否处于关中断状态，为什么\_\_queue\_work()要运行在关中断的状态下呢？读者可以先思考一下，这个问题稍后讲述cancel\_work\_sync()函数时再详细介绍。

\_\_WQ\_DRAINING标志位表不要销毁workqueue，那么挂入workqueue中所有的work都要处理完毕才能把这个workqueue销毁。在销毁过程中, —般不允许再有新的work加入队列中，有一种特例情况是正在清空work时又触发了一个queue work操作，这种情况被称为chained work。

```c
[__queue_work() ]
retry:
	if (req_cpu == WORK_CPU_UNBOUND)
		cpu = raw_smp_processor_id();

	/* pwq which will be used unless @work is executing elsewhere */
	if (!(wq->flags & WQ_UNBOUND))
		pwq = per_cpu_ptr(wq->cpu_pwqs, cpu);
	else
		pwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));

	last_pool = get_work_pool(work);
	if (last_pool && last_pool != pwq->pool) {
		struct worker *worker;

		spin_lock(&last_pool->lock);

		worker = find_worker_executing_work(last_pool, work);

		if (worker && worker->current_pwq->wq == wq) {
			pwq = worker->current_pwq;
		} else {
			/* meh... not running there, queue here */
			spin_unlock(&last_pool->lock);
			spin_lock(&pwq->pool->lock);
		}
	} else {
		spin_lock(&pwq->pool->lock);
	}

	if (unlikely(!pwq->refcnt)) {
		if (wq->flags & WQ_UNBOUND) {
			spin_unlock(&pwq->pool->lock);
			cpu_relax();
			goto retry;
		}
		/* oops */
		WARN_ONCE(true, "workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt",
			  wq->name, cpu);
	}
```

pool\_workqueue数据结构是桥梁枢纽，想把work加入到workqueue中，首先需要找到一个合适的pool\_workqueue枢纽。对于BOUND类型的workqueue，直接使用本地CPU对应的pool\_workqueue枢纽;如果是UNOUND类型的workqueue，调用unbound\_pwq\_by\_node()函数来寻找本地node节点对应的UNBOUND类型的pool\_workqueue.

```c
[kernel/workqueue.c]
static struct pool_workqueue *unbound_pwq_by_node(struct workqueue_struct *wq,
						  int node)
{
	return rcu_dereference_raw(wq->numa_pwq_tbl[node]);
}
```

对于UNBOUND类型的workqueue，workqueue\_struct数据结构中的numa\_pwq\_tbl\[\]数组存放着每个系统node节点对应的UNBOUND类型的pool\_workqueue枢纽。

第25〜42行代码，每个work\_struct数据结构的data成员可以用于记录worker\_pool的ID号，那么get\_work\_pool()函数可以用于查询该work上一次是在哪个worker\_pool中运行的。

```c
[kernel/workqueue.c]
static struct worker_pool *get_work_pool(struct work_struct *work)
{
	unsigned long data = atomic_long_read(&work->data);
	int pool_id;

	pool_id = data >> WORK_OFFQ_POOL_SHIFT;
	if (pool_id == WORK_OFFQ_POOL_NONE)
		return NULL;

	return idr_find(&worker_pool_idr, pool_id);
}
```

第 25行代码，返回该work上一次运行的worker\_pool。这里有一种情况，就是发现上一次运行的worker\_pool和这一次运行该work的pwq\->pool不一致。例如上一次是在CPU0对应的worker\_pool，这一次是在CPU1上的worker\_pool，这种情况下就要考查work是不是正运行在CPU0的worker\_pool中的某个工作线程里。如果是，那么这次work应该继续添加到CPU0上的worker\_pool上。find\_worker\_executing\_work()判断一个work是否在某个worker\_pool上正在运行，如果是，则返回这个正在执行的工作线程，这样可以利用其缓存热度。

```c

```

# 4 取消一个work

# 5 和调度器的交互

# 6 小结

在驱动开发中使用workqueue是比较简单的，特别是**使用系统默认的工作队列system\_wq**, 步骤如下。

- 使用**INIT\_WORK**()宏声明一个work和该work的回调函数。
- **调度一个work**: **schedule\_work**()。
- **取消一个work**: **cancel\_work\_sync**()
 
此外，有的驱动程序还**自己创建一个workqueue**，特别是**网络子系统**、**块设备子系统**等。

- 使用**alloc\_workqueue**()创建**新的workqueue**。
- 使用**INIT\_WORK**()宏声明一个**work**和**该work的回调函数**。
- 在新workqueue上**调度一个work**: **queue\_work**()
- **flush workqueue**上**所有work**: flush\_workqueue()

Linux内核还提供一个**workqueue机制**和**timer机制**结合的**延时机制delayed\_work**

要**理解CMWQ机制**，首先要明白旧版本的workqueue机制遇到了哪些问题，其次要清楚CMWQ机制中几个重要数据结构的关系. **CMWQ机制**把**workqueue**划分为**BOUND类型**和**UNBOUND类型**。

如图5.8所示是**BOUND类型workqueue机制**的架构图，对于**BOUND类型的workqueue**归纳如下。

- **每个**新建的workqueue，都有一个struct **workqueue\_struct**数据结构来描述。
- 对于**每个**新建的workqueue，**每个CPU**有**一个pool\_workqueue**数据结构来**连接workqueue**和**worker\_pool**.
- **每个CPU只有两个worker\_pool**数据结构来描述**工作池**，一个用于**普通优先级工作线程**，另一个用于**高优先级工作线程**。
- **worker\_pool**中可以有**多个工作线程**，动态管理工作线程。
- **worker\_pool**和**workqueue**是**1:N**的关系，即**一个worker\_pool**可以对应**多个workqueue**.
- pool\_workqueue是worker\_pool和workqueue之间的桥梁枢纽。
- **worker\_pool**和**worker工作线程**也是**1:N**的关系。

![config](./images/12.png)

**BOUND类型的work**是在**哪个CPU**上运行的呢？有几个API接口可以把**一个work**添加到**workqueue**上运行，其中**schedule\_work**()函数**倾向于使用本地CPU**，这样有利于利用**CPU的局部性原理**提高效率，而**queue\_work\_on**()函数可以**指定CPU**的。

对于**UNBOUND类型的workqueue**来说，其**工作线程没有绑定到某个固定的CPU**上。对于**UMA**机器，它可以在**全系统的CPU**内运行；对于**NUMA**机器，**每一个node**节点**创建一个worker\_pool**。

在**驱动开发**中，**UNBOUND类型**的**workqueue不太常用**，举一个典型的例子，Linux内核中有一个**优化启动时间(boot time**)的新接口Asynchronous functioncalls，实现是在kernel/asyn.c文件中。对于一些**不依赖硬件时序**且不需要串行执行的初始化部分，可以采用这个接口，现在电源管理子系统中有一个选项可以把一部分外设在suspend/resume过程中的操作用异步的方式来实现，从而优化其suspend/resume时间，详见kemel/power/main.c 中关于“pm\_async\_enabled” 的实现。

对于**长时间占用CPU资源**的一些负载(标记**WQ\_CPU\_INTENSIVE**), Linux内核倾向于**使用UNBOUND类型的workqueue**, 这样可以利用**系统进程调度器**来优化选择在哪个CPU上运行，例如**drivers/md/raid5.c**驱动。

如下动态管理技术值得读者仔细品味。

- 动态管理**工作线程数量**，包括**动态创建工作线程**和**动态管理活跃工作线程**等。
- 动态**唤醒工作线程**。