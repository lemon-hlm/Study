[TOC]

在阅读本节前请思考如下小问题。

- RCU相比读写锁有哪些优势？
- 请解释Quiescent State和Grace Period。
- 请简述RCU实现的基本原理。
- 在大型系统中，经典RCU遇到了什么问题？ Tree RCU又是如何解决该问题的？
- 在RCU实现中，为什么要使用ULONG\_CMP\_GE()和ULONG\_CMP\_LT()宏来比较两个数的大小，而不直接使用大于号或者小于号来比较？
- 请简述一个Grace Period的生命周期及其状态机的变化。

# 0 概述

RCU全称**read\-copy\-update**，是 Linux内核中一种重要的**同步机制**。

Linux内核中己经有
了**原子操作**、**spinlock**、**读写spinlock**、**读写信号量**、**mutex**等锁机制，为什么要单独设计一个比它们实现要复杂得多的新机制呢？

回忆**spinlock**、**读写信号量**和**mutex**的实现，它们都使用了**原子操作指令**，即**原子地访问内存**，**多CPU**争用**共享的变量**会让**cache—致性变得很糟(！！！**)，使得**性能下降**。

以**读写信号量**为例，除了**上述缺点**外，读写信号量还有一个**致命弱点**，它**只允许多个读者同时存在**，但是**读者和写者不能同时存在(！！！**)。

那么**RCU机制要实现的目标**是，希望**读者线程(！！！)没有同步开销**，或者说**同步开销变得很小**，甚至可以忽略不计，**不需要额外的锁**，不需要使用**原子操作**指令和**内存屏障**，即可**畅通无阻地访问(！！！**)；而把需要**同步的任务**交给**写者线程(！！！**)，**写者线程**等待**所有读者线程**完成后才会把**旧数据销毁**。在RCU中，如果有**多个写者(！！！**)同时存在，那么需要**额外的保护机制(！！！**)。

RCU机制的**原理**可以概括为**RCU记录**了**所有指向共享数据的指针的使用者**，当要**修改**该共享数据时，首先**创建一个副本**，在**副本中修改**。**所有读访问线程**都离开读临界区之后，**指针**指向新的**修改后副本的指针**，并且**删除旧数据**。

RCU的一个重要的应用场景是**链表**，有效地**提高遍历读取数据的效率**。**读取链表**成员数据时通常只需要**rcu\_read\_lock**(), 允许**多个线程(！！！)同时读取该链表**，并且允许**一个线程(！！！**)同时**修改链表**。那为什么这个过程能**保证链表访问的正确性**呢？

在**读者遍历链表**时，假设**另外一个线程删除了一个节点**。**删除线程**会把这个**节点从链表中移出**，但**不会直接销毁**它。RCU会等到**所有读线程读取**完成后，才会**销毁**这个节点。

RCU提供的接口如下。

- rcu\_read\_lock()/rcu\_read\_unlock(): 组成一个**RCU读临界**。
- rcu\_dereference(): 用于获取**被RCU保护的指针**(RCU protected pointer)，**读者线程**要访问**RCU保护的共享数据**，需要使用**该函数创建一个新指针**，并且**指向RCU被保护的指针**。
- rcu\_assign\_pointer(): 通常用在**写者线程**。在**写者线程**完成新数据的**修改**后，调用该接口可以让**被RCU保护的指针**指向**新创建的数据**，用RCU的术语是发布(Publish) 了更新后的数据。
- synchronize\_rcu(): 同步等待**所有现存的读访问完成**。
- call\_rcu(): 注册一个**回调函数**，当**所有**现存的**读访问完成**后，调用这个回调函数**销毁旧数据**。

下面通过一个RCU简单的例子来理解上述接口的含义，该例子来源于内核源代码中Documents/RCU/whatisRCU.txt ， 并且省略了一些异常处理情况。

```c
[RCU的一个简单例子]

```

该例子的目的是通过RCU机制保护my\_test\_init()分配的共享数据结构g\_ptr，另外创建了一个读者线程和一个写者线程来模拟同步场景。

对于**读者线程**myrcu\_reader\_thread:

- 通过**rcu\_read\_lock**()和**rcu\_read\_unlock**()来构建一个**读者临界区**。
- 调用rcu\_dereference()获取**被保护数据g\_ptr指针**的一个**副本**，即**指针p**，这时**p和g\_ptr**都指向**旧的被保护数据**。
- 读者线程每隔200毫秒读取一次被保护数据。

对于写者线程myrcu\_writer\_thread:

- 分配一个新的保护数据new\_ptr，并修改相应数据。
- **rcu\_assign\_pointer**()让g\_ptr指向新数据。
- call\_rcua注册一个回调函数，确保所有对旧数据的引用都执行完成之后，才调用
回调函数来删除旧数据old\_data。
- 写者线程每隔400毫秒修改被保护数据。

上述过程如图4.6所示。

![config](./images/8.png)

在所有的读访问完成之后，内核可以释放旧数据，对于**何时释放旧数据**，内核提供了两个API函数：synchronize\_rcu()和 call\_rcu().

# 1 经典RCU和Tree RCU

本章重点介绍**经典RCU**和**Tree RCU的实现**，**可睡眠**和**可抢占RCU**留给读者自行阅读。

RCU 里有两个很重要的概念，分别是**宽限期(Grace Period, GP**)和**静止状态(Quiescent State, QS**).

- Grace Period, **宽限期**。GP有**生命周期**，有**开始**和**结束**之分。在**GP开始那一刻**算起，当**所有处于读者临界区**的CPU都**离开**了临界区，也就是**都至少发生了一次Quiescent State**, 那么认为**一个GP可以结束**了。GP**结束**后，RCU会调用注册的**回调函数**，例如销毁旧数据等。
- Quiescent State, **静止状态**。在RCU设计中，如果**一个CPU处于RCU读者临界区**中，说明它的状态是**活跃**的；相反，如果在**时钟tick**中检测到该CPU处于**用户模式或idle模式(！！！**)，说明该CPU已经离开了读者临界区，那么它是**静止状态**。在**不支持抢占的RCU实现(！！！**)中，只要检测到CPU有**上下文切换**，就可以知道**离开了读者临界区**。

RCU在Linux 2.5内核开发时己经加入到Linux内核，但是在**Linux 2.6.29之前**的RCU通常被称为**经典RCU(Classic RCU**)。经典RCU在**大型系统**中遇到了**性能问题**，后来在Linux **2.6.29**中IBM的内核专家 Paul E.McKenney 提出了**Tree RCU**的实现，Tree RCU也被称为**Hierarchical RCU**。

经典RCU实现在超级大系统中遇到了问题，特别是**有些系统**的**CPU核心超过了1024个**，甚至达到**4096**个。**经典RCU**在判断**是否完成一次GP**时采用**全局的cpumask位图**，每个比特位表示一个CPU，那么在1024个CPU核心的系统中，cpumask位图就有1024个比特位。**每个CPU**在**GP开始**时要**设置位图中对应的比特位**，**GP结束**时要**清相应的比特位**。**全局的cpumask位图**会导致**很多CPU竞争使用**，那么**需要spinlock锁来保护位图**。这样导致该**锁争用变得很惨烈**，惨烈程度随着**CPU的个数线性递增**。以4核处理器为例，经典RCU的实现如图4.7所示。

![config](./images/9.png)

**Tree RCU**实现巧妙地解决了**cpumask位图竞争锁**的问题。以上述的4核处理器为例，假设Tree RCU把**两个CPU**分成**1个rcu\_node节点**，这样**4个CPU**被分配到**两个rcu\_node**节点上，另外还有**1个根rcu\_node**节点来**管理这两个rcu\_node节点**。如图4.8所示，节点1管理cpuO和cpul，节点2管理cpu2和cpu3，而节点0是根节点，管理节点1和节点2。**每个节点**只需要**两个比特位的位图**就可以**管理各自的CPU**或者节点，**每个节点(！！！**)都有**各自的spinlock锁(！！！**)来**保护相应的位图**。

![config](./images/10.png)

假设**4个CPU**都经历过**一个QS状态**，那么**4个CPU**首先在**Level 0层级**的**节点1**和**节点2**上**修改位图**。对于**节点1**或者**节点2**来说，**只有两个CPU来竞争锁**，这比经典RCU上的锁争用要减少一半。当**Level 0**上**节点1**和**节点2**上**位图都被清除干净后(！！！**)，才会清除**上一级节点的位图**，并且**只有最后清除节点的CPU(！！！**)才有机会去**尝试清除上一级节点的位图**。因此对于节点0来说，还是两个CPU来争用锁。整个过程都是只有两个CPU去争用一个锁，比经典RCU实现要减少一半。这类似于足球比赛，进入四强的4只队伍被分成上下半区，每个半区有两只球队，只有半决赛获胜的球队才能进入决赛。

Tree RCU为了实现**分层的结构**，定义了**3个很重要的数据结构**，分别是**struct rcu\_data**、**struct rcu\_node**和**struct rcu\_state**，另外还维护了一个比较隐晦的状态机。

```c
[kernel/rcu/tree.h]
struct rcu_data {
	unsigned long	completed;
	unsigned long	gpnum;
	bool		passed_quiesce;
	bool		qs_pending;
	struct rcu_node *mynode;
	unsigned long grpmask;
	struct rcu_head *nxtlist;
	struct rcu_head **nxttail[RCU_NEXT_SIZE];
	unsigned long	nxtcompleted[RCU_NEXT_SIZE];
	int cpu;
	struct rcu_state *rsp;
};
```

struct rcu\_data数据结构定义成**Per\-CPU变量**，每个CPU有一个独立的struct rcu\_data，有如下的重要的成员。

