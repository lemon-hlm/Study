[TOC]

在阅读本节前请思考如下小问题。

- RCU相比读写锁有哪些优势？
- 请解释Quiescent State和Grace Period。
- 请简述RCU实现的基本原理。
- 在大型系统中，经典RCU遇到了什么问题？ Tree RCU又是如何解决该问题的？
- 在RCU实现中，为什么要使用ULONG\_CMP\_GE()和ULONG\_CMP\_LT()宏来比较两个数的大小，而不直接使用大于号或者小于号来比较？
- 请简述一个Grace Period的生命周期及其状态机的变化。

# 0 概述

RCU全称**read\-copy\-update**，是 Linux内核中一种重要的**同步机制**。

Linux内核中己经有
了**原子操作**、**spinlock**、**读写spinlock**、**读写信号量**、**mutex**等锁机制，为什么要单独设计一个比它们实现要复杂得多的新机制呢？

回忆**spinlock**、**读写信号量**和**mutex**的实现，它们都使用了**原子操作指令**，即**原子地访问内存**，**多CPU**争用**共享的变量**会让**cache—致性变得很糟(！！！**)，使得**性能下降**。

以**读写信号量**为例，除了**上述缺点**外，读写信号量还有一个**致命弱点**，它**只允许多个读者同时存在**，但是**读者和写者不能同时存在(！！！**)。

那么**RCU机制要实现的目标**是，希望**读者线程(！！！)没有同步开销**，或者说**同步开销变得很小**，甚至可以忽略不计，**不需要额外的锁**，不需要使用**原子操作**指令和**内存屏障**，即可**畅通无阻地访问(！！！**)；而把需要**同步的任务**交给**写者线程(！！！**)，**写者线程**等待**所有读者线程**完成后才会把**旧数据销毁**。在RCU中，如果有**多个写者(！！！**)同时存在，那么需要**额外的保护机制(！！！**)。

RCU机制的**原理**可以概括为**RCU记录**了**所有指向共享数据的指针的使用者**，当要**修改**该共享数据时，首先**创建一个副本**，在**副本中修改**。**所有读访问线程**都离开读临界区之后，**指针**指向新的**修改后副本的指针**，并且**删除旧数据**。

RCU的一个重要的应用场景是**链表**，有效地**提高遍历读取数据的效率**。**读取链表**成员数据时通常只需要**rcu\_read\_lock**(), 允许**多个线程(！！！)同时读取该链表**，并且允许**一个线程(！！！**)同时**修改链表**。那为什么这个过程能**保证链表访问的正确性**呢？

在**读者遍历链表**时，假设**另外一个线程删除了一个节点**。**删除线程**会把这个**节点从链表中移出**，但**不会直接销毁**它。RCU会等到**所有读线程读取**完成后，才会**销毁**这个节点。

RCU提供的接口如下。

- rcu\_read\_lock()/rcu\_read\_unlock(): 组成一个**RCU读临界**。
- rcu\_dereference(): 用于获取**被RCU保护的指针**(RCU protected pointer)，**读者线程**要访问**RCU保护的共享数据**，需要使用**该函数创建一个新指针(！！！**)，并且**指向RCU被保护的指针**。
- rcu\_assign\_pointer(): 通常用在**写者线程**。在**写者线程**完成新数据的**修改**后，调用该接口可以让**被RCU保护的指针**指向**新创建的数据**，用RCU的术语是发布(Publish) 了更新后的数据。
- synchronize\_rcu(): 同步等待**所有现存的读访问完成**。
- call\_rcu(): 注册一个**回调函数(！！！**)，当**所有**现存的**读访问完成**后，调用这个回调函数**销毁旧数据**。

下面通过一个RCU简单的例子来理解上述接口的含义，该例子来源于内核源代码中Documents/RCU/whatisRCU.txt ，并且省略了一些异常处理情况。

```c
[RCU的一个简单例子]
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/init.h>
#include <linux/slab.h>
#include <linux/spinlock.h>
#include <linux/rcupdate.h>
#include <linux/kthread.h>
#include <linux/delay.h>

struct foo {
	int a;
	struct rcu_head rcu;
};
        
static struct foo *g_ptr;

static void myrcu_reader_thread(void *data) //读者线程
{
    struct foo *p = NULL;
    
    while(1){
        msleep(200);
        // 重点1
        rcu_read_lock();
        // 重点2
        p = rcu_dereference(g_ptr);
        if(p)
            printk("%s: read a=%d\n", __func__, p->a);
        // 重点3
        rcu_read_unlock();
    }
}

static void myrcu_del(struct rcu_head *rh)
{
    struct foo *p = container_of(rh, struct foo, rcu);
    printk ("%s: a=%d\n", __func__, p->a);
    kfree(p);
}

static void myrcu_writer_thread(void *p)    //写者线程
{
    struct foo *mew;
    struct foo *old;
    int value = (unsigned long)p;
    
    while(1){
        msleep(400);
        struct foo *new_ptr = kmalloc(sizeof(struct foo), GFP_KERNEL);
        old = g_ptr;
        printk("%s: write to new %d\n", __func__, value);
        *new_ptr = *old;
        new_ptr->a = value;
        // 重点
        rcu_assign_pointer(g_ptr, new_ptr);
        // 重点
        call_rcu(&old->rcu, myrcu_del);
        value++;
    }
}

static int __init my_test_init(void){
    struct task_struct *reader_thread;
    struct task_struct *writer_thread ；
    int value = 5;
    
    printk("figo: my module init\n");
    g_ptr = kzalloc(sizeof (struct foo), GFP_KERNEL);
    
    reader_thread = kthread_run(myrcu_reader_thread, NULL, "rcu_reader");
    writer_thread = kthread_run(myrcu_writer_thread, (void *)(unsigned long)value, "rcu_writer")
    return 0;
}

static void __exit my_test_exit(void)
{
    printk("goodbye\n");
    if(g_ptr)
        kfree(g_ptr);
}
MODULE_LICENSE("GPL");
module_init(my_test_init);
```

该例子的目的是通过RCU机制保护**my\_test\_init**()**分配的共享数据结构g\_ptr**，另外创建了一个读者线程和一个写者线程来模拟同步场景。

对于**读者线程**myrcu\_reader\_thread:

- 通过**rcu\_read\_lock**()和**rcu\_read\_unlock**()来构建一个**读者临界区**。
- 调用rcu\_dereference()获取**被保护数据g\_ptr指针**的一个**副本(！！！**)，即**指针p**，这时**p和g\_ptr**都指向**旧的被保护数据**。
- 读者线程每隔200毫秒读取一次被保护数据。

对于**写者线程**myrcu\_writer\_thread:

- 分配一个**新的保护数据new\_ptr**，并修改相应数据。
- **rcu\_assign\_pointer**()让g\_ptr指向**新数据**。
- call\_rcu()注册一个**回调函数(！！！**)，确保**所有对旧数据的引用都执行完成**之后，才调用回调函数来删除旧数据old\_data。
- 写者线程每隔400毫秒修改被保护数据。

上述过程如图4.6所示。

![config](./images/8.png)

在所有的**读访问完成**之后，内核可以释放旧数据，对于**何时释放旧数据**，内核提供了**两个API函数**：synchronize\_rcu()和 call\_rcu().

# 1 经典RCU和Tree RCU

本章重点介绍**经典RCU**和**Tree RCU的实现**，**可睡眠**和**可抢占RCU**留给读者自行阅读。

RCU 里有两个很重要的概念，分别是**宽限期(Grace Period, GP**)和**静止状态(Quiescent State, QS**).

- Grace Period, **宽限期**。GP有**生命周期**，有**开始**和**结束**之分。在**GP开始那一刻**算起，当**所有处于读者临界区**的CPU都**离开**了临界区，也就是**都至少发生了一次Quiescent State**, 那么认为**一个GP可以结束**了。GP**结束**后，RCU会调用注册的**回调函数**，例如销毁旧数据等。
- Quiescent State, **静止状态**。在RCU设计中，如果**一个CPU处于RCU读者临界区**中，说明它的状态是**活跃**的；相反，如果在**时钟tick**中检测到该CPU处于**用户模式或idle模式(！！！**)，说明该CPU已经**离开了读者临界区**，那么它是**静止状态**。在**不支持抢占的RCU实现(！！！**)中，只要检测到CPU有**上下文切换**，就可以知道**离开了读者临界区**。

RCU在Linux 2.5内核开发时己经加入到Linux内核，但是在**Linux 2.6.29之前**的RCU通常被称为**经典RCU(Classic RCU**)。经典RCU在**大型系统**中遇到了**性能问题**，后来在Linux **2.6.29**中IBM的内核专家 Paul E.McKenney 提出了**Tree RCU**的实现，Tree RCU也被称为**Hierarchical RCU**。

经典RCU实现在超级大系统中遇到了问题，特别是**有些系统**的**CPU核心超过了1024个**，甚至达到**4096**个。**经典RCU**在判断**是否完成一次GP**时采用**全局的cpumask位图(！！！**)，每个比特位表示一个CPU，那么在1024个CPU核心的系统中，cpumask位图就有1024个比特位。**每个CPU**在**GP开始**时要**设置位图中对应的比特位**，**GP结束**时要**清相应的比特位**。**全局的cpumask位图**会导致**很多CPU竞争使用**，那么**需要spinlock锁来保护位图(！！！**)。这样导致该**锁争用变得很惨烈**，惨烈程度随着**CPU的个数线性递增(！！！**)。以4核处理器为例，经典RCU的实现如图4.7所示。

![config](./images/9.png)

**Tree RCU**实现巧妙地解决了**cpumask位图竞争锁**的问题。以上述的4核处理器为例，假设Tree RCU把**两个CPU**分成**1个rcu\_node节点**，这样**4个CPU**被分配到**两个rcu\_node**节点上，另外还有**1个根rcu\_node**节点来**管理这两个rcu\_node节点**。如图4.8所示，节点1管理cpuO和cpul，节点2管理cpu2和cpu3，而节点0是根节点，管理节点1和节点2。**每个节点**只需要**两个比特位的位图(！！！**)就可以**管理各自的CPU**或者节点，**每个节点(！！！**)都有**各自的spinlock锁(！！！**)来**保护相应的位图**。

![config](./images/10.png)

注意: **CPU不算层次level！！！**

假设**4个CPU**都经历过**一个QS状态**，那么**4个CPU**首先在**Level 0层级**的**节点1**和**节点2**上**修改位图**。对于**节点1**或者**节点2**来说，**只有两个CPU来竞争锁**，这比经典RCU上的锁争用要减少一半。当**Level 0**上**节点1**和**节点2**上**位图都被清除干净后(！！！**)，才会清除**上一级节点的位图**，并且**只有最后清除节点的CPU(！！！**)才有机会去**尝试清除上一级节点的位图(！！！**)。因此对于节点0来说，还是两个CPU来争用锁。整个过程都是只有两个CPU去争用一个锁，比经典RCU实现要减少一半。这类似于足球比赛，进入四强的4只队伍被分成上下半区，每个半区有两只球队，只有半决赛获胜的球队才能进入决赛。

Tree RCU为了实现**分层的结构**，定义了**3个很重要的数据结构**，分别是**struct rcu\_data**、**struct rcu\_node**和**struct rcu\_state**，另外还维护了一个比较隐晦的**状态机(！！！**)。

```c
[kernel/rcu/tree.h]
struct rcu_data {
	unsigned long	completed;
	unsigned long	gpnum;
	bool		passed_quiesce;
	bool		qs_pending;
	struct rcu_node *mynode;
	unsigned long grpmask;
	struct rcu_head *nxtlist;
	struct rcu_head **nxttail[RCU_NEXT_SIZE];
	unsigned long	nxtcompleted[RCU_NEXT_SIZE];
	int cpu;
	struct rcu_state *rsp;
	...
};
```

struct **rcu\_data**数据结构定义成**Per\-CPU变量**，每个CPU有一个独立的struct rcu\_data，有如下的重要的成员。

注意: **CPU不算层次level！！！**

- gpnum: RCU内部对GP的一个**计数**。系统初始化时该值从\-300开始计数，每当**新建一个GP**，该值会**加1**。
- completed: 当**GP完成**时，该成员会**加1**。系统初始化时，completed和gpnum成员都等于\-300, 从这**两个成员值的变化**可以窥探出**GP状态机的运行状态**。
- passed\_quiesce: 当在**时钟tick处理函数**中检测到**rcu\_data对应的CPU**完成一次**Quiescent State**时，该成员**设置为true**.
- qs\_pending: 表示CPU正在**等待Quiescent State**。
- mynode：指向**父节点rcu\_node**。
- grpmask: **父节点rcu\_node**中有一个**qsmark位图(！！！**)。该**位图中每个比特位**代表一个**子节点**或**对应的rcu\_data**。grpmask代表**在qsmark位图**中的**相应的比特位**。
- nxtlist和nxttail: 组成一个**多层次的链表(！！！**)。
- cpu: 指**该rcu\_data所属的CPU ID**。
- rsp: 指向**rcu\_state**数据结构。

```c
[kernel/rcu/tree.h]
struct rcu_node {
	raw_spinlock_t lock;
	unsigned long gpnum;
	unsigned long completed;
	unsigned long qsmask;
	unsigned long qsmaskinit;
	unsigned long grpmask;
	int	grplo;
	int	grphi;
	u8	level;
	struct rcu_node *parent;
	...
} ____cacheline_internodealigned_in_smp;
```

struct rcu\_node是**Tree RCU**中重要的**组成节点**，它有**根节点(Root Node**)和**叶节点**之分。如果**Tree RCU只有一层**，那么**根节点**下面**直接管理着一个或多个rcu\_data**;如果Tree RCU有**多层**结构，那么**根节点管理着多个叶节点**，**最底层的叶节点**管理者**一个或多个rcu_data**。

- lock: rcu\_node节点**内部的spinlock锁**，用于**该节点所管辖**的**rcu\_data或叶节点之间的互斥操作(！！！**)。
- gpnum: 表示**当前GP**在**该节点的计数**。系统初始化为\-300，每当开始一个GP, 该值会增加1。
- completed: 表示**该节点上一次GP完成**时的计数。系统**初始化**时，和gpnum—样为-300。当**一个GP完成**时，**completed才会加1**。
- qsmark: 该**节点**用于管理**所属的rcu\_data或子节点的位图(！！！**)。**每个比特位**表示**一个rcu\_data**或**子节点**。每当**rcu\_data或子节点**完成了**Quiescent State状态**，相应的**比特位会被清除**。
- qsmaskinit: 每个GP初始化时，qsmaskinit等于qsmark的初始值。
- grpmask: 对应**其父节点中的qsmark位图相应比特位**。
- grplo: 该节点**最少管理CPU**或**子节点**的数量。
- grphi: 该节点最多管理CPU或子节点的数量。
- level: 表示该节点在Tree RCU中的**第几层**，**根节点在第0层**。
- parent: 指向**父节点**。

```c
[kernel/rcu/tree.h]
struct rcu_state {
	struct rcu_node node[NUM_RCU_NODES];
	struct rcu_node *level[RCU_NUM_LVLS];
	u32 levelcnt[MAX_RCU_LVLS + 1];
	u8 levelspread[RCU_NUM_LVLS];
	struct rcu_data __percpu *rda;
	void (*call)(struct rcu_head *head,
		     void (*func)(struct rcu_head *head));
	unsigned long gpnum;
	unsigned long completed;
	struct task_struct *gp_kthread;
	wait_queue_head_t gp_wq;
	short gp_state;
	const char *name;
	struct list_head flavors;
	...
};
```

RCU系统支持**多个不同类型的RCU状态**，例如rcu\_sched\_state、rcu\_bh\_state和rcu\_preempt\_state，它们分别使用struct rcu\_state数据结构来描述这些状态。**每种RCU类型**都有**独立的层次结构(！！！**)，即**根节点和rcu\_data**数据结构。

- node: **所有的rcu\_node**节点都存放到**此数组**中，方便进行**全部的节点扫描**，例如rcu\_for\_each\_node\_breadth\_first()宏。
- level: **指针数组**，每个成员指向Tree RCU**每一层**中的**第一个rcu\_node节点**。
- levelcnt: **数组**, **每一层**包含**rcu\_node节点的个数**。
- levelspread: **数组**, **每一层**管理可以管理的**CPU或子节点的个数**。
- rda: 指向**rcu\_data的Per\-CPU 变量**。
- **call**: 指向RCU的call\_rcu\_sched()、call\_rcu\_bh()和call\_rcu()函数。
- gpnum、completed: 与rcu\_node和rcu\_data数据结构中的成员含义类似。
- gp\_kthread: **RCU内核线程**，处理函数为**rcu\_gp\_kthread**()。
- gp\_wq: 在RCU**内核线程**中管理**睡眠唤醒的等待队列**。
- gp\_state: 管理**RCU内核线程睡眠唤醒的状态**。
- name: 该rcu\_state的名字。
- flavors: 几个独立的rcu\_state串成一个链表。

# 2 Tree RCU设计

## 2.1 初始化RCU层次结构

### 2.1.1 RCU层次结构概述

Tree RCU根据**CPU数量的大小**按照**树形结构**来组成其层次结构，称为**RCU Hierarchy**。内核中有**两个宏**帮助构建RCU层次结构，其中**CONFIG\_RCU\_FANOUT\_LEAF**表示一个**子叶子的CPU数量(！！！**)，**CONFIG\_RCU\_FANOUT**表示**每个层数**最多支持的**叶子数量**，**MAX\_RCU_LVLS**等于4表示**内核最多支持4层结构(！！！**)。

**三个维度(！！！**): **每个叶子的CPU数量**, **每层的最多叶子数量**, **最多层数**; **CPU不算level层次！！！**

```c
[arch/arm/configs/vexpress_defconfig]
CONFIG_RCU_FANOUT=32
CONFIG_RCU_FANOUT_LEAF=16

[kernel/rcu/tree.h]
#define MAX_RCU_LVLS 4
#define RCU_FANOUT_1	      (CONFIG_RCU_FANOUT_LEAF)
#define RCU_FANOUT_2	      (RCU_FANOUT_1 * CONFIG_RCU_FANOUT)
#define RCU_FANOUT_3	      (RCU_FANOUT_2 * CONFIG_RCU_FANOUT)
#define RCU_FANOUT_4	      (RCU_FANOUT_3 * CONFIG_RCU_FANOUT)

#if NR_CPUS <= RCU_FANOUT_1
#  define RCU_NUM_LVLS	      1
#  define NUM_RCU_LVL_0	      1
#  define NUM_RCU_LVL_1	      (NR_CPUS)
#  define NUM_RCU_LVL_2	      0
#  define NUM_RCU_LVL_3	      0
#  define NUM_RCU_LVL_4	      0
#elif NR_CPUS <= RCU_FANOUT_2
#  define RCU_NUM_LVLS	      2
#  define NUM_RCU_LVL_0	      1
#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
#  define NUM_RCU_LVL_2	      (NR_CPUS)
#  define NUM_RCU_LVL_3	      0
#  define NUM_RCU_LVL_4	      0
#elif NR_CPUS <= RCU_FANOUT_3
#  define RCU_NUM_LVLS	      3
#  define NUM_RCU_LVL_0	      1
#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
#  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
#  define NUM_RCU_LVL_3	      (NR_CPUS)
#  define NUM_RCU_LVL_4	      0
#elif NR_CPUS <= RCU_FANOUT_4
#  define RCU_NUM_LVLS	      4
#  define NUM_RCU_LVL_0	      1
#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_3)
#  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
#  define NUM_RCU_LVL_3	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
#  define NUM_RCU_LVL_4	      (NR_CPUS)
#else
# error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
#endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */
```

假设**CONFIG\_RCU\_FANOUT\_LEAF等于16**, **CONFIG\_RCU\_FANOUT等于32**, 那么可计算出**该系统RCU最大支持**的**CPU个数为524288**, 这已经远远大于一般超级系统的CPU个数。

以ARM Vexpress平台为例，最多支持4个Cortex A9, 那么它的RCU层次结构如图4.9所示，系统**只有一个层级**即Level 0, 并且**Level 0**层级中**只需要一个struct rcu\_node(！！！**)节点就可以容纳**4个struct rcu\_data(！！！**)数据结构。struct **rcu\_data**数据结构是**Per\-CPU变量**，每个CPU有一个独立的struct rcu\_data数据结构，其中**mynode(！！！**)成员指向所属的**struct rcu\_node**节点。

**系统初始化3个独立的struct rcu\_state(！！！**)用于不同的场景，分别为**rcu\_sched\_state**、**rcu\_bh\_state**和**rcu\_preempt\_state**。**每个struct rcu\_state(！！！**)都有**一套上述的RCU层次结构(！！！**)。

- **普通进程上下文**的RCU使用**rcu\_sched\_state**状态；
- **软中断上下文**则使用**rcu\_bh\_state**; 
- 如果系统配置了**CONFIG\_PREEMPT\_RCU**, 那么系统**默认使用rcu\_preempt\_state**, 它在**read\_lock**期间**允许其他进程抢占**。

![config](./images/12.png)

下面以两个层级的RCU结构为例，假设在一个32核的处理器中，CONFIG\_RCU\_FANOUT\_LEAF等于16，CONFIG\_RCU\_FANOUT等32，该处理器的RCU层次结构如图4.10所示。

![config](./images/13.png)

在**32核处理器**中，层次结构分成两层，**Level 0**包括**两个struct rcu\_node(！！！**)，其中**每个struct rcu\_node**管理**16个struct rcu\_data(！！！**)数据结构，分别表示**16个CPU的独立struct rcu\_data**数据结构; 在**Level 1**层级，有**一个struct rcu\_node(！！！**)节点**管理**着**Level 0层级**的**两个rcu\_node**节点，**Level 1**层级中的**rcu\_node**节点称为**根节点**，**Level 0**层级的**两个rcu\_node**节点是**叶节点**。

下面以4核处理器为例，详细介绍系统第一个GP的生命周期。

### 2.1.2 rcu\_state初始化

**struct rcu\_state**数据结构釆用**静态初始化**的方式，由**RCU\_STATE\_INITIALIZER**()来初始化一些重要的成员。

```c
[kernel/rcu/tree.c]
#define RCU_STATE_INITIALIZER(sname, sabbr, cr) \
DEFINE_RCU_TPS(sname) \
struct rcu_state sname##_state = { \
	.level = { &sname##_state.node[0] }, \
	.call = cr, \
	.fqs_state = RCU_GP_IDLE, \
	.gpnum = 0UL - 300UL, \
	.completed = 0UL - 300UL, \
	.orphan_lock = __RAW_SPIN_LOCK_UNLOCKED(&sname##_state.orphan_lock), \
	.orphan_nxttail = &sname##_state.orphan_nxtlist, \
	.orphan_donetail = &sname##_state.orphan_donelist, \
	.barrier_mutex = __MUTEX_INITIALIZER(sname##_state.barrier_mutex), \
	.onoff_mutex = __MUTEX_INITIALIZER(sname##_state.onoff_mutex), \
	.name = RCU_STATE_NAME(sname), \
	.abbr = sabbr, \
}; \
DEFINE_PER_CPU_SHARED_ALIGNED(struct rcu_data, sname##_data)
```

其中**gpnum和completed初始化为(0UL \- 300UL**)。读者可以会有疑问，这两个成员定义为**unsigned long类型**，为什么这里初始化为0UL \- 300UL呢？unsigned long类型为什么定义负数？

以**32位CPU**为例，**unsigned long类型**的**最大值是ULONG\_MAX(\~0UL**), 即**0xffff,ffff**。如果用**有符号类型**来表示就**是\-1** , 所以(0UL \- 300UL)用**无符号类型**来表示是4294966996,用十六进制来表示是0xfffffed4, **用有符号类型来表示是\-300**。gpnum和completed成员在**RCU系统**中会**一直在增长**，也就是**初始化**的0xfffffed4(**有符号类型等于\-300**)一直**增长到**(**有符号类型等于\-1**),然后变成0x0, 然后一直增长到然后又从0x0开始增长，一直循环下去。有符号类型变量有溢出问题，所以这里都使用无符号类型变量。为了描述方便和读者容易理解，抛开溢出问题，本章假设gpnum和 completed是**有符号类型**变量，**初始值从\-300**开始，虽然这样表述不准确。

### 2.1.3 内核启动RCU初始化

**RCU的初始化**在**内核启动**时会调用**rcu\_init**()函数，**RCU层次结构**的构建在**rcu\_init\_geometry**()和 **rcu\_init\_one**()函数中实现。

```c
[start_kernel()->rcu_init()] 
[kernel/rcu/tree.c]
void __init rcu_init(void)
{
	int cpu;

	rcu_init_geometry();
	// 初始化rcu_bh_state
	rcu_init_one(&rcu_bh_state, &rcu_bh_data);
	// 初始化rcu_sched_state
	rcu_init_one(&rcu_sched_state, &rcu_sched_data);
	// 初始化rcu_preempt_state
	__rcu_init_preempt();
	// 注册softirq回调
	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
    // 注册CPU Notifier子系统
	cpu_notifier(rcu_cpu_notify, 0);
	// 注册PM Notifier子系统
	pm_notifier(rcu_pm_notify, 0);
	// 位置1
	for_each_online_cpu(cpu)
		rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
}
```

这里会初始化**3个rcu\_state**，分别是**rcu\_sched\_state**、**rcu\_bh\_state**和**rcu\_preempt\_state**。

在**rcu\_init\_one**()函数里，除了构建**rcu\_state**、**rcu\_node**和**rcu\_data**之间的**树形结构**关系外，还会初始化一些**关键的数据结构成员**。

```c
# 4核处理器，假设叶节点CPU个数是16
rnp->gpnum = rsp->gpnum = -300
rnp->completed = rsp->completed = -300
rnp->grplo =0
rnp->grphi=3
rnp->level =0
rnp->qsmask =0
rnp->qsmaskinit=l
rnp->grpmask =0
```

另外还单独注册了一个**SoftIRQ回调函数！！！rcu\_process\_callbacks**()。

此外，还注册了**CPU Notifier**和**PM Notifier子系统**。

位置1代码，给系统中**每个online的CPU**都发送一个**CPU\_UP\_PREPARE事件**到**CPU Notifier子系统**中，在**回调函数rcu\_cpu\_notify**()中处理该事件。

```c
[kernel/rcu/tree.c]
static int rcu_cpu_notify(struct notifier_block *self,
				    unsigned long action, void *hcpu)
{
	long cpu = (long)hcpu;
	struct rcu_data *rdp = per_cpu_ptr(rcu_state_p->rda, cpu);
	struct rcu_node *rnp = rdp->mynode;
	struct rcu_state *rsp;

	switch (action) {
	case CPU_UP_PREPARE:
	case CPU_UP_PREPARE_FROZEN:
	    // 位置1
		rcu_prepare_cpu(cpu);
		...
		break;
	case CPU_ONLINE:
	case CPU_DOWN_FAILED:
		break;
	...
	default:
		break;
	}
	return NOTIFY_OK;
}
```

**rcu\_cpu\_notify**()函数主要为了**支持CPU热插拔**。对于**CPU\_UP\_PREPARE事件**的具体响应在**rcu\_prepare\_cpu**()函数中。

```c
[kernel/rcu/tree.c]
static void rcu_prepare_cpu(int cpu)
{
	struct rcu_state *rsp;

	for_each_rcu_flavor(rsp)
		rcu_init_percpu_data(cpu, rsp);
}
```

**for\_each\_rcu\_flavor**()遍历系统中**所有的struct rcu\_state**数据结构。

**rcu\_init\_percpu\_data**()函数**初始化每个CPU(！！！)上的struct rcu\_data**数据结构。

**rcu\_state(不是Per\-CPU, 每个都自有层次结构, 即有自己的节点和rcu\_data), 这里相当于遍历所有rcu\_state, 初始化其中的Per\-CPU rcu\_data**.

#### 2.1.3.1 初始化每个CPU的rcu\_data

删除了关中断和锁相关代码的函数代码片段如下。

```c
[rcu_cpu_notify() -> rcu_prepare_cpu() ->rcu_init_percpu_data()]
[kernel/rcu/tree.c]
static void
rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
{
	unsigned long mask;
	struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
	struct rcu_node *rnp = rcu_get_root(rsp);
    ...
    // 位置1
	init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
	rnp = rdp->mynode;
	mask = rdp->grpmask;
	do {
		rnp->qsmaskinit |= mask;
		mask = rnp->grpmask;
		if (rnp == rdp->mynode) {
			rdp->gpnum = rnp->completed;
			rdp->completed = rnp->completed;
			rdp->passed_quiesce = 0;
		}
		rnp = rnp->parent;
	} while (rnp != NULL && !(rnp->qsmaskinit & mask));
	...
}
```

首先，位置1代码中的**init\_callback\_list**()函数**初始化struct rcu\_data**数据结构中**nxttail\[\]成员**，它是一个**二级指针数组**，在初始化时把nxtlist指针本身的地址赋值给nxttail\[\]成员，如图4.11所示。

```c
[kernel/rcu/tree.c]
static void init_callback_list(struct rcu_data *rdp)
{
	int i;

	rdp->nxtlist = NULL;
	for (i = 0; i < RCU_NEXT_SIZE; i++)
		rdp->nxttail[i] = &rdp->nxtlist;
}
```

![config](./images/14.png)

接下来初始化rcu\_data几个重要的成员，其中rcu\_data\-\>gpnum = rdp\-\>completed = mp\-\>completed = \-300,且rdp\-\>passed\_quiesce= 0.

整个RCU数据结构初始化的效果如图4.12所示。

![config](./images/15.png)

#### 2.1.3.2 每个rcu\_state初始化一个内核线程

总结: 设置当前rcu\_state的GP状态是"reqwait", **睡眠等待**, 直到**rsp\-\>gp\_flags**设置为**RCU\_GP\_FLAG\_INIT**, 即收到**初始化一个GP的请求**, 被唤醒后, 就会调用rcu\_gp\_init()去初始化一个GP, 详见下面

另外在**系统初始化**时为**每个rcu\_state**分别**初始化了一个内核线程(！！！**)，内核线程的**名字以rcu\_state的名字**命名。内核线程的执行函数是rcu\_gp\_kthread().

```c
[kernel/rcu/tree.c]
static int __noreturn rcu_gp_kthread(void *arg)
{
	int fqs_state;
	int gf;
	unsigned long j;
	int ret;
	struct rcu_state *rsp = arg;
	struct rcu_node *rnp = rcu_get_root(rsp);

	for (;;) {

		/* Handle grace-period start. */
		for (;;) {
			trace_rcu_grace_period(rsp->name,
					       ACCESS_ONCE(rsp->gpnum),
					       TPS("reqwait"));
			rsp->gp_state = RCU_GP_WAIT_GPS;
			// 位置1
			wait_event_interruptible(rsp->gp_wq,
						 ACCESS_ONCE(rsp->gp_flags) &
						 RCU_GP_FLAG_INIT);
			/* Locking provides needed memory barrier. */
			// 位置2
			if (rcu_gp_init(rsp))
				break;
			cond_resched_rcu_qs();
			ACCESS_ONCE(rsp->gp_activity) = jiffies;
			WARN_ON(signal_pending(current));
			trace_rcu_grace_period(rsp->name,
					       ACCESS_ONCE(rsp->gpnum),
					       TPS("reqwaitsig"));
		}
        ...
        /* Handle grace-period end. */
		rcu_gp_cleanup(rsp);
	}	
    ...
}
```

该内核线程**创建并运行**之后，会在**wait\_event\_interruptible**()函数中**睡眠等待**，唤醒的条件是**rsp\-\>gp\_flags**要设置**RCU\_GP\_FLAG\_INIT**标志位，这是**初始化一个GP的请求(！！！**)，稍后会介绍。

## 2.2 开启一个GP

### 2.2.1 写者程序注册RCU回调函数

总结: 

(1) 参数: rcu\_head(每个RCU保护的数据都会内嵌一个), 回调函数指针(GP结束<读者执行完>, 被调用销毁)

(2) 将**rcu\_head**加入到**本地rcu\_data的nxttail链表**

**RCU写者程序(！！！**)通常需要调用**call\_rcu**()、**call\_rcu\_bh**()或**call\_rcu\_sched**()等函数来**通知RCU**系统**注册一个RCU回调函数(！！！**)。这三个接口分别对应三种state.

```c
[kernel/rcu/tree_plugin.h]
void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
{
	__call_rcu(head, func, &rcu_preempt_state, -1, 0);
}
EXPORT_SYMBOL_GPL(call_rcu);
```

核心函数在\_\_call\_rcu()中，代码片段如下：

```c
[call_rcu() -> __call_rcu()]
[kernel/rcu/tree.c]
static void
__call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu),
	   struct rcu_state *rsp, int cpu, bool lazy)
{
	unsigned long flags;
	struct rcu_data *rdp;

	head->func = func;
	head->next = NULL;

	local_irq_save(flags);
	rdp = this_cpu_ptr(rsp->rda);

	ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
	
	smp_mb();  /* Count before adding callback for rcu_barrier(). */
	*rdp->nxttail[RCU_NEXT_TAIL] = head;
	rdp->nxttail[RCU_NEXT_TAIL] = &head->next;

	/* Go handle any RCU core processing required. */
	__call_rcu_core(rsp, rdp, head, flags);
	local_irq_restore(flags);
}
```

\_\_call\_rcu()函数的第一个参数head指**rcu\_head**数据结构，通常**被RCU保护的数据结构**都**内嵌一个struct rcu\_head(！！！**)结构；第二个参数是**回调函数指针**，等**之前的RCU读者都执行完成**后，即**宽限期结束**之后**调用该回调函数来做销毁动作**；第三个参数是指在**哪个rcu\_state**上执行。

这里核心操作是把**head**加入到**本地rcu\_data(！！！)的nxttail链表(！！！**)中，其中，**nxtlist**指向**第一个加入链表的回调函数head指针(！！！**)，**nxttail[RCU\_NEXT\_TAIL**]指针指向**head\-\>next指针本身的地址**，因此下一个回调函数再加入时，\***nxttail[RCU\_NEXT\_TAIL**]指针指向**head\->next**指向的成员，如图4.13所示。

![config](./images/16.png)

### 2.2.2 时钟中断处理当前CPU的rcu\_data上事件

总结: 每次**时钟中断**处理函数**tick\_periodic**(), 检查**本地CPU**上**所有的rcu\_state(！！！**)对应的**rcu\_data**成员**nxttail链表有没有写者注册的回调函数**, 有的话**触发一个软中断raise\_softirq**().

在系统**每次时钟中断(！！！)处理函数tick\_periodic**()中，会调用**rcu\_check\_callbacks**()函数去检查**当前CPU(！！！**)上的rcu\_data是否有待处理的事情。

```c
[tick_handle_periodic () ->tick_periodic() ->update_process_times() ->rcu_check_callbacks()]
[kernel/rcu/tree.c]
void rcu_check_callbacks(int user)
{
	trace_rcu_utilization(TPS("Start scheduler-tick"));
	...
	if (rcu_pending())
		invoke_rcu_core();
	...
	trace_rcu_utilization(TPS("End scheduler-tick"));
}
```

**rcu\_check\_callbacks**()函数会做很多检查，现在**暂时只关注rcu\_pending**()函数做哪些检查。rcu\_pending()函数会检查**本地CPU**上**所有的rcu\_state**对应的**rcu\_data上有没有事情需要处理**，并内部调用\_\_rcu\_pending()来实现。\_\_rcu\_pending()函数里也做很多检查，我们暂时只关注和创建新GP相关的。

```c
[rcu_pending() -> __rcu_pending()]
[kernel/rcu/tree.c]
static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
{
	struct rcu_node *rnp = rdp->mynode;

	rdp->n_rcu_pending++;
	...
	/* Has RCU gone idle with this CPU needing another grace period? */
	if (cpu_needs_another_gp(rsp, rdp)) {
		rdp->n_rp_cpu_needs_gp++;
		return 1;
	}
    ...
	return 0;
}
```

cpu\_needs\_another\_gp()函数会检查当前**是否需要开启一个新的GP**.

```c
[rcu_pending() -> __rcu_pending() -> cpu_needs_another_gp()]
[kernel/rcu/tree.c]
static int
cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
{
	int i;

	...
	if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
		return 1;  /* Yes, this CPU has newly registered callbacks. */
	...
	return 0; /* No grace period needed. */
}
```

**cpu\_needs\_another\_gp**()函数同样会做很多检查，目前只关注nxttail[RCU\_NEXT\_READY\_TAIL]这一项。在链表初始化时，nxttail[RCU\_NEXT\_READY\_TAIL]指向nxtlist指针本身的地址，所以这里\*rdp\->nxttail[RCU\_NEXT\_READY\_TAIL\]相当于nxtlist链表头指向的内容，表示nxttail[RCU\_NEXT\_TAIL]链表有**新的RCU回调函数注册**，返回true。

回到**rcu\_check\_callbacks**()函数中，rcu\_pending()返回true，说明有事情需要处理，调用**invoke\_rcu\_core**()去**触发一个RCU软中断(！！！**), raise\_softirq()。之前**内核初始化已经注册了软中断**.

### 2.2.3 软中断处理函数

总结, 针对**每一个rcu\_state(！！！**):

(1) 检查rcu\_data成员**nxttail链表有没有写者注册的回调函数**, 有的话, 开启一个GP, 继续

(2) 调整链表; 设置rsp\-\>gp\_flags标志位为RCU\_GP\_FLAG\_INIT

(3) **rcu\_gp\_kthread\_wake()唤醒rcu\_state对应的内核线程(！！！**), 现在的状态变成了“**newreq**”，表示**有一个新的GP请求**

**RCU软中断**的处理函数是**rcu\_process\_callbacks()函数**，内部调用\_\_rcu\_process\_callbacks()函数去**处理每个rcu\_state的状况(！！！**)。

```c
[kernel/rcu/tree.c]
static void
__rcu_process_callbacks(struct rcu_state *rsp)
{
	unsigned long flags;
	bool needwake;
	struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);

	WARN_ON_ONCE(rdp->beenonline == 0);

	/* Update RCU state based on any recent quiescent states. */
	// 位置1
	rcu_check_quiescent_state(rsp, rdp);

	/* Does this CPU require a not-yet-started grace period? */
	local_irq_save(flags);
	// 位置2
	if (cpu_needs_another_gp(rsp, rdp)) {
		raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
		// 位置3
		needwake = rcu_start_gp(rsp);
		raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
		if (needwake)
		    // 唤醒内核线程
			rcu_gp_kthread_wake(rsp);
	} else {
		local_irq_restore(flags);
	}
}
```

位置1, rcu\_check\_quiescent\_state()会检查**RCU的quiescent state**，目前在此场景下GP还没有开始，我们暂时忽略它。

位置2的if中代码，这里才是真正需要**建立一个GP的时刻**，cpu\_needs\_another\_gp()函数会检查**nxttail**\[RCU\_NEXT\_TAIL\]链表中**是否注册了回调函数**。

位置3代码，调用**rcu\_start\_gp**()尝试去**开启一个GP**。

```c
[kernel/rcu/tree.c]
static bool rcu_start_gp(struct rcu_state *rsp)
{
	struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
	struct rcu_node *rnp = rcu_get_root(rsp);
	bool ret = false;

	ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
	ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
	return ret;
}
```

rcu\_advance\_cbs()函数是**妥善处理rcu\_data中nxttail链表**的函数。刚**初始化**时，**rcu\_data**数据结构中的**nxtcompleted[]值都为0**.

```c
[kernel/rcu/tree.c]
static bool rcu_advance_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
			    struct rcu_data *rdp)
{
	int i, j;

	/* If the CPU has no callbacks, nothing to do. */
	if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
		return false;

	for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
		if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
			break;
		rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
	}
	/* Clean up any sublist tail pointers that were misordered above. */
	for (j = RCU_WAIT_TAIL; j < i; j++)
		rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];

	/* Copy down callbacks to fill in empty sublists. */
	for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
		if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
			break;
		rdp->nxttail[j] = rdp->nxttail[i];
		rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
	}

	/* Classify any remaining callbacks. */
	return rcu_accelerate_cbs(rsp, rnp, rdp);
}
```

rcu\_accelerate\_cbs()函数也用于处理rcu\_data中nxttail链表

```c
[kernel/rcu/tree.c]
static bool rcu_accelerate_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
			       struct rcu_data *rdp)
{
	unsigned long c;
	int i;
	bool ret;

	/* If the CPU has no callbacks, nothing to do. */
	if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
		return false;

	c = rcu_cbs_completed(rsp, rnp);
	for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
		if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
		    !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
			break;

	if (++i >= RCU_NEXT_TAIL)
		return false;

	for (; i <= RCU_NEXT_TAIL; i++) {
		rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
		rdp->nxtcompleted[i] = c;
	}
	/* Record any needed additional grace periods. */
	ret = rcu_start_future_gp(rnp, rdp, NULL);

	/* Trace depending on how much we were able to accelerate. */
	if (!*rdp->nxttail[RCU_WAIT_TAIL])
		trace_rcu_grace_period(rsp->name, rdp->gpnum, TPS("AccWaitCB"));
	else
		trace_rcu_grace_period(rsp->name, rdp->gpnum, TPS("AccReadyCB"));
	return ret;
}
```

上面使用了两个特别的宏————ULONG\_CMP\_GE()和ULONG\_CMP\_LT().

```c
[include/linux/rcupdate.h]
#define ULONG_CMP_GE(a, b) (ULONG_MAX / 2 >= (a) - (b))
#define ULONG_CMP_LT(a, b) (ULONG_MAX / 2 < (a) - (b))
```

ULONG\_CMP\_GE(a, b)用于判断a是否大于等于b, 为什么这里不直接使用a>=b的表达式来判断呢？前文有提到过，RCU数据结构中有一些无符号类型的变量，例如gpnum和 completed是一直增长的，因此这里要很小心地处理溢出的问题。例如b=0xffff\_ffff, a=0, 那么直观感觉是b\>a。但是如果a是从加1后溢出便回滚到0, 那应该是a\>b。ULONG\_MAX/2相当于0xffff\_ffff右移一位后等于0x7fff\_ffff, 它等于有符号类型变量的最大值，那么ULONG\_CMP\_GE(a，b)宏等价于a\-b <= 0x7fff\_ffff。如果a \- b等于一个正数，那么说明a\> b。上面的例子中，b =0xffff\_ffff，a = 0，那么a\-b = 0\-0xffff\_ffff=0xl，符合我们的预期。

同样的道理，ULONG\_CMP\_LT(a，b)用于判断a是否小于b。ULONG\_CMP\_LT(a，b)宏等同于a\-b\> 0x7fff\_ffff, 0x7fff\_ffff再加1将变成0x8000\_0000,即有符号类型的最大负数值。

请读者自行阅读上述rcu\_advance\_cbs()和rcu\_accelerate\_cbs()函数，这里需要根据rdp\-〉nxttail\[\]指针指向、rdp\-\>nxtcompleted\[\]值和**GP的状态调整nxttail链表**。下面给出执行完rcu\_advance\_cbs()后rdp\-〉nxttail链表的情况，如图 4.14 所不。

![config](./images/17.png)

**rcu\_start\_gp\_advanced**()函数**设置rsp\-\>gp\_flags标志位为RCU\_GP\_FLAG\_INIT**, 稍后会去**唤醒RCU内核线程**。从trace\_rcu\_grace\_period()函数可以看到现在的状态变成了“**newreq**”，表示有一个新的GP请求。

```c
[kernel/rcu/tree.c]
static bool
rcu_start_gp_advanced(struct rcu_state *rsp, struct rcu_node *rnp,
		      struct rcu_data *rdp)
{
	if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
		return false;
	}
	ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
	trace_rcu_grace_period(rsp->name, ACCESS_ONCE(rsp->gpnum),
			       TPS("newreq"));

	return true;
}
```

回到\_\_rcu\_process\_callbacks()函数的位置3, **needwake**表示**需要唤醒RCU内核线程**，调用**rcu\_gp\_kthread\_wake**()函数去唤醒它。

## 2.3 初始化一个GP

RCU内核线程就会继续执行, 继续上面初始化后的动作, 执行里面的**rcu\_gp\_init**(), 去真正初始化GP, 这个线程是rcu\_state的

总结:

(1) 当前rcu\_state的rsp\-\>completed和rsp\-\>gpnum不相等, 说明当前已经有一个GP在运行, 不能开启一个新的, 返回

(2) 对**当前rcu\_state**的**rsp\-\>gpnum加1**, 标记状态变成“**newreq\->start**”.

(3) 遍历**当前rcu\_state**的**所有节点rcu\_node**, 对节点node相关变量赋值 

```c
# "start"状态下的rsp、rnp和rdp中关键变量的值变化情况
rsp->gpnum = -299
rsp->completed=-300
rnp->qsmask = rnp->qsmaskinit=0xF
rnp->gpnum=-299 // 重点
rnp->completed = -300
rdp->gpnum=-300 // 重点
rdp->completed =-300
```

(4) 对于**当前正在执行的CPU对应的节点rcu\_node**

- 若**rdp\-\>completed**等于**rnp\-\>completed**(当前CPU的completed等于对应节点的completed), 说明**当前CPU完成一次QS**;
- 不相等, 说明要开启一个GP,  将**所有节点rcu\_node\-\>gpnum**赋值为**rsp\-\>gpnum**, 即让这两个相等, rdp\-\>passed\_quiesce值初始化为0, rdp\-\>qs\_pending初始化为1, 现在状态变成"**newreq\-\>start\-\>cpustart**".

```c
# "cpustart"状态下的rsp、rnp和rdp中关键变量的值变化情况
rsp->gpnum = -299
rsp->completed=-300
rnp->qsmask = rnp->qsmaskinit=0xF
rnp->gpnum=-299
rnp->completed = -300
rdp->gpnum=-299  // 重点
rdp->completed =-300
rdp->passed_quiesce = 0
rdp->gpwrap = false
```

(5) 初始化GP后, 将进入**fqswait状态**, 继续**睡眠等待**, 两个条件可唤醒: 

- **rsp\-\>gp\_flags**状态标志位被设置为**RCU\_GP\_FLAG\_FQS**, 即有**强制处理quiescent state请求**; 
- **rnp\-\>qsmask(这个rnp是当前rcu\_state的Tree RCU的root根节点！！！**)被清0(以4核为例, 创建GP时rnp\-\>qsmask是0xF, 每个比特位代表一个CPU的rcu\_data, 被清零说明4个CPU都经历了quiescent state).

刚才己**唤醒了RCU内核线程**，在内核线程处理函数**rcu\_gp\_kthread**()中的第20行代码**rcu\_gp\_init**()函数才是**真正去初始化一个GP**。

```c
[rcu_gp_kthread() ->rcu_gp_init()]
static int rcu_gp_init(struct rcu_state *rsp)
{
	struct rcu_data *rdp;
	struct rcu_node *rnp = rcu_get_root(rsp);

	raw_spin_lock_irq(&rnp->lock);
	smp_mb__after_unlock_lock();
	// 位置1
	ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    // 位置2
	if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
		raw_spin_unlock_irq(&rnp->lock);
		return 0;
	}

	/* Record GP times before starting GP, hence smp_store_release(). */
	// 位置3
	smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
	// 位置4
	trace_rcu_grace_period(rsp->name, rsp->gpnum, TPS("start"));
	raw_spin_unlock_irq(&rnp->lock);
	mutex_lock(&rsp->onoff_mutex);
	smp_mb__after_unlock_lock(); /* ->gpnum increment before GP! */
	// 位置5
	rcu_for_each_node_breadth_first(rsp, rnp) {
		raw_spin_lock_irq(&rnp->lock);
		smp_mb__after_unlock_lock();
		rdp = this_cpu_ptr(rsp->rda);
		rnp->qsmask = rnp->qsmaskinit;
		ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
		WARN_ON_ONCE(rnp->completed != rsp->completed);
		ACCESS_ONCE(rnp->completed) = rsp->completed;
		// 位置6
		if (rnp == rdp->mynode)
			(void)__note_gp_changes(rsp, rnp, rdp);
		trace_rcu_grace_period_init(rsp->name, rnp->gpnum,
					    rnp->level, rnp->grplo,
					    rnp->grphi, rnp->qsmask);
		raw_spin_unlock_irq(&rnp->lock);
		cond_resched_rcu_qs();
		ACCESS_ONCE(rsp->gp_activity) = jiffies;
	}

	mutex_unlock(&rsp->onoff_mutex);
	return 1;
}
```

位置1代码，首先把rsp->gp\_flags标志位清0。

位置2代码，rcu\_gp\_in\_progress()函数判断**rsp\-\>completed和rsp\-\>gpnum**是否相等，如果**不相等**，说明当前**有一个GP正在执行**中，那么**不能开启一个新的GP**。

位置3代码，对**rsp\-\>gpnum**变量进行**加1操作**，使用smp\_store\_release()原子操作，它在修改变量之前插入smp\_mb()指令以便保证之前的读写操作己经完成。这时**rsp\-〉gpnum值**从**初始化的\-300**变**成\-299**。

位置4代码，trace\_rcu\_grace\_period()函数标记现在的**状态**转变为“**newreq\->start**”。

位置5整个，遍历**当前rcu\_state**中**所有的rcu\_node**节点，然后对rcu\_node节点的相关变量进行赋值。

```c
[kernel/rcu/tree.h]
#define rcu_for_each_node_breadth_first(rsp, rnp) \
	for ((rnp) = &(rsp)->node[0]; \
	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
```
rcu\_for\_each\_node\_breadth\_first()从**rcu\_node根节点开始遍历**。下面列举出“**start”状态**下rcu\_state、rcu\_node和rcu\_data数据结构中关键成员变量的变化情况。

```c
# "start"状态下的rsp、rnp和rdp中关键变量的值变化情况
rsp->gpnum = -299
rsp->completed=-300
rnp->qsmask = rnp->qsmaskinit=0xF
rnp->gpnum=-299 // 重点
rnp->completed = -300
rdp->gpnum=-300 // 重点
rdp->completed =-300
```

位置6两行, rdp\-\>mynode指向rcu\_data所属的父节点rcu\_node。\_\_**note\_gp\_changes**()函数用于**记录一个GP的开始和结束**。注意rcu\_for\_each\_node\_breadth\_first()会**遍历所有的rcu node**，但是**只有在执行rcu\_gp\_kthread线程的CPU**上才会调用\_\_**note\_gp\_changes**()，其他CPU则不会调用这个函数。

```c
[rcu_gp_kthread()->rcu_gp_init() -> __note_gp_changes()]
static bool __note_gp_changes(struct rcu_state *rsp, struct rcu_node *rnp,
			      struct rcu_data *rdp)
{
	bool ret;

	/* Handle the ends of any preceding grace periods first. */
	if (rdp->completed == rnp->completed &&
	    !unlikely(ACCESS_ONCE(rdp->gpwrap))) {

		/* No grace period end, so just accelerate recent callbacks. */
		// 位置1
		ret = rcu_accelerate_cbs(rsp, rnp, rdp);

	} else {
		ret = rcu_advance_cbs(rsp, rnp, rdp);
		rdp->completed = rnp->completed;
		trace_rcu_grace_period(rsp->name, rdp->gpnum, TPS("cpuend"));
	}
    // 位置2
	if (rdp->gpnum != rnp->gpnum || unlikely(ACCESS_ONCE(rdp->gpwrap))) {
		rdp->gpnum = rnp->gpnum;
		trace_rcu_grace_period(rsp->name, rdp->gpnum, TPS("cpustart"));
		rdp->passed_quiesce = 0;
		rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
		rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
		ACCESS_ONCE(rdp->gpwrap) = false;
	}
	return ret;
}
```

**rdp\-\>completed等于rnp\-\>completed**，说明当前**CPU**己经**完成了一次Quiescent State状态**。

\_\_note\_gp\_changes()函数会在**一个GP开始和结束时被调用到**，GP结束时调用\_\_note\_gp\_changes()函数，rnp\-\>completed和rdp\-\>completed的值不一样，稍后讲解rcu\_gp\_cleanup()时会看到这个变化。

言归正传，位置1代码中的rcu\_accelerate\_cbs()函数也用于处理nxttail链表，如图4.15所示。

把rdp\-\>completed赋值为rnp\-\>completed并不是说明GP还没有开始，而是当前CPU己经进入了Quiescent state，不需要再处理quiescent state的检测。注意rcu\_fbr\_each\_node\_breadth\_first()会遍历所有的rcu node，但是只有在执行了rcu\_gp\_init()函数的CPU上才会去调用\_\_note\_gp\_changes()，因为执行rcu\_gp\_init()的线程本身不会使用RCU，因此可以安全地认为它在Quiescent state中。

![config](./images/18.png)

位置2, **rdp\-\>completed不等于rnp\-\>completed**, 说明要开启一个新的GP, 因为rnp\-\>completed的值在**rcu\_gp\_init**()中被原子的加1, 而rdp\-\>completed值却还没被修改过. 把**rdp\-\>completed值**赋值等于**rnp\-\>completed**, rdp\-\>passed\_quiesce值初始化为0, rdp\->qs\_pending初始化为1, 现在状态变成"newreq\->start\-\>cpustart".

```c
# "cpustart"状态下的rsp、rnp和rdp中关键变量的值变化情况
rsp->gpnum = -299
rsp->completed=-300
rnp->qsmask = rnp->qsmaskinit=0xF
rnp->gpnum=-299
rnp->completed = -300
rdp->gpnum=-299  // 重点
rdp->completed =-300
rdp->passed_quiesce = 0
rdp->gpwrap = false
```

回到rcu\_gp\_kthread()函数中, rcu\_gp\_init()函数初始化完成后, 将退出当前for循环, 进入下一个for循环中.

```c
[rcu_gp_kthread()]
static int __noreturn rcu_gp_kthread(void *arg)
{
    ...
        /* Handle quiescent-state forcing. */
        // 第二个for循环
		for (;;) {
			if (!ret)
				rsp->jiffies_force_qs = jiffies + j;
			trace_rcu_grace_period(rsp->name,
					       ACCESS_ONCE(rsp->gpnum),
					       TPS("fqswait"));
			rsp->gp_state = RCU_GP_WAIT_FQS;
			ret = wait_event_interruptible_timeout(rsp->gp_wq,
					((gf = ACCESS_ONCE(rsp->gp_flags)) &
					 RCU_GP_FLAG_FQS) ||
					(!ACCESS_ONCE(rnp->qsmask) &&
					 !rcu_preempt_blocked_readers_cgp(rnp)),
					j);
			/* Locking provides needed memory barriers. */
			/* If grace period done, leave loop. */
			if (!ACCESS_ONCE(rnp->qsmask) &&
			    !rcu_preempt_blocked_readers_cgp(rnp))
				break;
			...
		}
```

RCU内核线程处理函数rcu\_gp\_kthread()中第二个for循环会进入"**fqswait"状态**, wait\_event\_interruptible\_timeout()让**该线程进入睡眠等待**, 被唤醒的条件有**两个**, 一是**rsp\-\>gp\_flags**状态标志位被**设置为RCU\_GP\_FLAG\_FQS**, 即有**强制处理quiescent state请求**; 二是**rnp\-\>qsmask被清0(这个rnp是当前rcu\_state的Tree RCU的root根节点！！！**). 以4核处理器为例, **创建GP**时rnp\-\>qsmask值为**0xF**, **每个比特位**代表一个CPU上的**rcu\_data**数据结构, 该值被**清0**, 表明**4个CPU**都经历过了**quiescent state**.

## 2.4 检测quiescent state

时钟中断处理函数中**update\_process\_times**()

总结:

(1) **rcu\_sched**和**rcu\_bh类型**的**RCU**, **当前CPU**处于**usermode**或处于**idle线程(从idle切过来的**)中, 离开了RCU临界区, 即经历了quiescent state.

(2) 现在**没有处于softirq上下文**中，对于**rcu\_bh类型**的RCU来说, 也是




**时钟中断处理函数**会调用**update\_process\_times**()函数判断**当前CPU**是否经过了一个**quiescent state**。

```c
[tick_handle_periodic() ->tick_periodic() ->update_process_times() ->update_process_times()]
[kernel/time/timer.c]
void update_process_times(int usertick)
{
    struct task_struct *p = current;
    ...
    rcu_check_callbacks(user_tick);
    scheduler_tick();
}
```

update\_process\_times()函数的参数**user\_tick**通过user\_mode()宏判断当前是否在 usermode。

```c
[kernel/rcu/tree.c]
void rcu_check_callbacks(int user)
{
	trace_rcu_utilization(TPS("Start scheduler-tick"));
	if (user || rcu_is_cpu_rrupt_from_idle()) {
		rcu_sched_qs();
		rcu_bh_qs();
    // 位置1
	} else if (!in_softirq()) {
		rcu_bh_qs();
	}
	rcu_preempt_check_callbacks();
	// 位置2
	if (rcu_pending())
		invoke_rcu_core();
	trace_rcu_utilization(TPS("End scheduler-tick"));
}
```

如何检测一个CPU是否已经经历过了quiescent state? 对于**rcu\_sched**和**rcu\_bh类型**的**RCU**来说，当在**时钟tick处理函数**中，检测到**当前CPU**处于**usermode**或处于**idle线程**中，说明从**开始一个GP**到**当前时刻**，当前**CPU己经离开了RCU临界区**，即经历过了quiescent state。

位置1代码，如果现在**没有处于softirq上下文**中，对于**rcu\_bh类型**的RCU来说，也经历过了一个quiescent state。

```c
void rcu_sched_qs(void)
{
	if (!__this_cpu_read(rcu_sched_data.passed_quiesce)) {
		__this_cpu_write(rcu_sched_data.passed_quiesce, 1);
	}
}
```

**rcu\_sched\_qs**()函数会往**本地CPU**的**rcu\_sched\_data**中的**passed\_quiesce成员写1**，表示该CPU经历过了一个**quiescent state**。

回到rcu\_check\_callbacks()函数中位置2代码，**rcu\_pending()函数判断是否需要触发RCU软中断**。

```c
[kernel/rcu/tree.c]
static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
{
	struct rcu_node *rnp = rdp->mynode;

	/* Is the RCU core waiting for a quiescent state from this CPU? */
	if (rcu_scheduler_fully_active &&
	    rdp->qs_pending && !rdp->passed_quiesce &&
	    rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr)) {
		rdp->n_rp_qs_pending++;
	} else if (rdp->qs_pending &&
		   (rdp->passed_quiesce ||
		    rdp->rcu_qs_ctr_snap != __this_cpu_read(rcu_qs_ctr))) {
		rdp->n_rp_report_qs++;
		return 1;
	}

	...
}
```

因为 rdp\-\>passed\_quiesce 被设置为1，所以rdp\->n\_rp\_report\_qs\+\+，并且返回 true，因此它会触发RCU软中断。

在RCU软中断处理函数\_\_rcu\_process\_callbacks()，先来看rcu\_check\_quiescent\_state()如何更新 RCU 的 quiescent states。

```c

```

首先note\_gp\_changes()函数会检查本地CPU的rcu\_data和对应的rcu\_node节点上的重要成员的变量。

```c
```

note\_gp\_changes()函数根据本地CPU对应的rcu\_data和对应的rcu\_node节点上的gpnum和completed值来判断G P 的状态是否有改变，这里要分如下两种情况来看。

........

rcu\_report\_qs\_mp()函数中的for循环会遍历整个rcu\_node层次结构，从**本地CPU对应的rcu\_node开始**向上遍历。注意**遍历方向是从下向上**，而**不会遍历同level的所有rcu\_node节点**。其中，参数mask指**本地CPU对应的rcu\_data**数据结构中**grpmask成员**。**rcu\_node**数据结构也有一个成员**qsmask**来描述它管辖的**CPU或子节点的cpumask位图**。第9行代码，如果CPU对应的rnp\-\>qsmask比特位己经被清0 , 说明之前执行过这部分代码了，直接返回。第 15行代码，**清除rnp\-\>qsmask位图**中**当前CPU对应的比特位**。第 16〜21行代码，清除了**本地CPU对应的比特位**后，**rnp\-\>qsmask还有比特位存在**，说明rcu\_node节点上还有其他CPU对应的rcu\_data还没有完成quiescent state状态，只能**直接返回**。这里必须要等待该rcu\_node节点上所有CPU都完成了quiescent state，并且**清除完rnp\-\>qsmask位图**，才能**向上遍历上一级的rcu\_node节点**。

程序运行到第22行代码，说明**这一层的rcu\_node中的qsmask位图**都己清除干净，那么就要剑指**上一级rcu\_node**了。**rcu\_node中的grpmask成员**，和rcu\_data数据结构中的grpmask成员含义相同，都是指**在父节点的位图中所在的比特位**。第28行代码，获取当前rcu\_node节点的父节点，然后继续一直按照上述逻辑清除父节点rcu\_node\-\>qsmask位图。

代码表示一直遍历到该Tree RCU树形结构的根节点，且根节点的rcu\_node\-\>qsmask位图被清除干净才会退出for循环。注意这里**只有清除完成根节点的rcu\_node\-\>qsmask位图**并且安全退出for循环，才有机会执行rcu\_report\_qs\_rsp()函数，其他情况都是直接退出该函数。

```c
static void rcu_report_qs_rsp(struct rcu_state *rsp, unsigned long flags)
	__releases(rcu_get_root(rsp)->lock)
{
	WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
	raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
	rcu_gp_kthread_wake(rsp);
}
```

rcu\_report\_qs\_rsp() 函数首先通过rcu\_gp\_in\_progress()判断当前是否处于GP的执行过程中，判断条件是rsp\-\>completed是否等于rsp\-\>gpnum 。如果不相等，说明正在一个GP的执行过程中，WARN\_ON\_ONCE() 是比较弱的 debug 语句，然后调用**rcu\_gp\_kthread\_wake**()函数去**唤醒RCU内核线程**。

## 2.5 GP结束

接着上面的RCU内核线程执行, 由于**Tree RCU根节点**的**rnp\-\>qsmask被清除干净**了, 所以被唤醒, 继续执行rcu\_gp\_cleanup(). 

总结:

(1) 从根节点遍历整个Tree, 每个节点rcu\_node\->completed都设置成rsp\-\>gpnum

(2) 对于当前CPU的rdp\-\>completed赋值为rnp\-\>completed, 标记GP状态为“cpuend”

(3) **rsp\-\>completed**值也设置成与**rsp\-\>gpnum一样**, 把状态标记为“end”，最后把**rsp\-\>fqs\_state**的状态设置为**初始值RCU\_GP\_IDLE**, 一个GP的生命周期真正己经完成了

回到RCU内核线程的处理函数**rcu\_gp\_kthread**()函数中，之前该内核线程被阻塞在**wait\_event\_interruptible\_timeout**()函数中，现在**调用rcu\_gp\_kthread\_wake**()函数去唤醒它。

由于**Tree RCU根节点**的**rnp\-\>qsmask被清除干净**了，所以**内核线程**也很快**退出了第2个for循环**，最后运行到rcu\_gp\_kthread()函数中的最后一步**rcu\_gp\_cleanup**()中。

```c
[rcu_gp_kthread()]
static int __noreturn rcu_gp_kthread(void *arg)
{
    ...
    for(;;){
        for(;;){
            
        }
        
        for(;;){
            
        }
    
        /* Handle grace-period end. */
        // 第三步
		rcu_gp_cleanup(rsp);
    }
    
}

static void rcu_gp_cleanup(struct rcu_state *rsp)
{
	unsigned long gp_duration;
	bool needgp = false;
	int nocb = 0;
	struct rcu_data *rdp;
	struct rcu_node *rnp = rcu_get_root(rsp);

	raw_spin_lock_irq(&rnp->lock);
	smp_mb__after_unlock_lock();
	raw_spin_unlock_irq(&rnp->lock);
    // 位置1
	rcu_for_each_node_breadth_first(rsp, rnp) {
		raw_spin_lock_irq(&rnp->lock);
		smp_mb__after_unlock_lock();
		// 位置2
		ACCESS_ONCE(rnp->completed) = rsp->gpnum;
		rdp = this_cpu_ptr(rsp->rda);
		// 位置3
		if (rnp == rdp->mynode)
			needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
		/* smp_mb() provided by prior unlock-lock pair. */
		raw_spin_unlock_irq(&rnp->lock);
		cond_resched_rcu_qs();
	}
	rnp = rcu_get_root(rsp);
	raw_spin_lock_irq(&rnp->lock);
	smp_mb__after_unlock_lock(); /* Order GP before ->completed update. */

	/* Declare grace period done. */
	// 位置4
	ACCESS_ONCE(rsp->completed) = rsp->gpnum;
	trace_rcu_grace_period(rsp->name, rsp->completed, TPS("end"));
	rsp->fqs_state = RCU_GP_IDLE;
	raw_spin_unlock_irq(&rnp->lock);
}
```

位置1, rcu\_for\_each\_node\_breadth\_first()函数**从rcu\_node根节点开始遍历整个RCU树形结构**。

位置2代码，**每个rcu\_node\-\>completed**成员都设置成**rsp\-\>gpnum一样的值**，在此场景中，rcu\_node\-\>completed = rsp\-\>gpnum = \-299。

位置3，对于**当前CPU对应的rcu\_node节点**，需要调用\_\_note\_gp\_changes()函数做一些清理工作。

```c
[kernel/rcu/tree.c]
static bool __note_gp_changes(struct rcu_state *rsp, struct rcu_node *rnp,
			      struct rcu_data *rdp)
{
	bool ret;

	/* Handle the ends of any preceding grace periods first. */
	if (rdp->completed == rnp->completed &&
	    !unlikely(ACCESS_ONCE(rdp->gpwrap))) {
        ...
	} else {
	    // 位置1
		ret = rcu_advance_cbs(rsp, rnp, rdp);
		// 位置2
		rdp->completed = rnp->completed;
		trace_rcu_grace_period(rsp->name, rdp->gpnum, TPS("cpuend"));
	}

	if (rdp->gpnum != rnp->gpnum || unlikely(ACCESS_ONCE(rdp->gpwrap))) {
		rdp->gpnum = rnp->gpnum;
		trace_rcu_grace_period(rsp->name, rdp->gpnum, TPS("cpustart"));
		...
	}
	return ret;
}
```

注意这时**rdp\-\>completed=\-300**，而**rnp\-\>completed=\-299**，因此这里会运行到位置1代码中。首先调用rcu\_advance\_cbs()函数来处理nxttail链表的情况，rcu\_advance\_cbs()函数之前己经介绍过，这次的情况如图4.16所示。

位置2代码把**rdp\-\>completed**赋值为**rnp\-\>completed**，即值为\-299,最后trace\_rcu\_grace\_period()标记GP状态为“cpuend”。

回到rcu\_gp\_cleanup()函数的位置4代码，这里才真正**标记一个GP的结束**，**rsp\-\>completed**值也设置成与**rsp\-\>gpnum一样**，等于\-299。trace\_rcu\_grace\_period()**把状态标记为“end**”，最后把**rsp\-\>fqs\_state**的状态设置为**初始值RCU\_GP\_IDLE**, 因此一个GP的生命周期己经完成了。

从代码中的**trace功能定义**的状态来看，**一个GP需要经历的状态转换**为: “**newreq \-\> start \-\> cpustart \-\> fqswait \-\> cpuend \-\>end**”。

![config](./images/19.png)

## 2.6 回调函数

当整个GP结束之后，就到了RCU最后一步，即调用回调函数来做一些销毁动作，调用回调函数还是在RCU软中断中触发。

.......

# 3 小结

## 3.1 背景和原理

spinlock、读写信号量和mutex的实现，它们都使用了原子操作指令，即原子地访问内存，多CPU争用共享的变量会让cache一致性变得很糟(！！！)，使得性能下降。读写信号量还有一个致命缺点, 只允许多个读者同时存在, 但读者和写者不能同时存在.

RCU实现目标是, 读者线程没有同步开销(不需要额外的锁, 不需要使用原子操作指令和内存屏障); 同步任务交给写者线程, 写者线程等所有读者线程完成把旧数据销毁, 多个写者则需要额外保护机制.

原理: RCU记录所有指向共享数据的指针的使用者, 当修改该共享数据时，首先创建一个副本，在副本中修改。所有读访问线程都离开读临界区之后，指针指向新的修改后副本的指针，并且删除旧数据。

## 3.2 操作接口

接口如下:

- rcu\_read\_lock()/rcu\_read\_unlock(): 组成一个**RCU读临界**。
- rcu\_dereference(): 用于获取**被RCU保护的指针**(RCU protected pointer)，**读者线程**要访问**RCU保护的共享数据**，需要使用**该函数创建一个新指针**，并且**指向RCU被保护的指针**。
- rcu\_assign\_pointer(): 通常用在**写者线程**。在**写者线程**完成新数据的**修改**后，调用该接口可以让**被RCU保护的指针**指向**新创建的数据**，用RCU的术语是发布(Publish) 了更新后的数据。
- synchronize\_rcu(): 同步等待**所有现存的读访问完成**。
- call\_rcu(): 注册一个**回调函数(！！！**)，当**所有**现存的**读访问完成**后，调用这个回调函数**销毁旧数据**。

可以看上面的使用例子

## 3.3 基本概念

Grace Period, 宽限期: **GP开始**到**所有读者临界区的CPU离开**算一个GP, GP结束调用回调函数

Quiescent State, 静止状态: 一个CPU处于读者临界区, 说明活跃, 离开读者临界区, 静止态

经典RCU使用全局cpumask位图, 每个比特一个CPU. 每个CPU在GP开始设置对应比特, 结束清相应比特. 多CPU会竞争使用, 需要使用spinlock, CPU越多竞争越惨烈.

Tree RCU解决cpumask位图竞争问题.

![config](./images/10.png)

以4核处理器为例，假设Tree RCU把**两个CPU**分成**1个rcu\_node节点**，这样**4个CPU**被分配到**两个rcu\_node**节点上，另外还有**1个根rcu\_node**节点来**管理这两个rcu\_node节点**。如图4.8所示，**节点1**管理**cpuO**和**cpul**，节点2管理cpu2和cpu3，而节点0是根节点，管理节点1和节点2。**每个节点**只需要**两个比特位的位图(！！！**)就可以**管理各自的CPU**或者节点，**每个节点(！！！**)都有**各自的spinlock锁(！！！**)来**保护相应的位图**。

注意: **CPU不算层次level！！！**

假设**4个CPU**都经历过**一个QS状态**，那么**4个CPU**首先在**Level 0层级**的**节点1**和**节点2**上**修改位图**。对于**节点1**或者**节点2**来说，**只有两个CPU来竞争锁**，这比经典RCU上的锁争用要减少一半。当**Level 0**上**节点1**和**节点2**上**位图都被清除干净后(！！！**)，才会清除**上一级节点的位图**，并且**只有最后清除节点的CPU(！！！**)才有机会去**尝试清除上一级节点的位图(！！！**)。因此对于节点0来说，还是两个CPU来争用锁。整个过程都是只有两个CPU去争用一个锁，比经典RCU实现要减少一半。

## 3.4 Linux实现

### 3.4.1 相关数据结构定义

Tree RCU实现, 定义了3个很重要的数据结构，分别是struct rcu\_data、struct rcu\_node和struct rcu\_state，另外还维护了一个比较隐晦的状态机(！！！).

- struct rcu\_data数据结构定义成Per\-CPU变量. gpnum和completed用于GP状态机的运行状态, 初始两个都等于\-300, 新建一个GP, gpnum加1; GP完成, completed加1. passed\_quiesce(bool): 当时钟tick检测到rcu\_data对应的CPU完成一次Quiescent State, 设这个为true. qs\_pending(bool): CPU正等待QS.
- struct rcu\_node是Tree RCU中的组成节点，它有根节点(Root Node)和叶节点之分。如果Tree RCU只有一层，那么根节点下面直接管理着一个或多个rcu\_data;如果Tree RCU有多层结构，那么根节点管理着多个叶节点，**最底层的叶节点**管理者**一个或多个rcu\_data**。
- RCU系统支持多个不同类型的RCU状态，使用struct rcu\_state数据结构来描述这些状态。每种RCU类型都有独立的层次结构(！！！)，即根节点和rcu\_data数据结构。也有gpnum和completed.

Tree通过三个维度确定层次关系: **每个叶子的CPU数量(CONFIG\_RCU\_FANOUT\_LEAF**), 每层的最多叶子数量(CONFIG\_RCU\_FANOUT), 最多层数(MAX\_RCU\_LVLS宏定义, 是4, CPU不算level层次！！！)

![config](./images/13.png)

在**32核处理器**中，层次结构分成两层，**Level 0**包括**两个struct rcu\_node(！！！**)，其中**每个struct rcu\_node**管理**16个struct rcu\_data(！！！**)数据结构，分别表示**16个CPU的独立struct rcu\_data**数据结构; 在**Level 1**层级，有**一个struct rcu\_node(！！！**)节点**管理**着**Level 0层级**的**两个rcu\_node**节点，**Level 1**层级中的**rcu\_node**节点称为**根节点**，**Level 0**层级的**两个rcu\_node**节点是**叶节点**。

### 3.4.2 内核启动进行RCU初始化

![config](./images/15.png)

(1) 初始化3个rcu\_state, rcu\_sched\_state(普通进程上下文的RCU)、rcu\_bh\_state(软中断上下文)和rcu\_preempt\_state(系统配置了CONFIG\_PREEMPT\_RCU, 默认使用这个)

(2) 注册一个SoftIRQ回调函数

(3) 初始化每个rcu\_state的层次结构和相应的Per\-CPU变量rcu\_data

(4) 为每个rcu\_state初始化一个内核线程, 以rcu\_state命名

### 3.4.3 开启一个GP

1. **写者程序注册RCU回调函数**:

**RCU写者程序(！！！**)通常需要调用**call\_rcu**()、**call\_rcu\_bh**()或**call\_rcu\_sched**()等函数来**通知RCU**系统**注册一个RCU回调函数(！！！**)。对应上面的三种state.

- 参数: rcu_head(每个RCU保护的数据都会内嵌一个), 回调函数指针(GP结束<读者执行完>, 被调用销毁)

- 将rcu\_head加入到本地rcu\_data的nxttail链表

2. 总结: 每次**时钟中断**处理函数**tick\_periodic**(), 检查**本地CPU**上**所有的rcu\_state(！！！**)对应的**rcu\_data**成员**nxttail链表有没有写者注册的回调函数**, 有的话**触发一个软中断raise\_softirq**().

3. **软中断处理函数**, 针对**每一个rcu\_state(！！！**): 检查rcu\_data成员nxttail链表**有没有写者注册的回调函数**, 有的话, 调整链表, 设置**rsp\->gp\_flags**标志位为**RCU\_GP\_FLAG\_INIT**, rcu\_gp\_kthread\_wake()唤醒**rcu\_state对应的内核线程(！！！**), 现在的状态变成了“**newreq**”，表示有**一个新的GP请求**, **rcu\_gp\_kthread\_wake**()唤醒**rcu\_state对应的内核线程(！！！**)

### 3.4.4 初始化一个GP

RCU内核线程就会继续执行, 继续上面初始化后的动作, 执行里面的**rcu\_gp\_init**(), 去真正初始化一个GP, 这个线程是rcu\_state的

(1) 当前rcu\_state的rsp\-\>completed和rsp\-\>gpnum不相等, 说明当前已经有一个GP在运行, 不能开启一个新的, 返回

(2) 将rsp\-\>gpnum加1

(3) 遍历所有node, 将所有node的gpnum赋值为rsp\-\>gpnum

(4) 对于当前CPU对应的节点rcu\_node, 

- 若rdp\-\>completed等于rnp\-\>completed(当前CPU的completed等于对应节点的completed), 说明当前CPU完成一次QS; 
- 不相等, 说明要开启一个GP, 将**所有节点rcu\_node\-\>gpnum**赋值为**rsp\-\>gpnum**, rdp\-\>passed\_quiesce值初始化为0, rdp\-\>qs\_pending初始化为1, 现在状态变成"**newreq\-\>start\-\>cpustart**".

(5) 初始化GP后, 进入fswait状态, 继续睡眠等待

### 3.4.5 检测QS

时钟中断处理函数判断当前CPU是否经过了一个quiescent state, 即退出了RCU临界区, 退出后自下往上清理Tree RCU的qsmask位图, 直到根节点rcu\_node\-\>qsmask位图清理后, 唤醒RCU内核线程

### 3.4.6 GP结束

接着上面的RCU内核线程执行, 由于**Tree RCU根节点**的**rnp\-\>qsmask被清除干净**了.

(1) 将**所有节点(！！！CPU的不是节点)rcu\_node**\->completed都设置成rsp\-\>gpnum, 当前CPU的rdp\-\>completed赋值为rnp\-\>completed, GP状态"cpuend"

(2) **rsp\-\>completed**值也设置成与**rsp\-\>gpnum一样**, 把状态标记为“end”，最后把**rsp\-\>fqs\_state**的状态设置为**初始值RCU\_GP\_IDLE**, 一个GP的生命周期真正完成

### 3.4.7 回调函数

整个GP结束, RCU调用回调函数做一些销毁动作, 还是在**RCU软中断中触发**.

从代码中的**trace功能定义**的状态来看，**一个GP需要经历的状态转换**为: “**newreq \-\> start \-\> cpustart \-\> fqswait \-\> cpuend \-\>end**”。

总结Tree RCU的实现中有如下几点需要大家再仔细体会。

- Tree RCU为了避免**修改CPU位图带来的锁争用**，巧妙设置了树形的层次结构，**rcu\_data**、**rcu\_node**和**rcu\_state**这 3 个数据结构组成一棵完美的树。
- Tree RCU的实现维护了一个**状态机**，这个状态机若隐若现，只有把**trace功能打开**了才能感觉到该状态机的存在，trace函数是trace\_rcu\_grace\_period()。
- 维护了一些以rcu\_data\-\>nxttail\[\]二级指针为首的链表，该链表的实现很巧妙地运用了二级指针的指向功能。
- rcu\_data、rcu\_node和rcu\_state这3个数据结构中的gpnum、completed、grpmask、passed\_quiesce、qs\_pending、qsmask等成员，正是这些成员的值的变化推动了Tree RCU状态机的运转。

RCU很复杂, 例如中断/NMI对RCU的处理、可睡眠RCU、可抢占RCU等内容都没有提及到。可以参考《Is Parallel Programming Hard, And, If So, What Can You Do About It?》(中文名《深入理解并行编程》)