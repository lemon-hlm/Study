[TOC]

# 1 前言

对于**中断处理**而言，linux将其分成了**两个部分**，一个叫做**中断handler（top half**），是**全程关闭中断**的，另外一部分是**deferable task（bottom half**），属于**不那么紧急需要处理**的事情。在执行bottom half的时候，是**开中断**的。有**多种bottom half的机制**，例如：**softirq**、**tasklet**、**workqueue**或是**直接创建一个kernel thread**来执行bottom half（这在旧的kernel驱动中常见，现在，一个理智的driver厂商是不会这么做的）。本文主要讨论softirq机制。由于**tasklet是基于softirq**的，因此本文也会提及tasklet，但主要是从需求层面考虑，不会涉及其具体的代码实现。

在普通的驱动中**一般是不会用到softirq**，但是由于驱动经常使用的tasklet是基于softirq的，因此，了解softirq机制有助于撰写更优雅的driver。**softirq不能动态分配**，都是**静态定义**的。**内核**已经定义了**若干种softirq number**，例如**网络数据的收发**、**block设备的数据访问**（数据量大，通信带宽高），**timer的deferable task**（时间方面要求高）。本文的第二章讨论了**softirq**和**tasklet**这两种机制**有何不同**，分别适用于**什么样的场景**。第三章描述了一些context的概念，这是要理解后续内容的基础。第四章是进入**softirq的实现**，对比hard irq来解析soft irq的注册、触发，调度的过程。

注：本文中的linux kernel的版本是3.14

# 2 为何有softirq和tasklet

## 2.1 为何有top half和bottom half

中断处理模块是任何OS中最重要的一个模块，对系统的性能会有直接的影响。想像一下：如果在通过U盘进行大量数据拷贝的时候，你按下一个key，需要半秒的时间才显示出来，这个场景是否让你崩溃？因此，对于那些复杂的、需要大量数据处理的硬件中断，我们不能让handler中处理完一切再恢复现场（handler是全程关闭中断的），而是仅仅在**handler**中**处理一部分**，具体包括：

（1）有**实时性**要求的

（2）和**硬件相关**的。例如**ack中断**，**read HW FIFO to ram**等

（3）如果是**共享中断**，那么获取**硬件中断状态**以便**判断是否是本中断发生**

除此之外，其他的内容都是放到bottom half中处理。在把中断处理过程划分成top half和bottom half之后，关中断的top half被瘦身，可以非常快速的执行完毕，大大减少了系统关中断的时间，提高了系统的性能。

我们可以基于下面的系统进一步的进行讨论：

![config](./images/10.gif)

当**网卡控制器**的FIFO收到的来自**以太网的数据**的时候（例如半满的时候，可以软件设定），可以**将该事件**通过**irq signal**送达**Interrupt Controller**。Interrupt Controller可以把中断**分发给系统中的Processor A or B**。

**NIC的中断处理过程**大概包括：**mask and ack interrupt controller**-------->**ack NIC**-------->**copy FIFO to ram**------>**handle Data in the ram**----------->**unmask interrupt controller**

我们先假设**Processor A处理**了这个**网卡中断事件**，于是NIC的中断handler在Processor A上欢快的执行，这时候，**Processor A的本地中断是disable**的。NIC的中断handler在执行的过程中，**网络数据仍然源源不断的到来**，但是，如果**NIC的中断handler不操作NIC的寄存器**来**ack这个中断**的话，NIC是**不会触发下一次中断**的。还好，我们的NIC interrupt handler总是在**最开始就会ack**，因此，这不会导致性能问题。ack之后，NIC已经具体再次trigger中断的能力。当Processor A上的handler 在处理接收来自网络的数据的时候，**NIC的FIFO很可能又收到新的数据**，并**trigger了中断**，这时候，**Interrupt controller还没有umask**，因此，即便还有Processor B（也就是说有**处理器资源**），**中断控制器**也**无法把这个中断送达处理器系统**。因此，只能眼睁睁的看着NIC FIFO填满数据，数据溢出，或者向对端发出拥塞信号，无论如何，整体的系统性能是受到严重的影响。

注意：对于新的interrupt controller，可能没有mask和umask操作，但是原理是一样的，只不过NIC的handler执行完毕要发生EOI而已。

要解决上面的问题，最重要的是尽快的**执行完中断handler，打开中断，unmask IRQ（或者发送EOI**），方法就是把**耗时的handle Data in the ram**这个步骤**踢出handler**，让其在bottom half中执行。

## 2.2 为何有softirq和tasklet

OK，linux kernel已经把中断处理分成了top half和bottom half，看起来已经不错了，那**为何**还要提供**softirq**、**tasklet**和**workqueue**这些bottom half机制，linux kernel本来就够复杂了，bottom half还来添乱。实际上，在早期的linux kernel还真是只有一个bottom half机制，简称BH，简单好用，但是性能不佳。后来，linux kernel的开发者开发了task queue机制，试图来替代BH，当然，最后task queue也消失在内核代码中了。现在的linux kernel提供了**三种bottom half的机制**，来应对**不同的需求**。

**workqueue**和**softirq**、**tasklet**有**本质的区别**：**workqueue**运行在**process context**，而**softirq**和**tasklet**运行在**interrupt context**。因此，出现workqueue是不奇怪的，在有**sleep需求**的场景中，defering task必须延迟到kernel thread中执行，也就是说**必须使用workqueue机制**。softirq和tasklet是怎么回事呢？从本质上将，**bottom half机制**的设计有**两方面的需求**，一个是**性能**，一个是**易用性**。设计一个通用的bottom half机制来满足这两个需求非常的困难，因此，内核提供了softirq和tasklet两种机制。**softirq更倾向于性能**，而**tasklet更倾向于易用性**。

我们还是进入实际的例子吧，还是使用上一节的系统图。在**引入softirq之后**，网络数据的处理如下：

**关中断**：**mask and ack interrupt controller**-------->**ack NIC**-------->**copy FIFO to ram**------>**raise softirq**------>**unmask interrupt controller**

**开中断**：在**softirq**上下文中进行**handle Data in the ram**的动作

同样的，我们先假设**Processor A**处理了这个网卡中断事件，很快的完成了**基本的HW操作**后，**raise softirq**。在**返回中断现场前**，会**检查softirq的触发情况**，因此，后续**网络数据处理的softirq**在p**rocessor A**上执行。在**执行过程中**，NIC硬件**再次触发中断**，**Interrupt controller将该中断分发给processor B**，执行动作**和Processor A是类似**的，因此，最后，网络数据处理的softirq在**processor B**上执行。

为了性能，同一类型的softirq有可能在不同的CPU上并发执行，这给使用者带来了极大的痛苦，因为驱动工程师在撰写softirq的回调函数的时候要考虑重入，考虑并发，要引入同步机制。但是，为了性能，我们必须如此。

当网络数据处理的softirq同时在Processor A和B上运行的时候，网卡中断又来了（可能是10G的网卡吧）。这时候，中断分发给processor A，这时候，processor A上的handler仍然会raise softirq，但是并不会调度该softirq。也就是说，softirq在一个CPU上是串行执行的。这种情况下，系统性能瓶颈是CPU资源，需要增加更多的CPU来解决该问题。

如果是tasklet的情况会如何呢？为何tasklet性能不如softirq呢？如果一个tasklet在processor A上被调度执行，那么它永远也不会同时在processor B上执行，也就是说，tasklet是串行执行的（注：不同的tasklet还是会并发的），不需要考虑重入的问题。我们还是用网卡这个例子吧（注意：这个例子仅仅是用来对比，实际上，网络数据是使用softirq机制的），同样是上面的系统结构图。假设使用tasklet，网络数据的处理如下：

关中断：mask and ack interrupt controller-------->ack NIC-------->copy FIFO to ram------>schedule tasklet------>unmask interrupt controller

开中断：在softirq上下文中（一般使用TASKLET_SOFTIRQ这个softirq）进行handle Data in the ram的动作

同样的，我们先假设Processor A处理了这个网卡中断事件，很快的完成了基本的HW操作后，schedule tasklet（同时也就raise TASKLET\_SOFTIRQ softirq）。在返回中断现场前，会检查softirq的触发情况，因此，在TASKLET\_SOFTIRQ softirq的handler中，获取tasklet相关信息并在processor A上执行该tasklet的handler。在执行过程中，NIC硬件再次触发中断，Interrupt controller将该中断分发给processor B，执行动作和Processor A是类似的，虽然TASKLET\_SOFTIRQ softirq在processor B上可以执行，但是，在检查tasklet的状态的时候，如果发现该tasklet在其他processor上已经正在运行，那么该tasklet不会被处理，一直等到在processor A上的tasklet处理完，在processor B上的这个tasklet才能被执行。这样的串行化操作虽然对驱动工程师是一个福利，但是对性能而言是极大的损伤。