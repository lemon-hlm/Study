- 1 前言
- 2 WQ\_POWER\_EFFICIENT的处理
- 3 分配workqueue的内存
    - 3.1 workqueue和pool workqueue的关系
    - 3.2 workqueue attribute
    - 3.3 unbound workqueue和NUMA之间的联系
- 4 初始化workqueue的成员
- 5 分配pool workqueue的内存并建立workqueue和pool workqueue的关系
- 6 应用新的attribute到workqueue中
    - 6.1 健康检查
    - 6.2 分配内存并初始化
    - 6.3 如何为unbound workqueue的pool workqueue寻找对应的线程池？
    - 6.4 给各个node分配pool workqueue并初始化
    - 6.5 安装

# 1 前言

本文主要以\_\_alloc\_workqueue\_key函数为主线，描述CMWQ中的创建一个workqueue实例的代码过程。

# 2 WQ\_POWER\_EFFICIENT的处理

\_\_alloc\_workqueue\_key函数的一开始有如下的代码：

```c
if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient) 
        flags |= WQ_UNBOUND;
```

在kernel中，有两种线程池，一种是线程池是per cpu的，也就是说，系统中有多少个cpu，就会创建多少个线程池，cpu x上的线程池创建的worker线程也只会运行在cpu x上。另外一种是unbound thread pool，该线程池创建的worker线程可以调度到任意的cpu上去。由于cache locality的原因，per cpu的线程池的性能会好一些，但是对power saving有一些影响。设计往往如此，workqueue需要在performance和power saving之间平衡，想要更好的性能，那么最好让一个cpu上的worker thread来处理work，这样的话，cache命中率会比较高，性能会更好。但是，从电源管理的角度来看，最好的策略是让idle状态的cpu尽可能的保持idle，而不是反复idle，working，idle again。

我们来一个例子辅助理解上面的内容。在t1时刻，work被调度到CPU A上执行，t2时刻work执行完毕，CPU A进入idle，t3时刻有一个新的work需要处理，这时候调度work到那个CPU会好些呢？是处于working状态的CPU B还是处于idle状态的CPU A呢？如果调度到CPU A上运行，那么，由于之前处理过work，其cache内容新鲜热辣，处理起work当然是得心应手，速度很快，但是，这需要将CPU A从idle状态中唤醒。选择CPU B呢就不存在将CPU 从idle状态唤醒，从而获取power saving方面的好处。

了解了上面的基础内容之后，我们再来检视per cpu thread pool和unbound thread pool。当workqueue收到一个要处理的work，如果该workqueue是unbound类型的话，那么该work由unbound thread pool处理并把调度该work去哪一个CPU执行这样的策略交给系统的调度器模块来完成，对于scheduler而言，它会考虑CPU core的idle状态，从而尽可能的让CPU保持在idle状态，从而节省了功耗。因此，如果一个workqueue有WQ\_UNBOUND这样的flag，则说明该workqueue上挂入的work处理是考虑到power saving的。如果workqueue没有WQ\_UNBOUND flag，则说明该workqueue是per cpu的，这时候，调度哪一个CPU core运行worker thread来处理work已经不是scheduler可以控制的了，这样，也就间接影响了功耗。

有两个参数可以控制workqueue在performance和power saving之间的平衡：

1、各个workqueue需要通过WQ\_POWER\_EFFICIENT来标记自己在功耗方面的属性

2、系统级别的内核参数workqueue.power\_efficient。

使用workqueue的用户知道自己在电源管理方面的特点，如果该workqueue在unbound的时候会极大的降低功耗，那么就需要加上WQ\_POWER\_EFFICIENT的标记。这时候，如果没有标记WQ\_UNBOUND，那么缺省workqueue会创建per cpu thread pool来处理work。不过，也可以通过workqueue.power\_efficient这个内核参数来修改workqueue的行为：

```c
#ifdef CONFIG_WQ_POWER_EFFICIENT_DEFAULT 
static bool wq_power_efficient = true; 
#else 
static bool wq_power_efficient; 
#endif

module_param_named(power_efficient, wq_power_efficient, bool, 0444);
```

如果wq\_power\_efficient设定为true，那么WQ\_POWER\_EFFICIENT的标记的workqueue就会强制按照unbound workqueue来处理，即使没有标记WQ\_UNBOUND。

# 3 分配workqueue的内存

```c
if (flags & WQ_UNBOUND) 
    tbl_size = nr_node_ids * sizeof(wq->numa_pwq_tbl[0]); －－－only for unbound workqueue

wq = kzalloc(sizeof(*wq) + tbl_size, GFP_KERNEL);

if (flags & WQ_UNBOUND) { 
        wq->unbound_attrs = alloc_workqueue_attrs(GFP_KERNEL); －－only for unbound workqueue 
    }
```

代码很简单，与其要解释代码，不如来解释一些基本概念。

## 3.1 workqueue和pool workqueue的关系

我们先给出一个简化版本的workqueue\_struct定义，如下：

```c
struct workqueue_struct { 
    struct list_head    pwqs;  
    struct list_head    list;


    struct pool_workqueue __percpu *cpu_pwqs;  －－－－－指向per cpu的pool workqueue 
    struct pool_workqueue __rcu *numa_pwq_tbl[]; －－－－指向per node的pool workqueue 
};
```

这里涉及2个数据结构：workqueue\_struct和pool\_workqueue，为何如此处理呢？我们知道，在CMWQ中，workqueue和thread pool没有严格的一一对应关系了，因此，系统中的workqueue们共享一组thread pool，因此，workqueue中的成员包括两个类别：global类型和per thread pool类型的，我们把那些per thread pool类型的数据集合起来就形成了pool\_workqueue的定义。

挂入workqueue的work终究需要worker pool中的某个worker thread来处理，也就是说，workqueue要和系统中那些共享的worker thread pool进行连接，这是通过pool\_workqueue（该数据结构会包含一个指向worker pool的指针）的数据结构来管理的。和这个workqueue相关的pool\_workqueue被挂入一个链表，链表头就是workqueue\_struct中的pwqs成员。

和旧的workqueue机制一样，系统维护了一个所有workqueue的list，list head定义如下：

```c
static LIST_HEAD(workqueues);
```

workqueue\_struct中的list成员就是挂入这个链表的节点。

workqueue有两种：unbound workqueue和per cpu workqueue。对于per cpu类型，cpu\_pwqs指向了一组per cpu的pool\_workqueue数据结构，用来维护workqueue和per cpu thread pool之间的关系。每个cpu都有两个thread pool，normal和高优先级的线程池，到底cpu\_pwqs指向哪一个pool\_workqueue（worker thread）是和workqueue的flag相关，如果标有WQ\_HIGHPRI，那么cpu\_pwqs指向高优先级的线程池。unbound workqueue对应的pool\_workqueue和workqueue属性相关，我们在下一节描述。

## 3.2 workqueue attribute

挂入workqueue的work终究是需要worker线程来处理，针对worker线程有下面几个考量点（我们称之attribute）：

（1）该worker线程的优先级

（2）该worker线程运行在哪一个CPU上

（3）如果worker线程可以运行在多个CPU上，且这些CPU属于不同的NUMA node，那么是否在所有的NUMA node中都可以获取良好的性能。

对于per\-CPU的workqueue，2和3不存在问题，哪个cpu上queue的work就在哪个cpu上执行，由于只能在一个确定的cpu上执行，因此起NUMA的node也是确定的（一个CPU不可能属于两个NUMA node）。置于优先级，per-CPU的workqueue使用WQ\_HIGHPRI来标记。综上所述，per\-CPU的workqueue不需要单独定义一个workqueue attribute，这也是为何在workqueue\_struct中只有unbound\_attrs这个成员来记录unbound workqueue的属性。

unbound workqueue由于不绑定在具体的cpu上，可以运行在系统中的任何一个cpu，直觉上似乎系统中有一个unbound thread pool就OK了，不过让一个thread pool创建多种属性的worker线程是一个好的设计吗？本质上，thread pool应该创建属性一样的worker thread。因此，我们通过workqueue属性来对unbound workqueue进行分类，workqueue属性定义如下：

```c
struct workqueue_attrs { 
    int            nice;        /* nice level */ 
    cpumask_var_t        cpumask;    /* allowed CPUs */ 
    bool            no_numa;    /* disable NUMA affinity */ 
};
```

nice是一个和thread优先级相关的属性，nice越低则优先级越高。cpumask是该workqueue挂入的work允许在哪些cpu上运行。no\_numa是一个和NUMA affinity相关的设定。

## 3.3 unbound workqueue和NUMA之间的联系

UMA系统中，所有的processor看到的内存都是一样的，访问速度也是一样，无所谓local or remote，因此，内核线程如果要分配内存，那么也是无所谓，统一安排即可。在NUMA系统中，不同的一个或者一组cpu看到的memory是不一样的，我们假设node 0中有CPU A和B，node 1中有CPU C和D，如果运行在CPU A上内核线程现在要迁移到CPU C上的时候，悲剧发生了：该线程在A CPU创建并运行的时候，分配的内存是node 0中的memory，这些memory是local的访问速度很快，当迁移到CPU C上的时候，原来local memory变成remote，性能大大降低。因此，unbound workqueue需要引入NUMA的考量点。

NUMA是内存管理的范畴，本文不会深入描述，我们暂且放开NUMA，先思考这样的一个问题：一个确定属性的unbound workqueue需要几个线程池？看起来一个就够了，毕竟workqueue的属性已经确定了，一个线程池创建相同属性的worker thread就行了。但是我们来看一个例子：假设workqueue的work是可以在node 0中的CPU A和B，以及node 1中CPU C和D上处理，如果只有一个thread pool，那么就会存在worker thread在不同node之间的迁移问题。为了解决这个问题，实际上unbound workqueue实际上是创建了per node的pool_workqueue（thread pool）

当然，是否使用per node的pool workqueue用户是可以通过下面的参数进行设定的：

（1）workqueue attribute中的no_numa成员

（2）通过workqueue.disable\_numa这个参数，disable所有workqueue的numa affinity的支持。

```c
static bool wq_disable_numa; 
module_param_named(disable_numa, wq_disable_numa, bool, 0444);
```

# 4 初始化workqueue的成员

```c
va_start(args, lock_name); 
vsnprintf(wq->name, sizeof(wq->name), fmt, args);－－－－－set workqueue name 
va_end(args);

max_active = max_active ?: WQ_DFL_ACTIVE; 
max_active = wq_clamp_max_active(max_active, flags, wq->name); 
wq->flags = flags; 
wq->saved_max_active = max_active; 
mutex_init(&wq->mutex); 
atomic_set(&wq->nr_pwqs_to_flush, 0); 
INIT_LIST_HEAD(&wq->pwqs); 
INIT_LIST_HEAD(&wq->flusher_queue); 
INIT_LIST_HEAD(&wq->flusher_overflow); 
INIT_LIST_HEAD(&wq->maydays);

lockdep_init_map(&wq->lockdep_map, lock_name, key, 0); 
INIT_LIST_HEAD(&wq->list);
```

除了max active，没有什么要说的，代码都简单而且直观。如果用户没有设定max active（或者说max active等于0），那么系统会给出一个缺省的设定。系统定义了两个最大值WQ\_MAX\_ACTIVE（512）和WQ\_UNBOUND_MAX\_ACTIVE（和cpu数目有关，最大值是cpu数目乘以4，当然也不能大于WQ\_MAX\_ACTIVE），分别限定per cpu workqueue和unbound workqueue的最大可以创建的worker thread的数目。wq\_clamp\_max\_active可以将max active限制在一个确定的范围内。

# 5 分配pool workqueue的内存并建立workqueue和pool workqueue的关系

这部分的代码主要涉及alloc\_and\_link\_pwqs函数，如下：

```c
static int alloc_and_link_pwqs(struct workqueue_struct *wq) 
{ 
    bool highpri = wq->flags & WQ_HIGHPRI;－－－－normal or high priority？ 
    int cpu, ret;

    if (!(wq->flags & WQ_UNBOUND)) {－－－－－per cpu workqueue的处理 
        wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);

        for_each_possible_cpu(cpu) {－－－－－逐个cpu进行设定 
            struct pool_workqueue *pwq =    per_cpu_ptr(wq->cpu_pwqs, cpu); 
            struct worker_pool *cpu_pools = per_cpu(cpu_worker_pools, cpu);

            init_pwq(pwq, wq, &cpu_pools[highpri]);  
            link_pwq(pwq);－－－－上面两行代码用来建立workqueue、pool wq和thread pool之间的关系 
        } 
        return 0; 
    } else if (wq->flags & __WQ_ORDERED) {－－－－－ordered unbound workqueue的处理 
        ret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]); 
        return ret; 
    } else {－－－－－unbound workqueue的处理 
        return apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]); 
    } 
}
```

通过alloc\_percpu可以为每一个cpu分配一个pool\_workqueue的memory。每个pool\_workqueue都有一个对应的worker thread pool，对于per\-CPU workqueue，它是静态定义的，如下：

```c
static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], 
                     cpu_worker_pools);
```

init\_pwq函数初始化pool\_workqueue，最重要的是设定其对应的workqueue和worker pool。link\_pwq主要是将pool\_workqueue挂入它所属的workqueue的链表中。对于unbound workqueue，apply\_workqueue\_attrs完成分配pool workqueue并建立workqueue和pool workqueue的关系。

# 6 应用新的attribute到workqueue中

unbound workqueue有两种，一种是normal type，另外一种是ordered type，这种workqueue上的work是严格按照顺序执行的，不存在并发问题。ordered unbound workqueue的行为类似过去的single thread workqueue。但是，无论那种类型的unbound workqueue都使用apply\_workqueue\_attrs来建立workqueue、pool wq和thread pool之间的关系。

## 6.1 健康检查

```c
if (WARN_ON(!(wq->flags & WQ_UNBOUND))) 
    return -EINVAL;

if (WARN_ON((wq->flags & __WQ_ORDERED) && !list_empty(&wq->pwqs))) 
    return -EINVAL;
```

只有unbound类型的workqueue才有attribute，才可以apply attributes。对于ordered类型的unbound workqueue，属于它的pool workqueue（worker thread pool）只能有一个，否则无法限制work是按照顺序执行。

## 6.2 分配内存并初始化

```c
pwq_tbl = kzalloc(nr_node_ids * sizeof(pwq_tbl[0]), GFP_KERNEL); 
new_attrs = alloc_workqueue_attrs(GFP_KERNEL); 
tmp_attrs = alloc_workqueue_attrs(GFP_KERNEL); 
copy_workqueue_attrs(new_attrs, attrs); 
cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask); 
copy_workqueue_attrs(tmp_attrs, new_attrs);
```

pwq\_tbl数组用来保存unbound workqueue各个node的pool workqueue的指针，new\_attrs和tmp\_attrs都是一些计算workqueue attribute的中间变量，开始的时候设定为用户传入的workqueue的attribute。

## 6.3 如何为unbound workqueue的pool workqueue寻找对应的线程池？

具体的代码在get\_unbound\_pool函数中。本节不描述具体的代码，只说明基本原理，大家可以自行阅读代码。

per cpu的workqueue的pool workqueue对应的线程池也是per cpu的，每个cpu有两个线程池（normal和high priority），因此将pool workqueue和thread pool对应起来是非常简单的事情。对于unbound workqueue，对应关系没有那么直接，如果属性相同，多个unbound workqueue的pool workqueue可能对应一个thread pool。

系统使用哈希表来保存所有的unbound worker thread pool，定义如下：

```c
static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);
```

在创建unbound workqueue的时候，pool workqueue对应的worker thread pool需要在这个哈希表中搜索，如果有相同属性的worker thread pool的话，那么就不需要创建新的线程池，代码如下：

```c
hash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) { 
    if (wqattrs_equal(pool->attrs, attrs)) { －－－－检查属性是否相同 
        pool->refcnt++; 
        return pool; －－－－－－－在哈希表找到适合的unbound线程池 
    } 
}
```

如果没有相同属性的thread pool，那么需要创建一个并挂入哈希表。

## 6.4 给各个node分配pool workqueue并初始化

在进入代码之前，先了解一些基础知识。缺省情况下，挂入unbound workqueue的works最好是考虑NUMA Affinity，这样可以获取更好的性能。当然，实际上用户可以通过workqueue.disable\_numa这个内核参数来关闭这个特性，这时候，系统需要一个default pool workqueue（workqueue\_struct的dfl\_pwq成员），所有的per node的pool workqueue指针都是执行default pool workqueue。

workqueue.disable_numa是enable的情况下是否不需要default pool workqueue了呢？也不是，我们举一个简单的例子，一个系统的构成是这样的：node 0中有CPU A和B，node 1中有CPU C和D，node 2中有CPU E和F，假设workqueue的attribute规定work只能在CPU A 和C上运行，那么在node 0和node 1中创建自己的pool workqueue是ok的，毕竟node 0中有CPU A，node 1中有CPU C，该node创建的worker thread可以在A或者C上运行。但是对于node 2节点，没有任何的CPU允许处理该workqueue的work，在这种情况下，没有必要为node 2建立自己的pool workqueue，而是使用default pool workqueue。

OK，我们来看代码：

```c
dfl_pwq = alloc_unbound_pwq(wq, new_attrs); －－－－－分配default pool workqueue

for_each_node(node) { －－－－遍历node 
    if (wq_calc_node_cpumask(attrs, node, -1, tmp_attrs->cpumask)) { －－－是否使用default pool wq 
        pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs); －－－该node使用自己的pool wq 
    } else { 
        dfl_pwq->refcnt++; 
        pwq_tbl[node] = dfl_pwq; －－－－该node使用default pool wq 
    } 
}
```

值得一提的是wq\_calc\_node\_cpumask这个函数，这个函数会根据该node的cpu情况以及workqueue attribute中的cpumask成员来更新tmp\_attrs\->cpumask，因此，在pwq\_tbl\[node\] = alloc\_unbound\_pwq(wq, tmp\_attrs); 这行代码中，为该node分配pool workqueue对应的线程池的时候，去掉了本node中不存在的cpu。例如node 0中有CPU A和B，workqueue的attribute规定work只能在CPU A 和C上运行，那么创建node 0上的pool workqueue以及对应的worker thread pool的时候，需要删除CPU C，也就是说，node 0上的线程池的属性中的cpumask仅仅支持CPU A了。

## 6.5 安装

所有的node的pool workqueue及其worker thread pool已经ready，需要安装到workqueue中了：

```c
for_each_node(node) 
        pwq_tbl[node] = numa_pwq_tbl_install(wq, node, pwq_tbl[node]);  
    link_pwq(dfl_pwq); 
    swap(wq->dfl_pwq, dfl_pwq);
```

代码非常简单，这里就不细述了。