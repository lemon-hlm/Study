[TOC]

# 1 前言

对于中断处理而言，linux将其分成了两个部分，一个叫做中断handler（top half），属于**不那么紧急**需要处理的事情被推迟执行，我们称之**deferable task**，或者叫做**bottom half**，。具体如何推迟执行分成下面几种情况：

1、推迟到**top half执行完**毕

2、推迟到**某个指定的时间片（例如40ms）之后**执行

3、推迟到**某个内核线程被调度**的时候执行

对于**第一种**情况，内核中的机制包括**softirq机制**和**tasklet机制**。**第二种**情况是属于**softirq机制**的一种应用场景（**timer类型的softirq**），在本站的**时间子系统**的系列文档中会描述。**第三种**情况主要包括**threaded irq handler**以及**通用的workqueue机制**，当然也包括**自己创建**该驱动专属kernel thread（**不推荐**使用）。本文主要描述tasklet这种机制，第二章描述一些背景知识和和tasklet的思考，第三章结合代码描述tasklet的原理。

注：本文中的linux kernel的版本是4.0

# 2 为什么需要tasklet？

## 2.1 基本的思考

我们的驱动程序或者内核模块真的需要tasklet吗？每个人都有自己的看法。我们先抛开linux kernel中的机制，首先进行一番逻辑思考。

将**中断处理**分成**top half**（**cpu和外设之间的交互！！！**，**获取状态**，**ack状态**，**收发数据**等）和**bottom half**（后段的**数据处理**）已经深入人心，对于任何的OS都一样，将**不那么紧急**的事情推迟到**bottom half**中执行是OK的，具体**如何推迟执行**分成**两种类型**：有**具体时间要求**的（对应linux kernel中的**低精度timer和高精度timer**）和**没有具体时间要求**的。对于**没有具体时间**要求的又可以分成两种：

（1）**越快越好**型，这种实际上是有**性能要求**的，除了中断top half可以抢占其执行，其他的进程上下文（无论该进程的优先级多么的高）是不会影响其执行的，一言以蔽之，在不影响中断延迟的情况下，OS会尽快处理。

（2）**随遇而安**型。这种属于那种没有性能需求的，其调度执行依赖系统的调度器。

本质上讲，越快越好型的bottom half不应该太多，而且tasklet的callback函数**不能执行时间过长**，否则会产生进程调度延迟过大的现象，甚至是非常长而且不确定的延迟，对real time的系统会产生很坏的影响。

## 2.2 对linux中的bottom half机制的思考

在linux kernel中，“**越快越好型**”有**两种**，**softirq**和**tasklet**，“**随遇而安**型”也有**两种**，**workqueue**和**threaded irq handler**。“**越快越好**型”能否**只留下一个softirq**呢？对于崇尚简单就是美的程序员当然希望如此。为了回答这个问题，我们先看看**tasklet**对于softirq而言有哪些**好处**：

（1）**tasklet**可以**动态分配**，也可以**静态分配**，数量不限。

（2）**同一种tasklet**在**多个cpu**上也**不会并行执行**，这使得程序员在撰写tasklet function的时候比较方便，减少了对**并发**的考虑（当然损失了性能）。

对于**第一种好处**，其实也就是为乱用tasklet打开了方便之门，很多撰写驱动的软件工程师不会仔细考量其driver**是否有性能需求**就直接使用了**tasklet机制**。对于**第二种好处**，本身**考虑并发**就是**软件工程师的职责**。因此，看起来tasklet并**没有引入特别的好处**，而且**和softirq一样**，都**不能sleep**，限制了handler撰写的方便性，看起来其实并没有存在的必要。在4.0 kernel的代码中，grep一下tasklet的使用，实际上是一个很长的列表，只要对这些使用进行**简单的归类**就**可以删除对tasklet的使用**。对于那些有**性能需求**的，可以考虑**并入softirq**，其他的可以考虑**使用workqueue**来取代。Steven Rostedt试图进行这方面的尝试（ http://lwn.net/Articles/239484/ ），不过这个patch始终未能进入main line。

# 3 tasklet的基本原理

## 3.1 如何抽象一个tasklet

内核中用下面的数据结构来表示tasklet：

```c
[include/linux/interrupt.h]
struct tasklet_struct 
{ 
    struct tasklet_struct *next; 
    unsigned long state; 
    atomic_t count; 
    void (*func)(unsigned long); 
    unsigned long data; 
};
```

**每个cpu**都会**维护一个链表(！！！**)，将**本cpu**需要处理的**tasklet管理**起来，**next**这个成员指向了该**链表中的下一个tasklet**。**func**和**data**成员描述了**该tasklet的callback函数**，func是**调用函数**，data是**传递给func的参数**。state成员表示**该tasklet的状态**，**TASKLET\_STATE\_SCHED**表示该tasklet已经**被调度到某个CPU上执行**，TASKLET\_STATE\_RUN表示该tasklet**正在某个cpu上执行**。**count成员**是和**enable或者disable该tasklet**的状态相关，如果**count等于0**那么该tasklet是处于**enable**的，如果大于0，表示该tasklet是disable的。在**softirq文档**中，我们知道**local\_bh\_disable/enable函数**就是用来**disable/enable bottom half(！！！所有softirq和tasklet！！！**)的，这里就**包括softirq和tasklet**。但是，有的时候内核同步的场景不需disable所有的softirq和tasklet，而仅仅是**disable该tasklet**，这时候，tasklet\_disable和tasklet\_enable就派上用场了。

```c
static inline void tasklet_disable(struct tasklet_struct *t) 
{ 
    tasklet_disable_nosync(t);－－－－－－－给tasklet的count加一 
    tasklet_unlock_wait(t);－－－－－如果该tasklet处于running状态，那么需要等到该tasklet执行完毕 
    smp_mb(); 
}

static inline void tasklet_enable(struct tasklet_struct *t) 
{ 
    smp_mb__before_atomic(); 
    atomic_dec(&t->count);－－－－－－－给tasklet的count减一 
}
```

tasklet\_disable和tasklet\_enable支持嵌套，但是需要成对使用。

## 3.2 系统如何管理tasklet？

系统中的**每个cpu**都会维护**一个tasklet的链表(！！！**)，定义如下：

```c
static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec); 
static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
```

linux kernel中，和**tasklet**相关的softirq有两项，**HI\_SOFTIRQ**用于**高优先级的tasklet**，**TASKLET\_SOFTIRQ**用于**普通的tasklet**。对于**softirq**而言，**优先级**就是出现在**softirq pending register（\_\_softirq\_pending）中的先后顺序(！！！**)，位于**bit 0拥有最高的优先级**，也就是说，如果有**多个不同类型**的**softirq同时触发**，那么**执行的先后顺序**依赖在**softirq pending register的位置**，kernel总是从右向左依次判断是否置位，如果置位则执行。**HI\_SOFTIRQ占据了bit 0**，其优先级甚至**高过timer**，需要慎用（实际上，我grep了内核代码，似乎没有发现对HI\_SOFTIRQ的使用）。当然**HI\_SOFTIRQ和TASKLET\_SOFTIRQ的机理**是一样的，因此本文只讨论TASKLET\_SOFTIRQ，大家可以举一反三。

## 3.3 如何定义一个tasklet？

你可以用下面的宏定义来**静态定义tasklet**：

```c
#define DECLARE_TASKLET(name, func, data) \ 
struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }

#define DECLARE_TASKLET_DISABLED(name, func, data) \ 
struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }
```

这两个宏都可以静态定义一个struct tasklet\_struct的变量，只不过初始化后的tasklet一个是处于eable状态，一个处于disable状态的。当然，也可以**动态分配tasklet**，然后调用**tasklet\_init**来**初始化该tasklet**。

## 3.4 如何调度一个tasklet

为了**调度一个tasklet执行**，我们可以使用**tasklet\_schedule**这个接口：

```c
static inline void tasklet_schedule(struct tasklet_struct *t) 
{ 
    if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) 
        __tasklet_schedule(t); 
}
```

程序在**多个上下文**中可以**多次调度同一个tasklet执行**（也可能**来自多个cpu core**），不过实际上**该tasklet只会一次挂入首次调度到的那个cpu的tasklet链表(！！！**)，也就是说，即便是**多次调用tasklet\_schedule**，实际上**tasklet只会挂入一个指定CPU的tasklet队列**中（而且**只会挂入一次**），也就是说只会调度一次执行。这是通过**TASKLET\_STATE\_SCHED这个flag**来完成的，我们可以用下面的图片来描述：

![config](./images/11.gif)

我们假设**HW block A**的**驱动**使用的**tasklet机制**并且在**中断handler（top half**）中将**静态定义的tasklet**（这个tasklet是**各个cpu共享**的，不是per cpu的）**调度执行**（也就是**调用tasklet\_schedule**函数）。当**HW block A检测到硬件**的动作（例如**接收FIFO中数据达到半满**）就会**触发IRQ line上的电平或者边缘信号**，**GIC**检测到**该信号**会将**该中断**分发给**某个CPU执行其top half handler(！！！**)，我们假设这次是**cpu0**，因此**该driver的tasklet**被挂入**CPU0对应的tasklet链表！！！**（tasklet\_vec）并**将state的状态**设定为**TASKLET\_STATE\_SCHED**。**HW block A的驱动**中的**tasklet虽已调度**，但是**没有执行(！！！**)，如果这时候，硬件**又一次触发中断**并**在cpu1上执行top handler！！！**，虽然**tasklet\_schedule函数被再次调用**，但是由于**TASKLET\_STATE\_SCHED已经设定**，因此**不会**将**HW block A的驱动**中的这个**tasklet挂入cpu1的tasklet链表**中。

下面我们再仔细研究一下底层的\_\_tasklet\_schedule函数：

```c
void __tasklet_schedule(struct tasklet_struct *t) 
{ 
    unsigned long flags;

    local_irq_save(flags);－－－－（1） 
    t->next = NULL;－－－－（2） 
    *__this_cpu_read(tasklet_vec.tail) = t; 
    __this_cpu_write(tasklet_vec.tail, &(t->next)); 
    raise_softirq_irqoff(TASKLET_SOFTIRQ);－－－－（3） 
    local_irq_restore(flags); 
}
```

（1）下面的**链表操作是per\-cpu**的，因此这里**禁止本地中断**就可以**拦截所有的并发**。

（2）这里的三行代码就是**将一个tasklet挂入链表的尾部**

（3）**raise TASKLET\_SOFTIRQ类型**的**softirq**。

## 3.5 在什么时机会执行tasklet？

上面描述了tasklet的调度，当然**调度tasklet不等于执行tasklet(！！！**)，系统会在适合的时间点执行tasklet callback function。由于**tasklet是基于softirq**的，因此，我们首先总结一下**softirq的执行场景**：

（1）在**中断返回用户空间（进程上下文**）的时候，如果有**pending的softirq**，那么将执行该softirq的处理函数。这里限定了**中断返回用户空间**也就是意味着**限制了下面两个场景的softirq被触发执行**：

（a）中断返回hard interrupt context，也就是**中断嵌套**的场景

（b）中断返回software interrupt context，也就是**中断抢占软中断上下文**的场景

（2）上面的描述缺少了一种场景：**中断返回内核态的进程上下文**的场景，这里我们需要详细说明。**进程上下文**中调用**local\_bh\_enable**的时候，如果有**pending的softirq**，那么将**执行该softirq的处理函数**。由于**内核同步**的要求，**进程上下文**中有可能会调用**local\_bh\_enable/disable来保护临界区**。在**临界区代码执行过程**中，**中断随时**会到来，**抢占该进程（内核态）的执行**（注意：这里**只是disable了bottom half，没有禁止中断！！！**）。在这种情况下，中断返回的时候是否会执行softirq handler呢？当然不会，我们disable了bottom half的执行，也就是意味着不能执行softirq handler，但是**本质上bottom half应该比进程上下文有更高的优先级(！！！**)，一旦条件允许，要**立刻抢占进程上下文(！！！**)的执行，因此，当立刻离开临界区，调用**local\_bh\_enable**的时候，会**检查softirq pending**，如果**bottom half处于enable的状态**，pending的s**oftirq handler会被执行**。

（3）系统太繁忙了，不过的**产生中断**，**raise softirq**，由于**bottom half的优先级高**，从而导致进程无法调度执行。这种情况下，**softirq会推迟到softirqd这个内核线程**中去执行。

对于**TASKLET\_SOFTIRQ类型的softirq**，其**handler是tasklet\_action**，我们来看看各个tasklet是如何执行的：

```c
static void tasklet_action(struct softirq_action *a) 
{ 
    struct tasklet_struct *list;

    local_irq_disable();－－－－－－－－－－（1） 
    list = __this_cpu_read(tasklet_vec.head); 
    __this_cpu_write(tasklet_vec.head, NULL); 
    __this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head)); 
    local_irq_enable();

    while (list) {－－－－－－－－－遍历tasklet链表 
        struct tasklet_struct *t = list;

        list = list->next;

        if (tasklet_trylock(t)) {－－－－－－－－（2） 
            if (!atomic_read(&t->count)) {－－－－－－（3） 
                if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state)) 
                    BUG(); 
                t->func(t->data); 
                tasklet_unlock(t); 
                continue;－－－－－处理下一个tasklet 
            } 
            tasklet_unlock(t);－－－－清除TASKLET_STATE_RUN标记 
        }

        local_irq_disable();－－－－－－－（4） 
        t->next = NULL; 
        *__this_cpu_read(tasklet_vec.tail) = t; 
        __this_cpu_write(tasklet_vec.tail, &(t->next)); 
        __raise_softirq_irqoff(TASKLET_SOFTIRQ); －－－－－－再次触发softirq，等待下一个执行时机 
        local_irq_enable(); 
    } 
}
```

（1）从本cpu的tasklet链表中取出全部的tasklet，保存在list这个临时变量中，同时重新初始化本cpu的tasklet链表，使该链表为空。由于bottom half是开中断执行的，因此在操作tasklet链表的时候需要使用关中断保护

（2）tasklet\_trylock主要是用来设定该tasklet的state为TASKLET\_STATE\_RUN，同时判断该tasklet是否已经处于执行状态，这个状态很重要，它决定了后续的代码逻辑。

```c
static inline int tasklet_trylock(struct tasklet_struct *t) 
{ 
    return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state); 
}
```

你也许会奇怪：为何这里从tasklet的链表中摘下一个本cpu要处理的tasklet list，而这个list中的tasklet已经处于running状态了，会有这种情况吗？会的，我们再次回到上面的那个软硬件结构图。同样的，HW block A的驱动使用的tasklet机制并且在中断handler（top half）中将静态定义的tasklet 调度执行。HW block A的硬件中断首先送达cpu0处理，因此该driver的tasklet被挂入CPU0对应的tasklet链表并在适当的时间点上开始执行该tasklet。这时候，cpu0的硬件中断又来了，该driver的tasklet callback function被抢占，虽然tasklet仍然处于running状态。与此同时，HW block A硬件又一次触发中断并在cpu1上执行，这时候，该driver的tasklet处于running状态，并且TASKLET\_STATE\_SCHED已经被清除，因此，调用tasklet\_schedule函数将会使得该driver的tasklet挂入cpu1的tasklet链表中。由于cpu0在处理其他硬件中断，因此，cpu1的tasklet后发先至，进入tasklet_action函数调用，这时候，当从cpu1的tasklet摘取所有需要处理的tasklet链表中，HW block A对应的tasklet实际上已经是在cpu0上处于执行状态了。

我们在设计tasklet的时候就规定，同一种类型的tasklet只能在一个cpu上执行，因此tasklet_trylock就是起这个作用的。

（3）检查该tasklet是否处于enable状态，如果是，说明该tasklet可以真正进入执行状态了。主要的动作就是清除TASKLET\_STATE\_SCHED状态，执行tasklet callback function。

（4）如果该tasklet已经在别的cpu上执行了，那么我们将其挂入该cpu的tasklet链表的尾部，这样，在下一个tasklet执行时机到来的时候，kernel会再次尝试执行该tasklet，在这个时间点，也许其他cpu上的该tasklet已经执行完毕了。通过这样代码逻辑，保证了特定的tasklet只会在一个cpu上执行，不会在多个cpu上并发。